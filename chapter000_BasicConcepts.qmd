---
title: "Basic Concepts"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(patchwork)
library(gghalves)
library(ggridges)
library(here)
library(ggthemes)
library(gt)
library(gtsummary)


ggplot2::theme_set(
  ggthemes::theme_few(base_size = 15)
  )

```

```{r}
#| label: basics-001
#| fig-width: 10
#| fig-cap: The necessary statistical ingredients.
knitr::include_graphics(here::here("chapter000","000_basics_root.png"))
```

::: {.content-visible when-profile="script"}

Statistics is a fundamental field that plays a crucial role in various disciplines, from science and economics to social sciences and beyond. It's the science of collecting, organizing, analyzing, interpreting, and presenting data. In this introductory overview, we'll explore some key concepts and ideas that form the foundation of statistics:

1. **Data:** At the heart of statistics is data. Data can be anything from numbers and measurements to observations and information collected from experiments, surveys, or observations. In statistical analysis, we work with two main types of data: quantitative (numerical) and qualitative (categorical).

2. **Descriptive Statistics:** Descriptive statistics involve methods for summarizing and organizing data. These methods help us understand the basic characteristics of data, such as measures of central tendency (mean, median, mode) and measures of variability (range, variance, standard deviation).

3. **Inferential Statistics:** Inferential statistics is about making predictions, inferences, or decisions about a population based on a sample of data. This involves hypothesis testing, confidence intervals, and regression analysis, among other techniques.

4. **Probability:** Probability theory is the foundation of statistics. It deals with uncertainty and randomness. We use probability to describe the likelihood of events occurring in various situations, which is essential for making statistical inferences.

5. **Sampling:** In most cases, it's impractical to collect data from an entire population. Instead, we often work with samples, which are smaller subsets of the population. The process of selecting and analyzing samples is a critical aspect of statistical analysis.

6. **Variables:** Variables are characteristics or attributes that can vary from one individual or item to another. They can be categorized as dependent (response) or independent (predictor) variables, depending on their role in a statistical analysis.

7. **Distributions:** A probability distribution describes the possible values of a variable and their associated probabilities. Common distributions include the normal distribution, binomial distribution, and Poisson distribution, among others.

8. **Statistical Software:** In the modern era, statistical analysis is often conducted using specialized software packages like R, Python (with libraries like NumPy and Pandas), SPSS, or Excel. These tools facilitate data manipulation, visualization, and complex statistical calculations.

9. **Ethics and Bias:** It's essential to consider ethical principles in statistical analysis, including issues related to data privacy, confidentiality, and the potential for bias in data collection and interpretation.

10. **Real-World Applications:** Statistics has a wide range of applications, from medical research to marketing, finance, and social sciences. It helps us make informed decisions and draw meaningful insights from data in various fields.

:::

## Probability

### Overview

Probability theory is a fundamental concept in the field of statistics, serving as the foundation upon which many statistical methods and models are built. 

### What is Probability?

Probability is a mathematical concept that quantifies the uncertainty or randomness of events. 
It provides a way to measure the likelihood of different outcomes occurring in a given situation. 
In essence, probability is a numerical representation of our uncertainty.

### Basic Probability Terminology {.smaller}

- **Experiment**: An experiment is any process or procedure that results in an outcome. 
For example, rolling a fair six-sided die is an experiment.

- **Outcome**: An outcome is a possible result of an experiment. 
When rolling a die, the outcomes are the numbers 1 through 6.

- **Sample Space (S)**: The sample space is the set of all possible outcomes of an experiment. 
For a fair six-sided die, the sample space is $\{1, 2, 3, 4, 5, 6\}$.

- **Event (E)**: An event is a specific subset of the sample space. 
It represents a particular set of outcomes that we are interested in. 
For instance, "rolling an even number" is an event for a six-sided die, which includes outcomes $\{2, 4, 6\}$.

### Probability Notation

In probability theory, we use notation to represent various concepts:

- **P(E)**: Probability of event E occurring.
- **P(A and B)**: Probability of both events A and B occurring.
- **P(A or B)**: Probability of either event A or event B occurring.
- **P(E')**: Probability of the complement of event E, which is the probability of E not occurring.

### The Fundamental Principles of Probability {.smaller}

There are two fundamental principles of probability:

- **The Addition Rule**: It states that the probability of either event A or event B occurring is given by the sum of their individual probabilities, provided that the events are mutually exclusive (i.e., they cannot both occur simultaneously).

\begin{align}
P(A \; or \; B) = P(A) + P(B)
\end{align}

- **The Multiplication Rule**: It states that the probability of both event A and event B occurring is the product of their individual probabilities, provided that the events are independent (i.e., the occurrence of one event does not affect the occurrence of the other).

\begin{align}
P(A \; and\;B) = P(A) * P(B)
\end{align}

### Example: Rolling a Fair Six-Sided Die

::: {.content-visible when-profile="script"}

Consider rolling a fair six-sided die. 

- Sample Space (S): $\{1, 2, 3, 4, 5, 6\}$ (@fig-prob)
- Event A: Rolling an even number = $\{2, 4, 6\}$ (@fig-prob)
- Event B: Rolling a number greater than $3 = \{4, 5, 6\}$ (@fig-prob)
:::

```{r}
#| label: fig-prob
#| out-width: 75%
#| fig-cap: This example's sample space, as well as event A and event B.
knitr::include_graphics(here::here("chapter000","010_Probability.png"))
```


### Probability in action - The Galton Board
::: {.content-visible when-profile="script"}

A Galton board, also known as a bean machine or a quincunx, is a mechanical device that demonstrates the principles of probability and the normal distribution. 
It was invented by Sir Francis Galton[^6] in the late 19th century. The Galton board consists of a vertical board with a series of pegs or nails arranged in triangular or hexagonal patterns.

A Galton board, also known as a bean machine or a quincunx, is a mechanical device that demonstrates the principles of probability and the normal distribution. It was invented by Sir Francis Galton in the late 19th century. The Galton board consists of a vertical board with a series of pegs or nails arranged in triangular or hexagonal patterns.

1. **Initial Release**: At the top of the Galton board, a ball or particle is released. 
This ball can take one of two paths at each peg, either to the left or to the right. 
The decision at each peg is determined by chance, such as the flip of a coin or the roll of a die. 
This represents a random event.

2. **Multiple Trials**: As the ball progresses downward, it encounters several pegs, each of which randomly directs it either left or right. 
The ball continues to bounce off pegs until it reaches the bottom.

3. **Accumulation**: Over multiple trials or runs of the Galton board, you will notice that the balls accumulate in a pattern at the bottom. 
This pattern forms a bell-shaped curve, which is the hallmark of a normal distribution.

4. **Normal Distribution**: The accumulation of balls at the bottom resembles the shape of a normal distribution curve. 
This means that the majority of balls will tend to accumulate in the center, forming the peak of the curve, while fewer balls will accumulate at the extreme left and right sides.

The Galton board is a visual representation of the [central limit theorem](#clt), a fundamental concept in probability theory. 
It demonstrates how random events, when repeated many times, tend to follow a normal distribution. 
This distribution is commonly observed in various natural phenomena and is essential in statistical analysis.

[^6]: Sir Francis Galton (1822-1911): Influential English scientist, notable for his contributions to statistics and genetics.

:::

```{r}
#| label: fig-plinko
#| out-width: 75%
#| fig-cap: A Galton board in action.

knitr::include_graphics(here::here("chapter000","011_Plinko.png"))

```


#### Statistics and Probabbility

::: {.content-visible when-profile="slides"}

What is the probability for a ball to land in one of the bins?

:::

::: {.content-visible when-profile="script"}

The Galton board is a nice example how statistics emerge from probability.

:::

##### Define the problem

- The board has $n$ rows of pegs (columns)
- Each ball has an equal probability of moving left or right (assuming no bias)
- The number of rightward moves determines the final position in the bins

::: {.content-visible when-profile="script"}


##### Step 2: Binomial Probability Distribution

Each ball independently moves right ($R$) or left ($L$) with a probability of $p=0.5$.

The number of rightwards moves follows a binomial distribution.

\begin{align}
P(k) = \binom{n}{k} p^k (1 - p)^{n - k} 
\end{align}

$n$
: total number of columns (or pegs encountered)

$k$
: number of rightward moves

$\binom{n}{k}$
: biomial coefficient, given by $\binom{n}{k} = \frac{n!}{k!(n-k)!}$

with $p = 0.5$ this simplifies to

\begin{align}
P(k) = \binom{n}{k} ( \frac{1}{2})^n
\end{align}

##### Step 3: Position Mapping

The final position of a ball in a bin corresponds to the number of rightwards moves $k$. 
If the bins are indexed from $0$ to $n$ (where $k=0$ means all left moves and $k=n$ means all right moves) the probability of landing in bin $k$ is:

\begin{align}
P(k) = \frac{n!}{k!(n-k)!}(\frac{1}{2})^n
\end{align}

:::

::: {.content-visible when-profile="slides"}

##### Formulate the Problem

- ball posistion depends on the number of rightwards moves $k$
- rows are indexed from $0$ to $n$ ($k = 0 \rightarrow \text{all left}; k = n \rightarrow \text{all right}$)

for $n = 4$ bins there are $4! = 24$ ways for the ball to choose

What is the probability for a ball to "choose"?


##### The probability for a specific sequence

Suppose we have $n = 6$ rows. 
What is the probability for this sequence:

$$RRLLRL$$

::: {.fragment .fade-in}
$$P(RRLLRL) = \frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2} = \left( \frac{1}{2} \right)^n$$ 
:::

::: {.fragment .fade-in}

Is it different for another sequence?

:::

##### Bins and Sequence

Is it sure, that every ball that follows a specific sequence lands in it's own bin?

::: {.fragment .fade-in}

Of course not.

:::

::: {.fragment .fade-in}

How is that probability calculated?

:::

##### Generalizing to $k$ right moves

Calculate the probability of exactly $k = 3$ right moves in $n = 6$

::: {.fragment .fade-in}

How many possibilities are there in total?

:::

::: {.fragment .fade-in}

$6! = 6 \times 5\times 4 \times 3 \times 2 \times 1 = 720$

:::

::: {.fragment .fade-in}

The factorial takes ALL sequences into account, but does it matter?

:::

::: {.fragment .fade-in}

We do not care for the order in which the turns are taken!

How do we correct for that?

:::

##### Correction for order


We correct for $k! = 3 \times 2 \times 1 = 6$, the number of ways to arrange the $k$ chosen elements among themselves.

$$\frac{n!}{k!} = \frac{720}{6} = 120$$

::: {.fragment .fade-in}

Are we finished?

:::

##### Correction for Unselected Elements

$(n-k)$ elements are not *chosen*, so we divide by another factorial, to correct fo the *NOT* chosen ones.

$$(n-k)! = (6-3)! = 3! = 6$$

##### Binomial Coefficient

$$\frac{n!}{k!(n-k)!} = \frac{720}{6\times6} = 20$$

This is called the *Binomial Coefficient* and it is written

$$\binom{n}{k}$$

So, the number of ways to choose $3$ elements out of $6$ is $20$.



##### Probability??

Now we know how many ways are there to choose, but in order to cacluclate a probability we still need to multiply this with the probability for **each** sequence

$$P(k) = \binom{n}{k}\cdot{\frac{1}{2}}^n$$

##### Detailed Calculation

::: {.r-stack}

::: {.fragment .fade-out}

::: {style="font-size: 70%;"}

| $k$ |  $n!$ |  $k!$ |       $(n-k)!$      | $\binom{n}{k} = \binom{6}{k}$ | $P(k) = \binom{6}{k}\cdot{\frac{1}{2}}^{n=6}$ |
|:---:|:-----:|:-----:|:-------------------:|:-----------------------------:|:---------------------------------------------:|
| $0$ | $720$ |  $1$  | $(6-0)! = 6! = 720$ | $\frac{720}{1 \cdot 720} = 1$ |    $P(0) = 1 \cdot \frac{1}{64} = 0.015624$   |
| $1$ |       |       |                     |                               |                                               |
| $2$ |       |       |                     |                               |                                               |
| $3$ |       |       |                     |                               |                                               |
| $4$ |       |       |                     |                               |                                               |
| $5$ |       |       |                     |                               |                                               |
| $6$ |       |       |                     |                               |                                               |

:::

:::

::: {.fragment .fade-in-then-out}

::: {style="font-size: 70%;"}

| $k$ |  $n!$ |  $k!$ |       $(n-k)!$      | $\binom{n}{k} = \binom{6}{k}$ | $P(k) = \binom{6}{k}\cdot{\frac{1}{2}}^{n=6}$ |
|:---:|:-----:|:-----:|:-------------------:|:-----------------------------:|:---------------------------------------------:|
| $0$ | $720$ |  $1$  | $(6-0)! = 6! = 720$ | $\frac{720}{1 \cdot 720} = 1$ |    $P(0) = 1 \cdot \frac{1}{64} = 0.015624$   |
| $1$ | $720$ |  $1$  | $(6-1)! = 5! = 120$ | $\frac{720}{1 \cdot 120} = 6$ |    $P(1) = 6 \cdot \frac{1}{64} = 0.093750$   |
| $2$ | $720$ |  $2$  |  $(6-2)! = 4! = 24$ | $\frac{720}{2 \cdot 24} = 15$ |   $P(2) = 15 \cdot \frac{1}{64} = 0.234375$   |
| $3$ | $720$ |  $6$  |  $(6-3)! = 3! = 6$  |  $\frac{720}{6 \cdot 6} = 20$ |   $P(3) = 20 \cdot \frac{1}{64} = 0.312500$   |
| $4$ | $720$ |  $24$ |  $(6-4)! = 2! = 2$  | $\frac{720}{24 \cdot 2} = 15$ |   $P(2) = 15 \cdot \frac{1}{64} = 0.234375$   |
| $5$ | $720$ | $120$ |  $(6-5)! = 1! = 1$  | $\frac{720}{120 \cdot 1} = 6$ |    $P(5) = 6 \cdot \frac{1}{64} = 0.093750$   |
| $6$ | $720$ | $720$ |  $(6-6)! = 0! = 1$  | $\frac{720}{720 \cdot 1} = 1$ |    $P(6) = 1 \cdot \frac{1}{64} = 0.015624$   |

:::

:::

:::

##### Graphical representation

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| fig-width: 15
#| fig-height: 8


galton_dat <- data.frame(
  bin_nr = seq(0,6),
  n = 6
) |> 
  rowwise() |> 
  mutate(
    binom_coef = pracma::nchoosek(n,bin_nr),
    Prob = binom_coef*(0.5^n)
  )

galton_dat |> 
  ggplot(
    aes(
      x = bin_nr,
      y = Prob
      )
  )+
  geom_col()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  theme_minimal(
    base_size = 15
  )

```

:::

::: {.fragment .fade-in}

```{r}
#| fig-width: 15
#| fig-height: 8


galton_dat <- data.frame(
  bin_nr = seq(0,1000),
  n = 1000
) |> 
  rowwise() |> 
  mutate(
    binom_coef = pracma::nchoosek(n,bin_nr),
    Prob = binom_coef*(0.5^n)
  )

galton_dat |> 
  ggplot(
    aes(
      x = bin_nr,
      y = Prob
      )
  )+
  geom_col()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  theme_minimal(
    base_size = 15
  )

```

:::

:::

:::

## {{< acr CLT >}}

::: {.r-stack}

::: {.content-visible when-profile="slides"}

::: {.fragment .fade-out}

```{r}
#| label: fig-clt-data
#| out-width: 75%
#| fig-cap: The population from which the samples to show the CLT are taken.

# Set the parameters for the population distribution
population_size <- 10000  # Size of the population
population_distribution <- runif(population_size, min = 1, max = 100)  # Uniform distribution

ggplot(data = data.frame(x = population_distribution),aes(x))+
  geom_density(fill = "gray")+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0.05,0))+
  labs(
    title = "The uniform distribution from which the samples are drawn",
    x = "value (min = 1, max = 100)"
      )+
  theme_few()
```

:::

:::

::: {.fragment}
```{r}
#| label: fig-clt
#| out-width: 99%
#| fig-cap: The central limit theorem in action.  

# Number of samples to take
num_samples <- 1000

# Set the sample sizes for each iteration
sample_sizes <- c(2, 10, 50,200)

# Create a data frame to store results
results <- data.frame()

# Simulate sampling and calculate sample means
for (sample_size in sample_sizes) {
  sample_means <- replicate(num_samples, mean(sample(population_distribution, sample_size)))
  result <- data.frame(Sample_Size = sample_size, Sample_Means = sample_means)
  results <- bind_rows(results, result)
}

# Plot the CLT demonstration using ggplot2
ggplot(results, aes(x = Sample_Means)) +
  geom_histogram(binwidth = 2, fill = "gray", color = "black") +
  facet_wrap(~ Sample_Size, scales = "free_x") +
  geom_vline(aes(xintercept = mean(population_distribution), color = "Population Mean"), linetype = "solid", size = 1,key_glyph = draw_key_path) +
  scale_color_manual(values = c("Population Mean" = "black")) +
  labs(
    title = "Central Limit Theorem Demonstration",
    x = "Sample Mean",
    y = "Frequency",
    color = ""
  ) +
  theme(legend.position = "bottom")
```

:::

:::


::: {.content-visible when-profile="script"}

The primary reason for the existence of the normal distribution in many real-world datasets is the {{< acr CLT >}} [@1981369198]. 
The {{< acr CLT >}} states that when you take a large enough number of random samples from any population, the distribution of the sample means will tend to follow a normal distribution, even if the original population distribution is not normal. 
This means that the normal distribution emerges as a statistical consequence of aggregating random data points.
This is shown in @fig-clt.

From $n=10000$ uniformly distributed data points (the *population*) ($min=1, max = 100$) either $2,10,50$ or $200$ samples are taken randomly (the *samples*).
For each of the samples the mean is calculated, resulting in $1000$ mean values for each ($2,10,50$ or $200$) sample size.
In @fig-clt the results from this numerical study are shown.
The larger the sample size, the closer the mean calculated [$\bar{x}$](index.qmd#mean-gloss)is to the population mean ([$\mu_0$](index.qmd#truemean-gloss)).
The effect is especially large on the standard deviation, resulting in a smaller standard deviation the larger the sample size is.

:::

## {{< acr LLN >}}

```{r}
#| label: fig-lln
#| out-width: 75%
#| fig-cap: The Law of Large Numbers in Action with die rolls as an example.
#| fig-pos: "H"

# Number of die rolls to simulate
num_rolls <- 1000

# Simulate die rolls
set.seed(123)  # For reproducibility
die_rolls <- sample(1:6, num_rolls, replace = TRUE)

# Calculate running average
running_average <- cumsum(die_rolls) / (1:num_rolls)

# Create a data frame
rolls_data <- data.frame(
  Roll_Number = 1:num_rolls,
  Die_Result = die_rolls,
  Running_Average = running_average
)

# Create a plot using ggplot2
ggplot(rolls_data, aes(x = Roll_Number)) +
  geom_line(aes(y = Running_Average)) +
  geom_hline(yintercept = 3.5, linetype = "dashed") +
  labs(
    title = "Law of Large Numbers",
    x = "Number of Die Rolls",
    y = "Running Average"
  ) +
  scale_y_continuous(breaks = c(1,2,3,3.5,4,5,6),
                     limits = c(1,6))


```

::: {.content-visible when-profile="script"}

The {{< acr CLT >}} states that as the size of a random sample increases, the sample average converges to the population mean. 
This law, along with the {{< acr CLT >}}, explains why the normal distribution frequently arises. 
When you take many small, independent, and identically distributed measurements and compute their averages, these averages tend to cluster around the true population mean, forming a normal distribution [@1981369198, @johnson1994continuous].

The [LLN](#lln) ar work is shown in @fig-lln.
A fair six-sided die is rolled 1000 times and the running average of the roll results after each roll is calculated. 
The resulting line plot shows how the running average approaches the expected value of $3.5$, which is the average of all possible outcomes of the die. 
The  line in the plot represents the running average
It fluctuates at the beginning but gradually converges toward the expected value of $3.5$. 
To emphasize this convergence, a dashed line indicating the theoretical expected value which is essentially the expected value applied to each roll. 
This visualization demonstrates the Law of Large Numbers, which states that as the number of trials or rolls increases, the *sample mean* (running average in this case) approaches the *population mean* (expected value) with greater accuracy, showing the predictability and stability of random processes over a large number of observations.

:::



## Population 

```{r}
#| label: population-001
#| out-width: 75%
#| fig-cap: An example for a population.
knitr::include_graphics(here::here("chapter000","002_Population.png"))
```

::: {.content-visible when-profile="script"}

In statistics, a population is the complete set of individuals, items, or data points that are the subject of a study.
Understanding populations and how to work with them is fundamental in statistical analysis, as it forms the basis for making meaningful inferences and drawing conclusions about the broader group being studied.
It is the complete collection of all elements that share a common characteristic or feature and is of interest to the researcher. 
The population can vary widely depending on the research question or problem at hand.
A populations *true mean* is depicted with [$\mu_0$](index.qmd#truemean-gloss) and the variance is depicted with [$\sigma_0^2$](index.qmd#truevariance-gloss).

:::

## Sample

```{r}
#| label: sample-001
#| out-width: 75%
#| fig-cap: A sample drawn from the population.
knitr::include_graphics(here::here("chapter000","003_Sample.png"))
```

::: {.content-visible when-profile="script"}

The key principles behind a sample include its role as a manageable subset of data, which can be chosen randomly or purposefully. 
Ideally, it should be representative, reflecting the characteristics and diversity of the larger population. 
Statistical techniques are then applied to this sample to make inferences, estimate population parameters, or test hypotheses. 
The size of the sample matters, as a larger sample often leads to more precise estimates, but it should be determined based on research goals and available resources. 
Various sampling methods, such as random sampling, stratified sampling, or cluster sampling, can be employed depending on the research objectives and population characteristics.
A samples *true mean* is depicted with [$\bar{x}$](index.qmd#mean-gloss) and the variance is depicted with [$sd$](index.qmd#sd-gloss).
:::

## Descriptive Statistics

::: {.content-visible when-profile="script"}

Descriptive statistics are used to summarize and describe the main features of a data set. 
They provide a way to organize, present, and analyze data in a meaningful and concise manner. 
Descriptive statistics do not involve making inferences or drawing conclusions beyond the data that is being analyzed.
Instead, they aim to provide a clear and accurate representation of the data set. 
Some common techniques and measures used in descriptive statistics include:

:::

### Example Data: The drive shaft exercise

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-drive-shaft-intro
#| out-width: 75%
#| fig-cap: The drive shaft specification.

knitr::include_graphics(here::here("chapter000","005_DriveShaft.png"))
```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-drive-shaft-pop-smpl
#| out-width: 95%
#| fig-cap: Difference between the population of ALL drive shafts and a sample of drive shafts.

knitr::include_graphics(here::here("chapter000","DriveShaft_Intro.png"))

```

:::

:::

### Measures of Central Tendency

::: {.content-visible when-profile="script"}

Measures of central tendency are essential in statistics because they provide a single value that summarizes or represents the center point or typical value of a dataset. The main reasons for using these measures include:

* Simplification of Data: They condense large sets of data into a single representative value, making the data easier to understand and interpret.

* Comparison Across Datasets: They allow for straightforward comparison between different groups or datasets by providing a common reference point.

* Foundation for Further Analysis: Many statistical techniques and models rely on an understanding of central tendency as a starting point, such as in regression analysis or hypothesis testing.

* Decision-Making: In fields such as economics, education, and public policy, central tendency helps inform decisions based on typical outcomes or behaviors (e.g., average income, median test scores).

* Identification of Patterns: They help identify patterns and trends over time, especially in time-series data or longitudinal studies.

:::

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-dat
#| out-width: 95%
#| fig-cap: Some drive shaft sample data in a 2D plot of sample index vs. variable value

knitr::include_graphics(here::here("chapter000","DriveShaft_central_tendency.png"))
```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-central-tend
#| out-width: 95%
#| fig-cap: Some drive shaft sample data in a 2D plot of sample index vs. variable value

knitr::include_graphics(here::here("chapter000","DriveShaft_ct_data.png"))
```

:::

:::

#### Mean 

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-mean-ds
#| out-width: 50%
#| fig-cap: A graphical depiction of the mean

knitr::include_graphics(here::here("chapter000","mean_ds.png"))

```

\begin{align}
\text{discrete: } \mu = \mathbb{E}[X] &= \sum_{i=1}^{n} x_i \, p_i \\
\text{continous: }\mu = \mathbb{E}[X] &= \int_{-\infty}^{\infty} x \, f(x) \, dx
\end{align}

:::

::: {.content-visible when-profile="slides"}


::: {.fragment .fade-in-then-out}

::: {style="font-size: 200%;"}

? $$\mathbb{E}[X]$$ ?

:::

:::

::: {.fragment .fade-in-then-out}

::: {style="font-size: 200%;"}

Expected Value!

Do we know it?

:::

:::

::: {.content-visible when-profile="slides"}


::: {.fragment .fade-in-then-out}

*Intuition:* The mean (expected value) represents the **center of mass** of the distribution. 
It is the value around which the random variable tends to cluster. 

* For a fair die, $X = 1,2,3,4,5,6$:
* $\mu_0 = \frac{1+2+3+4+5+6}{6} = 3.5$

:::

:::

:::

::: {.fragment .fade-in-then-out}

\begin{align}
\text{population:} \; \mu &= \frac{1}{N}\sum_i^{N} x_i \\
\text{sample:} \; \bar{x} &= \frac{1}{n}\sum_i^{n} x_i 
\end{align}

:::

:::

#### Median

```{r}
#| label: fig-median-ds
#| out-width: 95%
#| fig-cap: A graphical depiction of the median

knitr::include_graphics(here::here("chapter000","median_ds.png"))
```

\begin{align}
\text{population:} \; m &= 
\begin{cases}
x_{\left(\frac{N+1}{2}\right)} & \text{if } N \text{ is odd} \\
\frac{1}{2} \left( x_{\left(\frac{N}{2}\right)} + x_{\left(\frac{N}{2} + 1\right)} \right) & \text{if } N \text{ is even}
\end{cases} \\ 
\text{sample:} \; \tilde{x} &= 
\begin{cases}
x_{\left(\frac{n+1}{2}\right)} & \text{if } n \text{ is odd} \\
\frac{1}{2} \left( x_{\left(\frac{n}{2}\right)} + x_{\left(\frac{n}{2} + 1\right)} \right) & \text{if } n \text{ is even}
\end{cases} 
\end{align}

### Measures of Spread

```{r}
#| label: fig-spread
#| out-width: 95%
#| fig-cap: Spread, Dispersion, Variance ... many names for measuring variability of data

knitr::include_graphics(here::here("chapter000","spread_ds.png"))
```

::: {.content-visible when-profile="script"}

Measures of spread (also called measures of dispersion or variability) are essential in statistics to  provide information about the distribution of data — specifically, how much the data values differ from each other and from the central tendency. 

* Contextualizing Central Tendency: The mean or median alone does not give a complete picture of the data. Two datasets can have the same mean but very different spreads.

* Understanding Data Consistency: Measures of spread indicate how consistent or reliable the data are. A small spread suggests the values are closely clustered around the mean, while a large spread indicates greater variability and less predictability.

* Identifying Outliers: Large measures of spread may indicate the presence of outliers — values that are significantly different from others in the dataset. This can be important in quality control, risk assessment, and anomaly detection.

* Comparing Distributions: Spread allows for meaningful comparison between different datasets.

* Informing Statistical Models: Many statistical methods, such as regression, hypothesis testing, and confidence intervals, rely on measures of spread (like variance or standard deviation) to estimate error, assess significance, or make predictions.

:::

#### Range 

```{r}
#| label: fig-range-ds
#| out-width: 95%
#| fig-cap: A graphical depiction of the range

knitr::include_graphics(here::here("chapter000","range_ds.png"))
```

\begin{align}
\text{Range} = x_{\text{max}} - x_{\text{min}}
\end{align}

There is no difference in computing the range for the population or the sample

#### Variance 

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-variance-ds
#| out-width: 75%
#| fig-cap: A graphical depiction of the variance

knitr::include_graphics(here::here("chapter000","variance_ds.png"))
```

For a random variable $X$ with mean $\mu=\mathbb{E}\!(X)$:

\begin{align}
\mathrm{Var}(X) &= \mathbb{E}\!\big[(X - \mu)^2\big] \label{var} \\
\mathrm{Var}(X) &= \mathbb{E}[X^2] - \big(\mathbb{E}[X]\big)^2
\end{align}

:::

::: {.fragment .fade-in-then-out}

It is the **expected squared deviation** from the mean.

* Always non-negative.

* Units: square of the units of $X$

:::

:::

#### Standad Deviation

::: {.r-stack}

::: {.fragment .fade-out}

\begin{align}
\sigma_0 &= \sqrt{\mathrm{Var}}
\end{align}

:::

::: {.content-visible when-profile="slides"}

::: {.fragment .fade-in-then-out}

?

:::

::: {.fragment .fade-in-then-out}

**Interpretability!**

:::

:::

::: {.fragment .fade-in-then-out}

\begin{align}
\text{population: } \sigma &= \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2}\\
\text{sample: } sd &= \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{align}


:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-bessel-intro
#| out-width: 95%
#| fig-cap: Why do we divide by n-1 for the sample variance?

knitr::include_graphics(here::here("chapter000","bessel_intro.png"))
```

:::

:::


##### The Bessel's correction

::: {.r-stack}

::: {.fragment .fade-out}

The variance calculated from a sample has one {{< acr dof >}} less, then the population variance.

:::

::: {.content-visible when-profile="slides"}

::: {.fragment .fade-in-then-out}

::: {.r-fit-text}

<center>

{{< acr dof >}}

</center>

:::

:::

::: {.fragment .fade-in-then-out}

In statistics, a {{< acr dof >}} refers to the number of independent values or quantities that can vary in an analysis without violating any given constraints.

:::

:::

::: {.fragment .fade-in-then-out}

::: {style="font-size: 70%;"}

Imagine you have 5 candies, and you want to give them to 5 friends — one candy to each. 
You decide how to give the first candy, then the second, third, and fourth. 
But when you get to the last candy, you have no choice — you have to give it to the last friend, so everyone gets one.

That’s kind of like degrees of freedom in statistics. 
It means how many things you’re free to choose before something has to be a certain way.

So if you're working with 5 numbers, and they all have to add up to a certain total (like a mean), you can choose 4 of them freely, but the last one has to be whatever makes the total come out right. 
That’s why we say there are 4 degrees of freedom — 4 numbers you can choose any way you want.

:::

:::


::: {.fragment .fade-in-then-out}

::: {style="font-size: 80%;"}

$$\{2,4,6\}$$

* Mean: $\bar{x} = \frac{2+4+6}{3} = 4$
* Deviations: $-2,0,2$
* Squared Deviations: $4,0,4$
* Sum of squared deviations: $8$


**with Bessel's correction: ** $sd^2 = \frac{8}{3-1} = 4$

**without Bessel's correction: ** $sd^2 = \frac{8}{3} \approx 2.67$ (biased, underestimates variance)

:::

:::

::: {.fragment .fade-in-then-out}

When computing the variance from a sample, we need to calculate $\bar{x}$, which *uses up* one degree of freedom and biases our estimate

:::

:::

##### Bessel's correction with increasing sample size

```{r}
#| include: false

# Set seed for reproducibility
set.seed(42)

# Create a large population from which to sample
population <- rnorm(100000, mean = 0, sd = 1)

true_population_variance <- var(population) * (length(population) - 1) / length(population)

# Define sample sizes
sample_sizes <- c(2,10,20,30,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000)

# Initialize list to store results
results <- data.frame()

# Loop through sample sizes
for (n in sample_sizes) {
  sample <- sample(population, size = n, replace = FALSE)
  sample_mean <- mean(sample)
  ssq <- sum((sample - sample_mean)^2)
  
  biased_var <- ssq / n
  unbiased_var <- ssq / (n - 1)
  
  results <- rbind(results, data.frame(
    SampleSize = n,
    BiasedVariance = biased_var,
    UnbiasedVariance = unbiased_var,
    Difference = unbiased_var - biased_var
  ))
}

# Reshape data for ggplot
results_long <- results %>%
  pivot_longer(cols = c("BiasedVariance", "UnbiasedVariance"),
               names_to = "Type", values_to = "Variance") |> 
  mutate(
    diff_to_true_var = abs(1-Variance)
  )

```

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-bessel-large-n
#| out-width: 95%
#| fig-cap: The biased (n) variance is not as prcises as the unbiased (n-1) variance estimate. This effect decreases with increasing sample size.

results_long |> 
  ggplot(
    aes(
      x = SampleSize, 
      y = Variance, 
      color = Type)) +
  geom_line(
    size = 1
    ) +
  geom_hline(yintercept = 1)+
  geom_vline(xintercept = 30)+
  labs(
    title = "Biased (n) and unbiased (n - 1) variance estimates",
    subtitle = "population size: 100000, drawn with: seed = 42, mean = 0, sd = 1 from rnorm",
    x = "Sample Size", 
    y = "Estimated Variance",
    color = "Variance Type") +
  scale_x_log10()+
  scale_color_manual(values = c("steelblue", "firebrick")) +
  theme(legend.position = "top")

```

:::


:::



#### Percentiles, quantiles

::: {.r-stack}

::: {.fragment .fade-out}

* Percentiles: Divide data into $100$ equal parts. The $p$th percentile is the value below which p\% of the observations fall.

* Quantiles: Generalization of percentiles. The q-th quantile is the value below which a fraction q of the data falls. For example:

  * $0.25$ quantile: $25$th percentile - first quartile (Q1)

  * $0.50$ quantile: $50$th percentile - median

  * $0.75$ quantile: $75$th percentile - third quartile (Q3)

:::

::: {.fragment .fade-in}
  
```{r}
#| label: fig-percentile-quantile
#| out-width: 95%
#| fig-cap: A graphical depiction of various percentiles, quantiles

# Sample dataset
scores <- c(55, 67, 78, 81, 90, 95, 100, 45, 60, 72)
scores_df <- data.frame(score = scores)

# Compute percentiles
p <- quantile(scores, probs = c(0.25, 0.5, 0.75))

# Create plot
ggplot(scores_df, aes(x = score)) +
  geom_histogram(binwidth = 5, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = p[1], linetype = "dashed", size = 1) +
  geom_vline(xintercept = p[2], linetype = "dashed", size = 1) +
  geom_vline(xintercept = p[3], linetype = "dashed", size = 1) +
  labs(title = "Exam Scores with 25th, 50th, 75th Percentiles",
       x = "Score",
       y = "Count") +
  annotate("text", x = p[1], y = 2, label = "25th",  angle = 90, vjust = -0.5) +
  annotate("text", x = p[2], y = 2, label = "50th",  angle = 90, vjust = -0.5) +
  annotate("text", x = p[3], y = 2, label = "75th",  angle = 90, vjust = -0.5)+
  scale_y_continuous(breaks = c(0,1,2))

```
  
:::

:::

### Histogram

```{r}
#| label: fig-plot-sample-001
#| out-width: 95%
#| fig-cap: An example for descriptive statistics (histogramm)

df <- industRial::syringe_diameter %>% 
  tidyr::pivot_longer(cols = starts_with("Sample"),
               names_to = "sample",
               values_to = "diameter")

hist_bin_width <- (max(df$diameter)-min(df$diameter))/40

df %>% 
  ggplot(aes(x = diameter))+
  geom_histogram(
    color = "white",
    # binwidth = hist_bin_width,
    bins = 40,
    # binwidth = 0.0001594
    breaks = seq(min(df$diameter), max(df$diameter), (max(df$diameter)-min(df$diameter))/40)
    )+
  stat_bin(
    aes(
      y=after_stat(count),
      label=after_stat(count)
      ),
    bins = 40,
    breaks = seq(min(df$diameter), max(df$diameter), (max(df$diameter)-min(df$diameter))/40),
    geom="text",
    hjust = 0.5, 
    position = position_nudge(y = 0.2)
    )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
    labels = scales::number_format(accuracy = 1),
    breaks = seq(0,100,1)
  )+
  labs(
    x = "diameter in mm",
    y = "count",
    title = "A histogram of the syringe data."
  )+
  scale_x_continuous(
    expand = c(0.01,0,0.01,0),
    breaks = seq(min(df$diameter), max(df$diameter), (max(df$diameter)-min(df$diameter))/40),
    guide = guide_axis(angle = 45),
    labels = scales::number_format(accuracy = 0.001)
  )
  

```

::: {.content-visible when-profile="script"}

An example for descriptive statistics is shown in @fig-plot-sample-001 as a histogram.
It shows data from a company that produces pharmaceutical syringes, taken from @Ram2021.
During the production of those syringes, the so called *barrel diameter* is a critical parameter to the function of the syringe and therefore of special interest for the [Quality Control](index.qmd#QC).

A histogram as shown in @fig-plot-sample-001 shows the data of `r nrow(df)` measurements during the [QC](index.qmd#QC).
On the `x-axis` the *barrel diameter* is shown, while the count of each *binned* diameter is shown on the `y-axis`.
The binning and of data is a crucial parameter for such a plot, because it already changes the appearance and width of the bars.
Binning is a trade-off between visibility and readability.

:::

::: {.content-visible when-profile="slides"}

### Bad histograms

```{r}
#| label: fig-plot-sample-002
#| out-width: 95%
#| fig-cap: A bad example for histograms.


plt1 <- df %>% 
  ggplot(aes(x = diameter))+
  geom_histogram(color = "white",bins = 1)+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    x = "diameter in mm",
    y = "count",
    title = "A histogram of the syringe data\nwith only one bin."
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )

plt2 <- df %>% 
  ggplot(aes(x = diameter))+
  geom_histogram(
    color = "white",
   breaks = seq(min(df$diameter), max(df$diameter), (max(df$diameter)-min(df$diameter))/4000)
   )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    x = "diameter in mm",
    y = "count",
    title = "A histogram of the syringe data\nwith too many bins."
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )

plt1+plt2  

```

:::

### Density plot

```{r}
#| label: fig-plot-density
#| out-width: 95%
#| fig-cap: An example for a density plot for the syringe data (barrel diameter).

df %>% 
  ggplot(aes(x = diameter, after_stat(scaled)))+
  geom_density(fill="grey")+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    x = "diameter in mm",
    y = "density",
    title = "A density plot of the syringe data."
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )
  

```

::: {.content-visible when-profile="script"}

Density plots are another way of displaying the statistical distribution of an underlying dataset.
The biggest strength of those plots is, that no binning is necessary in order to show the data.
The limitation of this kind of plot is the interpretability.
An example of a density plot for the syringe data is shown in @fig-plot-density.
On the `x-axis` the syringe barrel diameter is shown (as in a histogram).
The `y-axis` in contrast does not display the count of a binned category, but rather the {{< acr PDF >}} for the specific diameter.
The grey area under the density curve depicts the probability of a syringe diameter to appear in the data.
The complete area under the curve equals to $1$ meaning that a certain diameter is sure to appear in the data.

:::

### Boxplot

```{r}
#| label: fig-plot-sample-003
#| out-width: 95%
#| fig-cap: A boxplot of the same syringe data combined with the according histogram.
#| fig-pos: "H"


df %>% 
  ggplot(aes(x = diameter))+
  geom_histogram(color = "white",alpha = 0.4)+
  geom_boxplot(aes(y = 7),width = 4,lwd = 1,outlier.size = 1)+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    x = "diameter in mm",
    y = "count",
    title = "A histogram of the syringe data\nwith an overlayed boxplot."
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )
  

```

::: {.content-visible when-profile="script"}

It is very common to include and inspect measures of central tendency in the graphical depiction of data.
A `boxplot`, also known as a box-and-whisker plot, is a very common way of doing this.
A `boxplot` is a graphical representation of a dataset's distribution. 
It displays the following key statistics:

1. Median (middle value).
2. Quartiles ($25^{th}$ and $75^{th}$ percentiles), forming a box.
3. Minimum and maximum values (whiskers).
4. Outliers (data points significantly different from the rest).

The syringe data in boxplot form is shown in @fig-plot-sample-003 as an overlay of the histogram plot before.
Boxplots are useful for quickly understanding the central tendency, spread, and presence of outliers in a dataset, making them a valuable tool in data analysis and visualization.

:::


### Average, Standard deviation and Range

```{r}
#| label: fig-plot-sample-031
#| out-width: 95%
#| fig-cap: A histogram of the syringe data with mean, standard deviation and range.


df %>% 
  ggplot(aes(x = diameter))+
  geom_histogram(color = "white",alpha = 0.4)+
  geom_point(aes(x = mean(diameter),y = 7),size = 5,shape = 8)+
  geom_errorbarh(aes(xmin = (mean(diameter)-sd(diameter)),
                     xmax = (mean(diameter)+sd(diameter)),
                     y = 7,linetype = "standard deviation"))+
  geom_errorbarh(aes(xmin = (min(diameter)),
                     xmax = (max(diameter)),
                     y = 3,linetype = "range"))+
  scale_linetype_manual(values = c("standard deviation" = "solid",
                                   "range" = "dashed"))+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    x = "diameter in mm",
    y = "count",
    title = "A histogram of the syringe data\nwith mean, standard deviation and range",
    linetype = "type of spread"
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  theme(legend.position = "bottom")
  

```

::: {.content-visible when-profile="script"}

Very popular measures of central tendency include the *average* (mean) and the *standard deviation* (variance) of a dataset.
The computed mean from an actual dataset is depicted with [$\bar{x}$](index.qmd#mean-gloss) and calculated via \eqref{mean}.

\begin{align}
\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i \label{mean}
\end{align}

With [$n$]{index.qmd#n-gloss} being the number of datapoints and [$x_i$](index.qmd#xi-gloss) being the datapoints.
The *mean* is therefore the sum of all datapoints divided by the total number $n$ of all datapoints.
It is not to be confused with the true mean [$\mu_0$](index.qmd#truemean-gloss) of a population.

The computed *standard deviation* from an actual dataset is depicted with [$sd$]{index.qmd#sd-gloss} and calculated via \eqref{sd}.

\begin{align}
sd = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2} \label{sd}
\end{align}

The *standard deviation* can therefore be explained as the square root of the sum of all differences of each individual datapoints to the mean of a dataset divided by the number of datapoints.
It is not to be confused with the true variance [$\sigma_0^2$](index.qmd#truevariance-gloss) of a population.
The variance of a dataset can be calculated via \eqref{var2}.

\begin{align}
\sigma = sd^2 \label{var2}
\end{align}

The *range* from an actual dataset is depicted with [$r$](index.qmd#range-gloss) and calculated via \eqref{range}.

\begin{align}
r = \max(x_i) - \min(x_i) \label{range}
\end{align}

The *range* can therefore be interpreted as the range from minimum to maximum in a dataset.

:::

## Visualizing Groups {#sec-vis-grps}

### Boxplots

```{r}
#| label: fig-groups-boxplot
#| out-width: 95%
#| fig-cap: Boxplots of the syringe data with the samples as groups.

df %>% 
  ggplot(aes(x = diameter,y = sample))+
  geom_boxplot()+
  labs(
    x = "diameter in mm",
    title = "Boxplots of the syringe data with samples as groups"
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )
  

```

::: {.content-visible when-profile="script"}

The methods described above are especially useful when it comes to visualizing groups in data.
The data is discretized and the information density is increased.
As with every discretization comes also a loss of information.
It is therefore strongly advised to choose the right tool for the job.

If the underlying distribution of the data is unknown, a good start to visualize groups within data is usually a boxplot as shown in @fig-groups-boxplot.
The syringe data from @Ram2021 contains six different groups, one for every sample drawn. 
Each sample consists of 25 observations in total.
On the `x-axis` the *diameter* in mm is shown, the `y-axis` depicts the sample number.
The `boxplots` are then drawn as described above (median, $25^{th}$ and $75^{th}$ percentile box, $5^{th}$ and $95^{th}$ whisker).
The $25^{th}$ and $75^{th}$ percentile box is also known as the {{< acr IQR >}}.

:::

### Mean and standard deviation plots

```{r}
#| label: fig-groups-mean-sd
#| out-width: 95%
#| fig-cap: Mean and standard deviation plots of the groups in the dataset.

df_grouped <- df %>% 
  group_by(sample) %>% 
  summarise(
    mean_d = mean(diameter),
    sd_d = sd(diameter)
  )

df_grouped %>% 
  ggplot(aes(x = mean_d,y = sample))+
  geom_point(shape = 8,size = 5)+
  geom_errorbarh(aes(xmin = mean_d - sd_d,xmax = mean_d +sd_d,y=sample),
                 height = 0.35)+
  labs(
    x = "diameter in mm",
    title = "Mean and standard deviation plot of the groups"
  )+
  scale_x_continuous(
    expand = c(0.05,0,0.05,0)
  )
  

```

::: {.content-visible when-profile="script"}

If the data follows a normal distribution, showing the mean and standard deviation for each group is also very common.
For the syringe dataset, this is shown in @fig-groups-mean-sd.
The plot follows the same logic as for the boxplots (`x-axis-data`, `y-axis-data`), but the data itself shows the mean with a $\times$-symbol, as the length of the horizontal errorbars accords to $\bar{x} \pm sd(x)$.

:::


### Half-half plots

```{r}
#| label: fig-groups-halves
#| out-width: 95%
#| fig-cap: Half-half plots that incooperate different types of plots

df %>% 
  ggplot(aes(y = diameter,x = sample))+
  geom_half_violin(side = "l")+
    geom_dotplot(binaxis = "y", method="histodot", stackdir="up")+
  labs(
    x = "diameter in mm",
    title = "Half-Half plots including violin plots (PDF)\nand dotplots"
  )+
  scale_x_discrete(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0.05,0,0.05,0)
  )

```

::: {.content-visible when-profile="script"}

Boxplots and mean-and-standard-deviation plots sometimes hide some details within the data, that may be of interest or simply important.
Half-half plots, as shown in shown in @fig-groups-halves, incorporate different plot mechanisms.
The left half shows a violin plot, which outlines the underlying distribution of the data using the {{< acr PDF >}}.
This is very similar to a density plot.
The right half shows the original data points and give the user a visible clue about the sample size in the data size.
Note that the `y`-position of the points is jittered to counter *overplotting*.
Details can be found in @Tiedemann2022.

:::

### Ridgeline plots

```{r}
#| label: fig-groups-ridge
#| out-width: 95%
#| fig-cap: Ridgeline plots for distributions within groups.

df %>% 
  ggplot(aes(x = diameter,y = sample))+
  geom_density_ridges()+
  labs(
    x = "diameter in mm",
    title = "Ridgeline plots showing\nthe PDF for every group."
  )+
  scale_x_continuous(
    expand = c(0.05,0,0.05,0)
  )+
  scale_y_discrete(
    expand = c(0,0,0,0)
  )

```

::: {.content-visible when-profile="script"}

@fig-groups-ridge shows so called *ridgeline* plots as explained in @Wilke2022.
They are in essence density plots that use the `y-axis` to differentiate between the groups.
On the `x-axis` the density of the underlying dataset is shown.
More info on the creation of these plots and graphics is available in @Wickham2016 as well as @RGraphGallery.

:::

## The drive shaft exercise

### Introduction

```{r}
#| label: fig-drive-shaft-spec
#| out-width: 95%
#| fig-cap: The drive shaft specification.

knitr::include_graphics(here::here("chapter000","005_DriveShaft.png"))
```

::: {.content-visible when-profile="script"}

A drive shaft is a mechanical component used in various vehicles and machinery to transmit rotational power or torque from an engine or motor to the wheels or other driven components. 
It serves as a linkage between the power source and the driven part, allowing the transfer of energy to propel the vehicle or operate the machinery.

1. Material Selection: Quality steel or aluminum alloys are chosen based on the specific application and requirements.

2. Cutting and Machining: The selected material is cut and machined to achieve the desired shape and size. 
Precision machining is crucial for balance and performance.

3. Welding or Assembly: Multiple sections may be welded or assembled to achieve the required length. 
Proper welding techniques are used to maintain structural integrity.

4. Balancing: Balancing is critical to minimize vibrations and ensure smooth operation. 
Counterweights are added or mass distribution is adjusted.

5. Surface Treatment: Drive shafts are often coated or treated for corrosion resistance and durability. 
Common treatments include painting, plating, or applying protective coatings.

6. Quality Control: Rigorous quality control measures are employed to meet specific standards and tolerances. 
This includes dimensional checks, material testing, and defect inspections.

7. Packaging and Distribution: Once quality control is passed, drive shafts are packaged and prepared for distribution to manufacturers of vehicles or machinery.

The end diameter of a drive shaft is primarily determined by its torque capacity, length, and material selection. 
It needs to be designed to handle the maximum torque while maintaining structural integrity and flexibility as required by the specific application.
For efficient load transfer, there are ball bearings mounted on the end diameter.
Ball bearings at the end diameter of a drive shaft support its rotation, reducing friction. 
They handle axial and radial loads, need lubrication for longevity, and may include seals for protection. 
Proper alignment and maintenance are crucial for their performance and customization is possible to match specific requirements.

The end diameter of the drive shaft shall be $\varnothing 12\pm0.1mm$ (see @fig-drive-shaft-spec).
This example will haunt us the rest of this lecture.

:::

### Visualizing all the Data

```{r}
#| label: fig-ds-dist
#| layout-ncol: 2
#| out-width: 95%
#| fig-cap: The raw data of the measured drive shaft diameter.
#| fig-subcap: 
#| - The drive shaft data shown in a histogram.
#| - The drive shaft data shown in a density plot.

load(here("data","drive_shaft_data.Rdata"))

drive_shaft %>% 
  ggplot(aes(x = diameter))+
  geom_histogram()+
  geom_boxplot(aes(y = 30),width = 10)+
  labs(
    title = "A histogram of the drive shaft data"
  )+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0.05,0))+
  labs(y = "count")
  
drive_shaft %>% 
  ggplot(aes(x = diameter))+
  geom_density()+
  labs(
    title = "A density plot of the drive shaft data"
  )+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0.05,0))

```

::: {.content-visible when-profile="script"}

First, some descriptive statistics of $N=500$ produced drive shafts are shown in @tbl-ds-sum ($\bar{x}(sd), median(IQR)$).
This first table does not tell us an awful lot about the sample, apart from the classic statistical measures of central tendency and spread.

```{r}
#| label: tbl-ds-sum
#| tbl-cap: The summary table of the drive shaft data


load(here("data","drive_shaft_data.Rdata"))

# drive_shaft <- drive_shaft %>% 
#   filter(group == "group01")

drive_shaft %>% 
  tbl_summary(
    # by = group,
    include = diameter,
      statistic = list(
      all_continuous() ~ "{mean} ({sd}), {median} ({IQR})"
      # all_categorical() ~ "{n} / {N} ({p}%)"
    ),
  ) %>% 
    modify_header(label ~ "**Variable**") %>% 
  as_gt() 
  # tab_footnote(footnote = "mean (sd), median (IQR)")
  

```

In @fig-ds-dist the data and the distribution thereof is visualized using different modalities.
The complete `drive shaft data` is shown as a histogram (@fig-ds-dist-1) and as a density plot (@fig-ds-dist-2).
A single boxplot is plotted over the histogram data in @fig-ds-dist-1, providing a link to @tbl-ds-sum (median and {{< acr IQR >}}).
One important conclusion may be draw from those plots already:
There may be more than one dataset hidden inside the data. 
We will explore this possibility further.

:::

### Visualizing groups within the data

```{r}
#| label: fig-ds-grps
#| layout-ncol: 2
#| out-width: 95%
#| fig-cap: The raw data of the measured drive shaft diameter.
#| fig-subcap: 
#| - The groups visualized as boxplots (including the specification)
#| - The groups visualized as ridgeline plots

load(here("data","drive_shaft_data.Rdata"))

drive_shaft %>% 
  ggplot(aes(y = diameter,x = group))+
  geom_rect(xmin = -Inf, xmax = Inf, ymin = 11.9, ymax = 12.1,alpha = 0.1,fill = "azure2")+
  geom_jitter(alpha = 0.25,width = 0.2)+
  geom_boxplot(alpha = 0.5)+
  # geom_half_boxplot(alpha = 0.5)+
  # geom_half_violin(side = "r",alpha=0.5)+
  geom_hline(
    data = data.frame(spec = 12.0),
    aes(yintercept = spec,linetype = "nominal")
    )+
  geom_hline(
    data = data.frame(lo = 11.9),
    aes(yintercept = lo,linetype = "lower limit")
    )+
  geom_hline(
    data = data.frame(hi = 12.1),
    aes(yintercept = hi,linetype = "upper limit")
    )+
  scale_linetype_manual(
    values = c("nominal" = "solid", "lower limit" = "dashed", "upper limit" = "dashed")
      )+
  labs(
    title = "Drive shaft measurements and specification",
    x = "",
    y = "diameter in mm",
    linetype = ""
  )+
  theme(legend.position = "bottom")

drive_shaft %>% 
  ggplot(aes(x = diameter, y = group))+
  geom_density_ridges()+
  geom_vline(aes(xintercept = 12,linetype = "nominal"),key_glyph = draw_key_path)+
  geom_vline(aes(xintercept = 11.9,linetype = "lower limit"),key_glyph = draw_key_path)+
  geom_vline(aes(xintercept = 12.1,linetype = "upper limit"),key_glyph = draw_key_path)+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_discrete(expand = c(0,0,0.05,0))+
  labs(
    title = "Drive shaft measurement and specification",
    y = "",
    linetype = ""
      )+
  scale_linetype_manual(
    values = c("nominal" = "solid", "lower limit" = "dashed", "upper limit" = "dashed")
      )+
  theme(legend.position = "bottom")
  

```

::: {.content-visible when-profile="script"}

Fortunately for us, the groups that may be hidden within the data are marked in the orginal dataset and denoted as `group0x`.
Unfortunately for us, it is not known (purely from the data) how these groups come about.
Because we did get the dataset from a colleague, we need to investigate the *creation* of the dataset even further.
This is an important point, for without knowledge about the history of the data, it is *impossible* or at least *unadvisable* to make valid statements about the data.
We will go on with a table of summary statistics, see @tbl-ds-sum-grps.
Surprisingly, there are five groups hidden within the data, something we would no be able to spot from the raw data alone.


```{r}
#| label: tbl-ds-sum-grps
#| tbl-cap: The group summary table of the drive shaft data


load(here("data","drive_shaft_data.Rdata"))

drive_shaft <- drive_shaft %>%
  pivot_wider(names_from = "group",values_from = "diameter")

drive_shaft %>% 
  tbl_summary(
    # by = group,
    include = starts_with("group"),
      statistic = list(
      all_continuous() ~ "{mean} ({sd}), {median} ({IQR})"
      # all_categorical() ~ "{n} / {N} ({p}%)"
    ),
  ) %>% 
    modify_header(label ~ "**Variable**") %>% 
  as_gt() 
  # tab_footnote(footnote = "mean (sd), median (IQR)")
  

```

Again, the table is good to have, but not as engagingi for ourself and our co-workers to look at.
In order to make the data more approachable, we will use some techniques shown in @sec-vis-grps.

First in @fig-ds-grps-1 the raw data points are shown as points with overlayed boxplots.
On the `x-axis` the groups are depicted, while the {{< acr PoI >}} (in this case the *end diameter* of the drive shaft) is shown on the `y-axis`.
Because we are interested how the manufactured drive shafts behave {{< acr wrt >}} the specification limit, the `nominal` value as well as the `uppper` and the `lower` specification limit is also shown in the plot as horizontal lines.

In @fig-ds-grps-2 the data is shown as ridgeline density plots.
On the `x-axis` the diameter is depiected, while the `y-axis` shows two types of data.
First, the groups $1\ldots5$ are shown. 
For the individual groups, the probability is depicted as a line, therefore indicating which values are most probable in the given group.
Again, because we are interested how the  manufactured drive shafts behave .w.r.t the specification limit, the `nominal` value as well as the `uppper` and the `lower` specification limit is also shown in the plot as vertical lines.

:::


