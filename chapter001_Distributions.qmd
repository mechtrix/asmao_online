---
title: "Statistical Distributions"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(gghalves)
library(distributional)
library(here)
library(ggthemes)
library(ggExtra)
library(ggh4x)
library(ggridges)
library(ggdist)
library(tidydice)
library(patchwork)
library(gtsummary) 
library(gt) 



ggplot2::theme_set(
  ggthemes::theme_few(base_size = 15)
  )

```

## Why distributions?

<center>
Distributions are *models* of the *reality*.
</center>

```{r}
#| label: fig-intro
#| out-width: 95%
#| fig-cap: The model is perfect, but the real world is not.

# Generate data for a standard normal distribution (mean = 0, sd = 1)
x <- seq(-4, 4, length.out = 1000)  # Range of x-values (z-scores)
y <- dnorm(x, mean = 0, sd = 1)    # Probability density function (PDF)

# Create a data frame
df <- data.frame(x = x, y = y)

# Plot the normal distribution
plt_model <- ggplot(df, aes(x = x, y = y)) +
  geom_area(fill = "lightblue", alpha = 0.6) +  # Shaded area under the curve
  geom_line(color = "darkblue", size = 1) +     # Line for the curve
  # geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +  # Mean line
  labs(
    title = "Model"
  ) +
  theme_void()+
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  )

# Set seed for reproducibility
set.seed(123)

# Simulate 1000 random data points from a normal distribution (mean = 50, sd = 10)
simulated_data <- data.frame(
  values = rnorm(1000, mean = 50, sd = 10)
)

# Create histogram with ggplot2
plt_data <- ggplot(simulated_data, aes(x = values)) +
  geom_histogram(
    bins = 30,                     # Number of bins
    fill = "skyblue",             # Fill color
    color = "black",              # Border color
    alpha = 0.7                   # Transparency
  ) +
  labs(
    title = "real World"
  ) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  )

plt_model+plt_data

```


## Types of data

::: {.v-center-container}

```{r}
#| label: fig-datatypes
#| out-width: 95%
#| fig-cap: Data can be classified as different types.
#| fig-pos: "H"
knitr::include_graphics(here::here("chapter001","013_DataTypes.png"))
```

:::

::: {.content-visible when-profile="script"}
1. **Nominal Data:**
   - Description: Nominal data represents categories with no inherent order or ranking.
   - Examples: Colors, gender, or types of fruits.
   - Characteristics: Categories are distinct, but there is no meaningful numerical value associated.

2. **Ordinal Data:**
   - Description: Ordinal data has categories with a meaningful order or ranking, but the intervals between them are not consistent or measurable.
   - Examples: Educational levels (e.g., high school, bachelor's, master's), customer satisfaction ratings (e.g., low, medium, high).
   - Characteristics: The order is significant, but the differences between categories are not precisely quantifiable.

3. **Discrete Data:**
   - Description: Discrete data consists of separate, distinct values, often counted in whole numbers and with no intermediate values between them.
   - Examples: Number of students in a class, number of cars in a parking lot.
   - Characteristics: The data points are distinct and separate; they do not have infinite possible values within a given range.

4. **Continuous Data:**
   - Description: Continuous data can take any value within a given range and can be measured with precision.
   - Examples: Height, weight, temperature.
   - Characteristics: Values can be any real number within a range, and there are theoretically infinite possible values within that range.

:::

### Nominal Data

```{r}
#| label: fig-nomialdata
#| out-width: 95%
#| fig-cap: Some example for nominal data.

knitr::include_graphics(here::here("chapter001","014_NominalData.png"))

```

::: {.content-visible when-profile="script"}

Nominal data is a type of data that represents categories or labels without any specific order or ranking. 
These categories are distinct and non-numeric. 
For example, colors, types of fruits, or gender (male, female, other) are nominal data. 
Nominal data can be used for classification and grouping, but mathematical operations like addition or subtraction do not make sense in this context.

:::

### Ordinal Data

```{r}
#| label: fig-ordinaldata
#| out-width: 95%
#| fig-cap: Some example for ordinal data.

knitr::include_graphics(here("chapter001","015_OrdinalData.png"))

```

::: {.content-visible when-profile="script"}

Ordinal data represents categories that have a specific order or ranking. 
While the categories themselves may not have a consistent numeric difference between them, they can be arranged in a meaningful sequence.
A common example of ordinal data is survey responses with options like "strongly agree," "agree," "neutral," "disagree," and "strongly disagree." 
These categories indicate a level of agreement, but the differences between them may not be uniform or measurable.

:::

### Discrete Data

```{r}
#| label: fig-discretedata
#| out-width: 95%
#| fig-cap: Some example for discrete data.

knitr::include_graphics(here("chapter001","016_DiscreteData.png"))

```

::: {.content-visible when-profile="script"}

Discrete data consists of distinct, separate values that can be counted and usually come in whole numbers. 
These values can be finite or infinite, but they are not continuous. 
Examples include the number of students in a class, the count of cars in a parking lot, or the quantity of books in a library. 
Discrete data is often used in counting and can be represented as integers.

One quote in the literature about discrete data, shows how difficult the classification of data types can become (@Bibby_1980):
“... All actual sample spaces are discrete, and all observable random variables have discrete distributions. 
The continuous distribution is a mathematical construction, suitable for mathematical treatment, but not practically observable. ...”

:::



### Continous Data

```{r}
#| label: fig-continousdata
#| out-width: 95%
#| fig-cap: Some example for continous data.

knitr::include_graphics(here("chapter001","017_ContinousData.png"))


```

::: {.content-visible when-profile="script"}

Continuous data encompasses a wide range of values within a given interval and can take on any real number. 
There are infinite possibilities between any two points in a continuous dataset, making it suitable for measurements with high precision.
Examples of continuous data include temperature, height, weight, and time. 
It is important to note that continuous data can be measured with decimals or fractions and is not limited to whole numbers.

:::

## Uniform Distribution

### Fairness

<center>

If you had to guess a number between 1 and 10 with no other information, what is the **fairest** way to assign probabilities to each number? 

What if the range were 0 to 1 instead?

</center>


::: {.fragment .fade-in}

<center>

Every outcome is **equally** likely

</center>


:::


### Dice

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-unif-die
#| out-width: 35%
#| fig-cap: The probability for every face to turn up during a roll is uniformly distributed.

knitr::include_graphics(here("chapter001","die.png"))


```

:::

::: {.fragment .fade-in}

```{r}
#| label: fig-unif-pmf
#| out-width: 95%
#| fig-cap: A plot of the probabilities that every face of a die turns up. They are all the same.
#| 
df <- data.frame(
  face = factor(1:6),
  p    = rep(1/6, 6)
)

ggplot(df, aes(x = face, y = p)) +
  geom_col(fill = "#2C7FB8") +
  geom_text(aes(label = "1/6"), vjust = -0.4, size = 4) +
  scale_y_continuous(limits = c(0, 0.22), expand = expansion(mult = c(0, 0.05))) +
  labs(
    title = "PMF of a Fair Six-Sided Die",
    x = "Die face",
    y = "Probability"
  ) 

```

:::

:::

### Types of uniform distributions

* All outcomes in the range are *equally probable*
* The "range" can be:
  * a finite set of values (discrete)
  * a continuous interval (continuous)
  
\begin{align}
X \sim \mathrm{Uniform}(a,b) \text{ continuous}\\
X \sim \mathrm{Uniform}\{a,a+1,\ldots,b\} \text{ discrete}
\end{align}


### Visual comparison of discrete and continuous

```{r}
#| label: fig-unif-pmf-pdf
#| out-width: 95%
#| fig-cap: A comparison of continnuous and discrete uniformly distributed data. 

# Parameters
min_val <- 1 # Minimum value
max_val <- 10 # Maximum value
n_discrete <- 10 # Number of discrete points
n_continuous <- 1000 # Points for continuous curve

# Create data frames
discrete_data <- data.frame(
  x = min_val:max_val,
  y = rep(1 / n_discrete, n_discrete),
  type = "Discrete Uniform"
)

continuous_data <- data.frame(
  x = seq(min_val, max_val, length.out = n_continuous),
  y = dunif(seq(min_val, max_val, length.out = n_continuous), min_val, max_val),
  type = "Continuous Uniform"
)

# Combine data for plotting
plot_data <- rbind(
  transform(discrete_data, distribution = "Discrete"),
  transform(continuous_data, distribution = "Continuous")
)

# Create plot
ggplot() +
  # Continuous uniform distribution
  geom_line(
    data = subset(plot_data, distribution == "Continuous"),
    aes(x = x, y = y, color = type),
    size = 1
  ) +
  # Discrete uniform distribution
  geom_point(
    data = subset(plot_data, distribution == "Discrete"),
    aes(x = x, y = y, color = type),
    size = 3
  ) +
scale_x_continuous(
breaks = c(1,2,3,4,5,6,7,8,9,10)
  )+
  geom_segment(
    data = subset(plot_data, distribution == "Discrete"),
    aes(x = x, xend = x, y = 0, yend = y, color = type),
    linetype = "dashed",
    size = 0.5
  ) +
  # Aesthetics
  scale_color_manual(
    values = c("Continuous Uniform" = "blue", "Discrete Uniform" = "red")
  ) +
  labs(
    title = "Discrete vs Continuous Uniform Distribution",
    x = "Value",
    y = "Probability/Density",
    color = "Distribution Type"
  ) +
  theme(legend.position = "top")

```



### Core properties

```{r}
#| label: tbl-unif-prop
#| tbl-cap: The core properties between the discrete and continuous disitrbution are very similar.
#| 
# Load required packages
library(gt)
library(dplyr)

# Create the data frame for the table
uniform_table <- tibble(
  Property       = c("Probability", "Symmetry", "Intuition", "Use Cases"),
  Discrete       = c(
    "Equal for each outcome.",
    "Mean = midpoint of the range.",
    html("'Fair die' with <i>n</i> sides."),
    "Counting problems (e.g., dice, cards)."
  ),
  Continuous     = c(
    "Equal density over the interval.",
    "Mean = midpoint of the interval.",
    html("'Fair spinner' on a line segment."),
    "Measuring problems (e.g., time, space)."
  )
)

# Create the gt table with custom formatting
gt_uniform_table <- uniform_table %>%
  gt() %>%
  tab_header(
    title = md("**Core Properties: Discrete vs. Continuous Uniform Distributions**"),
    subtitle = md("*Equal probability everywhere, but for different types of data*")
  ) %>%
  cols_label(
    Property = md("**Property**"),
    Discrete = md("**Discrete Uniform**"),
    Continuous = md("**Continuous Uniform**")
  ) %>%
  fmt_markdown(columns = everything()) %>%
  tab_style(
    style = cell_text(weight = "bold", color = "white", align = "center"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body()
  ) %>%
  tab_style(
    style = cell_fill(color = "#f0f0f0"),
    locations = cells_body(columns = Discrete)
  ) %>%
  tab_style(
    style = cell_fill(color = "#e6f3ff"),
    locations = cells_body(columns = Continuous)
  ) %>%
  tab_options(
    table.font.size = px(16),
    heading.title.font.size = px(18),
    heading.subtitle.font.size = px(16)
  ) %>%
  tab_source_note(
    md("*Note: Both distributions share the 'fairness' property but apply to different data types.*")
  )

# Display the table
gt_uniform_table

```

## {{< acr PMF >}}


### But why?

* {{< acr PMF >}}s provide a way to calculate and assign probabilities to each distinct outcome.
* Each outcome is assigned a probability that corresponds to its position in the total number of outcomes.

### Basics

::: {.fragment .fade-in}

... assigns each outcome the same probability that always sum up to $1$ (example: six-sided die)

:::

::: {.fragment .fade-in}

$X \sim \mathrm{Uniform}\{a,b\}$ where $a$ and $b$ are integers and $a\leq b$

\begin{align}
f(k) = P (X = k)
\end{align}

:::

::: {.fragment .fade-in}

* $k$ meaning a specific value that $X$ can take (e.g., $k = 0,1,2, ...$)
* $f(k)$ is the probability that $X$ equals $k$

:::

### Key Properties 

1. $0 \leq f(k) \leq 1$ for all $k$
2. The of probabilities over all possible $k$ must equal $1$

\begin{align}
\sum_{all \; k}f(k) = 1
\end{align}

### 6-sided die

::: {.fragment .fade-in}

\begin{align}
X &\sim \mathrm{Uniform}\{a,a+1, ..., b\} \\
X &\sim \mathrm{Uniform}\{1,2,3,4,5,6\}
\end{align}

:::

::: {.fragment .fade-in}

Number of outcomes:

\begin{align}
N &= b-a+1\\
N &= 6-1+1=6
\end{align}

:::

::: {.fragment .fade-in}

Equal probability for each outcome: $f(k) = P (X = k)$

\begin{align}
f(a) = f(a+1)= ... = f(b) = c \text{ (some constant)}
\end{align}

:::

---


Sum of all probabilities must equal 1

\begin{align}
\sum_{k = a}^b f(k) = \underbrace{c+c+ ... +}_{N\text{times}} = N \cdot c = 1
\end{align}

::: {.fragment .fade-in}

Solving for $c$

\begin{align}
c = \frac{1}{N} = \frac{1}{b-a+1}
\end{align}

:::

---

::: {.fragment .fade-in}

Formal {{< acr PMF >}}:

\begin{align}
f(k) = \begin{cases}
\frac{1}{b - a + 1} & \text{if } k \in \{a, a+1, \dots, b\}, \\
0 & \text{otherwise.}
\end{cases}
\end{align}

:::

### Exercise

A tool changer in production randomly selects one of four tools (labeled 1 to 4) with equal probability.

* Describe the formal set in $X \sim ...$

::: {.fragment .fade-in}

$X \sim \mathrm{Uniform}\{1,2,3,4\}$

:::

* Formally calculate the number of outcomes ($N$)

::: {.fragment .fade-in}

$N = 4-1+1 = 4$

:::

---

* Describe the {{< acr PMF >}} formally

::: {.fragment .fade-in}

\begin{align}
f(k) = \begin{cases}
\frac{1}{4} & \text{if } k \in \{1,2,3,4\}, \\
0 & \text{otherwise.}
\end{cases}
\end{align}

:::

* What is the probability to select tool $2$

::: {.fragment .fade-in}

$P(X = 2) = f(2) = \frac{1}{4} = 0.25$

:::

### Summary {{< acr PMF >}}

* The {{< acr PMF >}} describes the probability distribution of a discrete random variable. 

* The probabilities associated with all hypothetical values must be non-negative and sum up to 1.

* A {{< acr PMF >}} is specific to *discrete* random variables, while a {{< acr PDF >}} is associated with continuous random variables.

* Unlike a PDF, which requires integration over an interval, the PMF **directly** provides probabilities for individual values.

* “mass” is conserved (similar to how physical mass is conserved).

## Binomial Distribution

### Classroom example

::: {.r-stack}

::: {.fragment .fade-out}

If we roll a fair six-sided die 10 times, how many times do you expect to get a specific outcome?

* 10 trials
* $P?$ to roll a $6$ (expected value)?
* record the number of times a specific outcome occurs
* repeat for 2-4 students

:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-sim-dice
#| out-width: 75%
#| fig-cap: Simulated die rolls (The die is rolle $10$ times, the experiment is repeated $5$ times)

roll_dice(times = 10, rounds = 5) %>% plot_dice()

```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-sim-likely
#| fig-width: 15
#| fig-height: 8
#| fig-cap: Is the outcome likely?

num_success <- 5
sum_num_rolls <- 50

binom_dice(times = 50) |> 
  plot_binom(highlight = c(7:9))+
  scale_x_continuous(
    breaks = seq(0,30)
  )

```

:::

:::

### Theory

::: {.content-visible when-profile="script"}

```{r}
#| label: fig-bn-dist
#| out-width: 95%
#| fig-cap: The binomial distribution

df <- expand.grid(x = seq(0,100), size = c(20,40,100),prob = c(0.5,0.7,0.95))

plt_df <- df %>% 
  mutate(probability = dbinom(x, size = size, prob = prob)) %>% 
  filter(probability>0.0001)

plt_theo <- expand.grid(
  size = c(20,40,100),
  prob = c(0.5,0.7,0.95)) %>% 
  mutate(xintercept = prob*size)

plt_binom <- plt_df %>% 
  ggplot(aes(x = x, y = probability, shape = as.factor(prob))) +
  geom_point()+
  geom_path(
    alpha = 0.4,
    linetype = "dotted"
    )+
  geom_vline(data = plt_theo,
    aes(xintercept = xintercept),
    linetype = "dashed",
    alpha = 0.5
    )+
  geom_label(
    data = plt_theo,
    aes(x = xintercept,
        y = 0.45,
        label = xintercept)
  )+
  facet_wrap(~size,
             scales = "free_x",
             labeller = label_both)+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    title = "The binomial distribution and the influence of different parameter",
    x = "number of successes",
    shape = "probability for success"
  )+
  theme(legend.position = "bottom")

plt_binom

```

The binomial distribution is a **discrete** probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. 
A Bernoulli trial, named after Swiss mathematician Jacob Bernoulli[^2], is a random experiment or trial with two possible outcomes: success and failure. 
These outcomes are typically labeled as $1$ for success and $0$ for failure. 
The key characteristics of a Bernoulli trial are:

1. **Two Outcomes:** There are only two possible outcomes in each trial, and they are mutually exclusive. 
For example, in a coin toss, the outcomes could be heads (success, represented as $1$) or tails (failure, represented as $0$).

2. **Constant Probability:** The probability of success  remains the same for each trial. 
This means that the likelihood of success and failure is consistent from one trial to the next.

3. **Independence:** Each trial is independent of others, meaning that the outcome of one trial does not influence the outcome of subsequent trials. 
For instance, the result of one coin toss doesn't affect the result of the next coin toss.

Examples of Bernoulli trials include:

- Flipping a coin (heads as success, tails as failure).
- Rolling a die and checking if a specific number appears (the number as success, others as failure).
- Testing whether a manufactured product is defective or non-defective (defective as success, non-defective as failure).

The Bernoulli trial is the fundamental building block for many other probability distributions, including the binomial distribution, which models the number of successes in a fixed number of Bernoulli trials.

:::

::: {.r-stack}

::: {.fragment .fade-out}

* probability distribution for discrete outcomes

* n repeated trials.

* two possible outcomes (e.g., success or failure).

* probability of success remains the same 

* each trial is independent 


:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-bn-dist-slides
#| out-width: 95%
#| fig-cap: The binomial distribution

plt_binom

```

:::

::: {.fragment .fade-in style="font-size: 75%;"}

The {{< acr PMF >}} for the binomial distribution is

\begin{align}
P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \nonumber
\end{align}

* $X$ random variable
* $k$ specific value of the random variable ($X$), represents the number of successes 
* $n$ total number of independent trials or experiments
* $p$ probability of success in a single trial $0<p<1$
* $(1 - p)$ probability of failure in a single trial
* $\binom{n}{k}$ binomial coefficient (“n choose k”)$\rightarrow \binom{n}{k} = \frac{n!}{k! \cdot (n - k)!}$ 

:::

::: {.content-visible when-profile="script"}

The {{< acr PMF >}}, also known as the discrete probability density function, is a fundamental concept in probability and statistics. 

* Definition:
The {{< acr PMF >}} describes the probability distribution of a discrete random variable. 
It gives the probability that the random variable takes on a specific value.
In other words, the PMF assigns probabilities to each possible outcome of the random variable.

* Formal Representation:
For a discrete random variable X, the PMF is denoted as $P(X = x)$, where $x$ represents a specific value.
Mathematically, the PMF is defined as: $P(X = x) = \text{{probability that }} X \text{{ takes the value }} x$

* Properties:
The probabilities associated with all hypothetical values must be non-negative and sum up to 1.
Thinking of probability as “mass” helps avoid mistakes, as the total probability for all possible outcomes is conserved (similar to how physical mass is conserved).

* Comparison with {{< acr PDF >}}:
A {{< acr PMF >}} is specific to *discrete* random variables, while a {{< acr PDF >}} is associated with continuous random variables.
Unlike a {{< acr PDF >}}, which requires integration over an interval, the {{< acr PMF >}} **directly** provides probabilities for individual values.

* Mode:
The value of the random variable with the largest probability mass is called the mode.

* Measure-Theoretic Formulation:
The {{< acr PMF >}} can be seen as a special case of more general measure-theoretic constructions.
It relates to the distribution of a random variable and the probability density function with respect to the counting measure.

The {{< acr PMF >}} for the binomial distribution is given in \eqref{PMFbinom}.

\begin{align}
P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \label{PMFbinom}
\end{align}

:::

:::

### The drive shaft exercise - Binomial Distribution

::: {.content-visible when-profile="script"}


In the context of a drive shaft, you can think of it as a model for the number of defective drive shafts in a production batch. Each drive shaft is either good (success) or defective (failure).

Let's say you have a batch of 100 drive shafts, and the probability of any single drive shaft being defective is $0.05 (5\%)$. 
You want to find the probability of having a certain number of defective drive shafts in this batch. 


[^2]: Jacob Bernoulli (1654-1705): Notable Swiss mathematician, known for Bernoulli's principle and significant contributions to calculus and probability theory.

:::

```{r}
#| label: fig-bn-ds
#| out-width: 95%
#| fig-cap: The binomial disitribution and the drive shaft exercise.
#| 
# Parameters
n <- 100  # Total number of drive shafts
p <- 0.05  # Probability of a defective drive shaft

# Generate a sequence of possible outcomes
x <- 0:n

# Calculate the probability mass function (PMF) of the binomial distribution
pmf <- dbinom(x, size = n, prob = p)

# Create a data frame for plotting
data <- data.frame(x = x, pmf = pmf)

# Create a probability distribution plot using ggplot2
ggplot(data, aes(x = x, y = pmf)) +
  geom_vline(xintercept = 5,linetype = "dashed")+
  geom_bar(stat = "identity", color = "white") +
  labs(title = "Binomial Distribution for 100 Drive Shafts",
       x = "Number of Defective Drive Shafts",
       y = "probability") +
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0.05,0))


```


## The Normal Distribution

```{r}
#| label: fig-normal-dist
#| out-width: 75%
#| fig-cap: The standarized normal distribution

# Generate data for a standard normal distribution
data <- data.frame(x = seq(-3, 3, by = 0.01), 
                   y = dnorm(seq(-3, 3, by = 0.01)))

# Create the ggplot2 plot
ggplot(data, aes(x, y)) +
  annotate(
    geom = "segment",
    x = c(-2,2),
    y = c(0,0),
    xend = c(-2,2),
    yend = c(0.13,0.13)
  )+
  geom_area(data = subset(data, x >= -3 & x <= 3), aes(x, y), fill = "azure4", alpha = 0.8) +
  geom_area(data = subset(data, x >= -2 & x <= 2), aes(x, y), fill = "azure3", alpha = 0.8) +
  geom_area(data = subset(data, x >= -1 & x <= 1), aes(x, y), fill = "azure1", alpha = 0.8) +
  geom_vline(xintercept = 0)+
  annotate(
    geom = "segment",
    x = c(-1,-2,-3),
    xend = c(1,2,3),
    y = c(0.23,0.13,0.03),
    yend = c(0.23,0.13,0.03),
    arrow = arrow(ends = "both",type = "closed",angle = 15)
  )+
  annotate(
    geom = "label",
    x = c(0,0,0),
    y = c(0.23,0.13,0.03),
    label = c("68%","95%","99.7%"),
  )+
  scale_x_continuous(
    breaks = seq(-4,4,1),
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  geom_line() +
  labs(title = "Normal Distribution with Sigma Areas",
       x = "Standard Deviations (Sigma)",
       y = "Probability Density")

```

::: {.content-visible when-profile="script"}

The normal distribution is a fundamental statistical concept that holds immense significance in the realms of engineering and production. 
It is often referred to as the Gaussian distribution or the bell curve, is a mathematical model that describes the distribution of data in various natural and human-made phenomena, see @johnson1994continuous. 
It forms a symmetrical curve when plotted, is centered around a mean ([$\mu_0$](index.qmd#truemean-gloss)) and balanced on both sides (@fig-normal-dist).
The spread or dispersion of the data points is characterized by [$\sigma_0^2$](index.qmd#truevariance-gloss).
Those two parameters completely define the normal distribution.
A remarkable property of the normal distribution is the empirical rule, which states that approximately $68\%$ of the data falls within one standard deviation from the mean, $95\%$ falls within two standard deviations, and $99.7\%$ falls within three standard deviations (@fig-normal-dist).
The existence of the normal distribution in the real world is a result of the combination of several factors, including the principles of statistics and probability, the {{< acr CLT >}}, and the behavior of random processes in nature and society. 

:::

::: {.content-visible when-profile="slides"}

### The Normal Distribution in real life

<center>

{{< video chapter001/nd_running_cut.mp4 width="700" >}}

</center>

:::

### Emergence

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| fig-width: 15
#| fig-height: 8


n_rows <- 12
k_values <- seq(-n_rows, n_rows, by = 2) # Possible final positions
probabilities <- dbinom((n_rows + k_values) / 2, n_rows, 0.5)
data.frame(k = k_values, prob = probabilities) %>%
  ggplot(aes(x = k, y = prob)) +
  geom_col(fill = "skyblue") +
  labs(title = "Discrete Distribution of Bean Positions (n=12 rows)")+
  scale_y_continuous(expand = c(0.0,0,0.01,0))

```

:::

::: {.fragment .fade-in}

```{r}
#| fig-width: 15
#| fig-height: 8


n_rows <- 24
k_values <- seq(-n_rows, n_rows, by = 2) # Possible final positions
probabilities <- dbinom((n_rows + k_values) / 2, n_rows, 0.5)
data.frame(k = k_values, prob = probabilities) %>%
  ggplot(aes(x = k, y = prob)) +
  geom_col(fill = "skyblue") +
  labs(title = "Discrete Distribution of Bean Positions (n=24 rows)")+
  scale_y_continuous(expand = c(0.0,0,0.01,0))

```

:::

:::


#### The math behind

+ After $n$ rows a balls final position is the sum of $n$ independent steps: $S = x_1 + x_2 + \ldots + x_n$ where $x_i = +1 \text{( right) or}  -1 \text{(left)}$
+ $\text{Number of paths to } k = \binom{n}{(n + k)/2}$ 

```{r}
#| out-width: 75%
#| label: fig-galtonboard-dist
#| fig-cap: The number of ways a ball can take


galton_dat <- data.frame(
  bin_nr = seq(0,6),
  n = 6
) |> 
  rowwise() |> 
  mutate(
    binom_coef = pracma::nchoosek(n,bin_nr),
    Prob = binom_coef*(0.5^n)
  )

galton_dat |> 
  ggplot(
    aes(
      x = bin_nr,
      y = binom_coef
      )
  )+
  geom_col(
    fill = "steelblue"
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    title = "The binomial coefficient...",
    subtitle = "...models the number of sequences a ball can take",
    y = "number of sequences",
    x = "... to land in bin number ..."
  )

```

#### Approximation with a smooth curve

- For large $n$, the binomial distribution looks like a bell curve. It can be approximated using the [Stirling approximation](https://planetmath.org/StirlingsApproximation)


```{r}
#| label: fig-normdist-spprox
#| fig-cap: The normal distribution can be approximated using a continous curve

ggplot(data.frame(k = k_values, prob = probabilities), aes(x = k, y = prob)) +
  geom_col(fill = "skyblue") +
  stat_function(
    fun = dnorm,
    args = list(mean = 0, sd = sqrt(n_rows)),
    color = "red",
    size = 2
  ) +
  labs(title = "Discrete (Beans) vs. Continuous (Normal) Approximation")+
  scale_y_continuous(expand = c(0.0,0,0.01,0))

```

#### Binomial Probability

- Probability of ending at position $k$: $P(S=k)\binom{n}{(n + k)/2}(\frac{1}{2})^n$

- For large $n$ we can approximate the binomial coefficient: $\binom{n}{m} \approx \frac{n^n}{m^m(n-m)^{n-m}}\sqrt{\frac{n}{2\pi m (n-m)}}$ where $m = (n+k)/2$

#### Large $n$

- Let $k = x$ (treat as continuous) and $n$ be large
- After simplification the {{< acr PMF >}} becomes:  $P(S=x) \approx \frac{1}{\sqrt{2\pi n}}e^{\frac{-x^2}{2n}}$
- Replace $n$ with $\sigma^2$ and allow for mean $\mu$: $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$

#### The Normal PDF Equation

1. $\frac{1}{\sqrt{2\pi\sigma^2}}$: Scaling factor to ensure Total Probability = 1
2. $e^{-\frac{(x-\mu)^2}{2\sigma^2}}$: The "bell" shape - exponential decay based on the distance to the mean

- **Symmetry**: The term $(x-\mu)^2$ ensures the curve is symmetric around $\mu$
- **Peak at mean**: The exponent is zero when $x = \mu$, giving the maximum value
- **Thickness of Tails**: $\sigma$ controls how spread out the curve is

#### Parameter influence

```{r}
#| label: fig-norm-params-inf
#| fig-cap: The influence of distributional parameters

# Interactive exploration of mu and sigma
ggplot(data.frame(x = c(-5, 5)), aes(x = x)) +
  geom_vline(xintercept = c(-1,1), color = "gray")+
  geom_hline(yintercept = 0,color = "gray")+
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue",size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2), color = "red",size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = -1, sd = 1), color = "green",size = 1.5) +
  annotate(
    geom = "text",
    color = c("blue","red","green"),
    label = c("μ = 0, σ = 1","μ = 0, σ = 2","μ = -1, σ = 1"),
    x = c(3), 
    y = c(0.3,0.25,0.2)
  )+
  scale_x_continuous(
    expand = c(0,0,0,0),
    breaks = seq(-7,7)
  )+
  labs(title = "Effect of μ (Mean) and σ (Standard Deviation)")



```

### The Buffon needle problem

<center>

$P$ for crossing a line?

</center>

```{r}
#| label: fig-buffon-needle-experiment
#| out-width: 95%
#| fig-cap: An Illustration of the Buffon Needle Problem.
#| fig-align: center

knitr::include_graphics(here::here("chapter001","buffon.png"))

```

#### Sample Size

```{r}
#| label: fig-buffon-needle
#| fig-cap: Counting needles leads to an esimtate of $\pi$.

set.seed(42)

buffon_simulation <- function(n_needles, l = 1.5, L = 2) {
  theta <- runif(n_needles, 0, pi) # Random angles
  y <- runif(n_needles, 0, L / 2) # Random distances
  crosses <- (y <= (l / 2) * sin(theta)) # Does needle cross?
  pi_estimate <- (2 * l) / (L * mean(crosses)) # Solve P = 2l/(πL) for π
  return(pi_estimate)
}

buffon_results <- data.frame(
  n = c(1,10,100,1000,10000,100000,1000000,10000000)
  ) |> 
  rowwise() |> 
  mutate(
    estimate = buffon_simulation(n)
  ) 

buffon_results |> 
  ggplot(
    aes(
      x = n,
      y = estimate
    )
  )+
  geom_line(size = 2)+
  geom_hline(yintercept = pi)+
  scale_x_continuous(
    trans = "log10",
    labels = function(x){ format(x, scientific = FALSE)}
    )+
  labs(
    title = "Estimation of π",
    y = "Estimation",
    x = "Number of needles"
  )

```

#### Connection to the Normal distribution {.incremental}

<center>
Why?
</center>

::: {.fragment}
Gaussian integral
:::

#### $\pi$ in the Normal Distribution

\begin{align}
I = \int_{-\infty}^{\infty}e^{-kx^2}\,dx\\
I^2 = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-k(x^2+y^2)}\,dx\,dy =  \int_{0}^{2\pi}\int_{0}^{\infty}e^{-k^2}r\,dr\,d\theta = \frac{\pi}{k}
\end{align}

- $e^{-k(x^2+y^2)}=e^{-kr^2}$ is **radially symmetric**, which simplifies the integral
- $\int_0^{2\pi}\,d\theta = 2\pi$ introduces $\pi$ in the normalization constant of the normal distribution

#### $\pi$ in the Buffon Needle Problem

- A needle of length $L$ is dropped onto a plane with parallel lines spaced $D \geq L$ apart
- Probability $P$ that the needle crosses a line is $\pi\approx \frac{2l}{D}\frac{\text{Hits}}{N\text{ total number}}$

- The needles orientation $\theta$ is uniformly distributed in $[0,\pi/2]$
- The crossing condition depends on $\sin{\theta}$, and integrating over $\theta$ introduces $\pi$ through the *average value of $\theta$* over its domain

#### What they share

<center>
Both problems translate a probabilistic question into a geometric one, where $\pi$ emerges naturally from integrating over angles or symmetric regions.
</center>

## Z - Standardization

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-z-scores-raw
#| out-width: 95%
#| fig-cap: How can we compare this data?

set.seed(123)

# Simulate production data from two machines
machine_A <- rnorm(1000, mean = 50, sd = 2)  # Machine A: mean=50g, sd=2g
machine_B <- rnorm(1000, mean = 75, sd = 5)  # Machine B: mean=75g, sd=5g

# Plot
tibble(
  value = c(machine_A, machine_B),
  machine = rep(c("A", "B"), each = 1000)
) %>%
  ggplot(aes(x = value, fill = machine)) +
  geom_density(alpha = 0.5) +
  labs(title = "Fill Weights from Two Machines (Different Means/SDs)")+
  scale_y_continuous(
    expand = c(0,0,0,0.05)
  )+
  scale_fill_grey()

```

:::

::: {.fragment .fade-in-then-out}

\begin{align}
z = \frac{x_i - \mu}{\sigma} \nonumber \\
z = \frac{x_i - \bar{x}}{sd} \nonumber
\end{align}

:::

::: {.fragment .fade-in-then-out}

Machine (A) value: $53$

\begin{align}
\bar{x} &= `r round(mean(machine_A),digits = 2)` \nonumber \\
sd_{x} &= `r round(sd(machine_A),digits = 2)` \nonumber \\
z\text{-score} &= \frac{x-\bar{x}}{sd_x} = \frac{53-`r round(mean(machine_A),digits = 2)`}{`r round(sd(machine_A),digits = 2)`} = `r round((53-mean(machine_A))/sd(machine_A),digits = 2)` \nonumber
\end{align}

:::

:::

::: {.content-visible when-profile="script"}

The {{< acr Z >}}-standardization, also known as {{< acr Z >}} or {{< acr Z >}}, is a common statistical technique used to transform data into a standard normal distribution with a mean of $0$ and a standard deviation of $1$ [@1981369198]. 
This transformation is useful for comparing and analyzing data that have different scales and units \eqref{zscore}.

\begin{align}
Z = \frac{x_i - \bar{x}}{sd} \label{zscore}
\end{align}

How the {{< acr Z >}} can be applied is shown in @fig-z-scores-raw and @fig-z-scores-scaled.
The data for group `X` and group `Y` may be measured in different units ( @fig-z-scores-raw).
To answer the question, which of the values $x_i (i=1\ldots5)$ is more probable, the single data points are transformed to the respective z-score using \eqref{zscore}.
In @fig-z-scores-scaled, the {{< acr Z >}} for both groups are plotted against each other.
The perfect correlation of the datapoints shows, that for every $x_i$ the same probability applies.
Thus, the datapoints are comparable.

:::

### Properties of of Z-scores

1. Shape: The distribution's shape remains normal
2. Relative Positions: Values maintain their percentiles
3. Outliers: Extreme values $|z|>3$ are easily flagged

### Comparison of standardized data

```{r}
#| label: fig-z-scores-scaled
#| out-width: 75%
#| fig-cap: Normalized data is easier to compare


machine_data <- data.frame(
  machine_A,machine_B
) %>% 
  mutate(
    machine_A_scaled = (machine_A-mean(machine_A))/sd(machine_A),
    machine_B_scaled = (machine_B-mean(machine_B))/sd(machine_B)) %>% 
  pivot_longer(cols = starts_with("machine"), names_to = "machine",values_to = "value") %>% 
  filter(str_detect(machine, "scaled"))

machine_data %>% 
  ggplot(
    aes(
      x = value,
      # fill = machine
      )
    ) +
  geom_density_ridges(
    aes(
      y = machine
    )
  )+
  scale_y_discrete(
    expand = c(0.0,0.0,0.05,0.05)
  )+
  labs(
    title = "Comparison of normalized data",
    y = ""
  )

```
### The Universal Yardstick

The standard normal distribution ($N(0,1)$) is the reference distribution (the perfect model).

- $\bar{x} = 0,\; sd = 1$
- Empirical Rule (68-65-99.7)
  - $68\%$ of data within $\pm1sd$
  - $95\%$ of data within $\pm2sd$
  - $99.7\%$ of data within $\pm3sd$

### The drive shaft exercise - Z-Standardization

```{r}
#| include: false

load(here("data","drive_shaft_data.Rdata"))

spec_nom <- 12
spec_lo <- 11.9
spec_hi <- 12.1

ds_sum <- drive_shaft %>% 
  group_by(group) %>% 
  summarise(
    mean_d = mean(diameter),
    sd_d = sd(diameter)
  )

spec_scaled <- ds_sum %>% 
  mutate(
    spec_nom_scl = (spec_nom-mean_d)/sd_d,
    spec_lo_scl = (spec_lo-mean_d)/sd_d,
    spec_hi_scl = (spec_hi-mean_d)/sd_d,
  )

drive_shaft <- drive_shaft %>% 
  pivot_wider(names_from = group, values_from = diameter) 

drive_shaft_scaled <- drive_shaft %>% 
  mutate(across(starts_with("group"),scale)) %>% 
  pivot_longer(starts_with("group"),names_to = "group",values_to = "diameter")
```

```{r}
#| label: fig-ds-z
#| out-width: 95%
#| fig-cap: The standardized data of the drive shaft data.
#| fig-pos: "H"

drive_shaft_scaled %>% 
 ggplot(aes(x = diameter))+
  stat_theodensity(
    aes(y = stat(density), 
        fill = "Normal Distribution",
        linetype = "theoretical"),
    distri = "norm", 
    geom = "area",
    color = "black",
    alpha = 0.5)+
  geom_density(aes(linetype = "data",fill = "data"),alpha=0.5)+
  facet_wrap(~group,
             scales = "free_y"
             )+
  geom_vline(data = spec_scaled,aes(xintercept = spec_nom_scl,linetype = "nominal"),key_glyph = draw_key_path)+
  geom_vline(data = spec_scaled,aes(xintercept = spec_lo_scl,linetype = "lower limit"),key_glyph = draw_key_path)+
  geom_vline(data = spec_scaled,aes(xintercept = spec_hi_scl,linetype = "upper limit"),key_glyph = draw_key_path)+
  scale_y_continuous(expand = c(0,0,0.05,0))+
  # scale_x_continuous(expand = c(0,0,0,0))+
  scale_linetype_manual(
    values = c(
      "nominal" = "dotdash", 
      "lower limit" = "dotted", 
      "upper limit" = "dotted",
      "theoretical" = "dashed",
      "data" = "solid")
      )+
  scale_fill_manual(values = c("Normal Distribution"="azure4",
                               "data" = "azure1"))+
  labs(
    title = "The drive shaft data with overlayed normal distributions",
    y = "density",
    x = "standard deviation",
    fill = "",
    linetype = ""
    )+
  theme(legend.position = "bottom")+
  guides(
    color = guide_legend(nrow = 2, byrow = TRUE),
    linetype = guide_legend(nrow = 2, byrow = TRUE),
    fill = guide_legend(nrow = 2 , byrow = TRUE),
    )

```

::: {.content-visible when-profile="script"}

In @fig-ds-z the standardized drive shaft data is shown.
The mean of the data ($\bar{x}$) is now centered at $0$ and the standard deviation is $1$.
For this case, the specification limits have also been transferred to the respective {{< acr Z >}}-score (even though they can not be interpreted as such anymore).
For every $x_i$ the probability to be within a normal distribution is now known.
When comparing this to the transferred specification limits, it is clear to see that for `group01` "most" of the data points are within the limits in contrast to `group03` where none of the data points lies within the specification limits.
When looking at `group03` we see, that the *nominal* specification limit is `r round(spec_scaled$spec_nom_scl[[3]],digits = 2)` standard deviations away from the centered mean of the datapoints.
The probability of a data point being located there is `r pnorm(round(spec_scaled$spec_nom_scl[[3]],digits = 2))` which does not sound an awful lot.
We will dwelve more into such investigation in another chapter, but this is a first step in the direction of inferential statistics.

:::

### When Z-Scores Go Wrong

1. Non-Normal Data:
  
- Z-Scores assume normality
  
2. Population vs. Sample
  
- Use population parameters ($\mu,\sigma$) if known, otherwise use sample estimates ($\bar{x},sd$)
  
3. Outliers:
  
- Z-scores are sensitive to outliers

### The Z-transform and the Galton Board

```{r}
#| fig-width: 15
#| fig-height: 8


galton_dat <- data.frame(
  bin_nr = seq(0,1000),
  n = 1000
) |> 
  rowwise() |> 
  mutate(
    binom_coef = pracma::nchoosek(n,bin_nr),
    Prob = binom_coef*(0.5^n)
  )

galton_dat |> 
  ggplot(
    aes(
      x = bin_nr,
      y = Prob
      )
  )+
  geom_col()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )

```

#### Applying the Z-transform

$$Z = \frac{X-\mu}{\sigma}$$

::: {.fragment .fade-in}

$$Z = \frac{X-\frac{n}{2}}{\frac{\sqrt{n}}{2}}$$
:::

::: {.fragment .fade-in}

$$\lim_{n\to\infty} P(a\leq Z \leq b)= \int_a^b \frac{1}{\sqrt{2\pi}}e^\frac{-z^2}{2} \,dz $$

:::

#### Converting the bionmial Formula to a Normal Form

Stirling appoximation: $n!\approx\sqrt{2\pi n} \left( \frac{n}{e} \right)^n$

Appprox: $\binom{n}{k} \approx \frac{\sqrt{2\pi n} \left( \frac{n}{e} \right)^n}{\sqrt{2\pi n} \left( \frac{k}{e} \right)^k \cdot \sqrt{2\pi(n-k)} \left( \frac{n-k}{e} \right)^{n-k}}$

simplifies to: $\binom{n}{k} = \frac{1}{\sqrt{2\pi n p (1-p)}}e^{-\frac{(k-np)^2}{2np(1-p)}}$

substituting $p=0.5$: $P(X = k) \approx \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(k-\mu)^2}{2\sigma^2}}$

<center>

Which is the {{< acr PDF >}}

</center>

### The drive shaft exercise - Normal Distribution

```{r}
#| label: fig-ds-nd
#| out-width: 75%
#| fig-cap: The drive shaft data with the respective normal distributions.

load(here("data","drive_shaft_data.Rdata"))

drive_shaft %>% 
  ggplot(aes(x = diameter))+
  geom_histogram()+
  stat_theodensity(
    aes(y = stat(density)*4, 
        fill = "Normal Distribution"),
    distri = "norm", 
    geom = "area",
    color = "black",
    alpha = 0.5)+
  geom_vline(aes(xintercept = 12,linetype = "nominal"),key_glyph = draw_key_path)+
  geom_vline(aes(xintercept = 11.9,linetype = "lower limit"),key_glyph = draw_key_path)+
  geom_vline(aes(xintercept = 12.1,linetype = "upper limit"),key_glyph = draw_key_path)+
  facet_wrap(~group,
             scales = "free_y")+
  scale_linetype_manual(
    values = c("nominal" = "solid", "lower limit" = "dashed", "upper limit" = "dashed")
      )+
  scale_y_continuous(expand = c(0,0,0.05,0))+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_fill_manual(values = c("Normal Distribution"="azure3"))+
  labs(
    title = "The drive shaft data with overlayed normal distributions",
    y = "count",
    fill = "",
    linetype = "Specification")+
  theme(legend.position = "bottom")


```

::: {.content-visible when-profile="script"}

In @fig-ds-nd the `drive shaft data` is shown for each group in a histogram.
As an overlay, the respective *normal distribution* (with the groups $\bar{x},sd$) is overlayed.
If the data is normally distributed, is a different question.

:::

## {{< acr PDF >}}

::: {.content-visible when-profile="script"}

```{r}
#| label: fig-pdf-000-scr
#| out-width: 75%
#| fig-cap: A visual represenstation of the PDF for the normal distribution.

knitr::include_graphics(here::here("chapter001","PDF_000.png"))

```

\begin{align}
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end{align}

A {{< acr PDF >}} is a mathematical function that describes the *likelihood* of a continuous random variable taking on a particular value. 
Unlike discrete probability distributions, which assign probabilities to specific values of a discrete random variable, a {{< acr PDF >}} describes the relative likelihood of the variable falling within a particular range of values. 
The total area under the curve of a {{< acr PDF >}} over its entire range is equal to 1, indicating that the variable must take on some value within that range. 
In other words, the integral of the {{< acr PDF >}} over its entire domain equals 1. 
The probability of a continuous random variable falling within a specific interval is given by the integral of the {{< acr PDF >}} over that interval.

:::


::: {.r-stack}

::: {.fragment .fade-out}

* there is no {{< acr PDF >}} for discrete variables
* can be interpreted as the relative likelihood
* is used to specify the probability of a random variable within a certain range

\begin{align}
\varphi(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end{align}

:::

::: {.fragment .fade-in}

```{r}
#| label: fig-pdf-000
#| out-width: 75%
#| fig-cap: A visual represenstation of the PDF for the normal distribution

knitr::include_graphics(here::here("chapter001","PDF_000.png"))

```

:::

:::

## {{< acr CDF >}}

::: {.content-visible when-profile="script"}

```{r}
#| label: fig-cdf-000-scr
#| out-width: 75%
#| fig-cap: A visual represenstation of the CDF for the normal distribution.

knitr::include_graphics(here::here("chapter001","CDF_000.png"))

```

A [cumulative density function (CDF)](#CDF), also known as a cumulative distribution function, describes the probability that a random variable will take on a value less than or equal to a given point. 
It is the integral of the {{< acr PDF >}} from negative infinity to a certain value.
The {{< acr CDF >}} provides a comprehensive view of the probability distribution of a random variable by showing how the probability accumulates as the value of the random variable increases. 
Unlike the {{< acr PDF >}}, which gives the probability density at a particular point, the CDF gives the cumulative probability up to that point.

\begin{align}
z &= \frac{x-\mu}{\sigma} \nonumber \\
\varphi(x) &= \frac{1}{2\pi}e^{\frac{-z^2}{2}} \\
\phi(x)& = \int \frac{1}{2\pi}e^{\frac{-x^2}{2}} \, dx \\
\lim_{x\to\infty} \phi(x) &= 1 \nonumber \\
\lim_{x\to - \infty} \phi(x) &= 0 \nonumber
\end{align}

:::

::: {.r-stack}

::: {.fragment .fade-out}

* shows the actual probability for a certain value
* is the integral of the {{< acr PDF >}} 
* area always sums up to $1$

:::

::: {.fragment .fade-in-then-out}

\begin{align}
z &= \frac{x-\mu}{\sigma} \nonumber \\
\varphi(x) &= \frac{1}{2\pi}e^{\frac{-z^2}{2}} \nonumber \\
\phi(x)& = \int \frac{1}{2\pi}e^{\frac{-x^2}{2}} \, dx \nonumber \\
\lim_{x\to + \infty} \phi(x) &= 1 \nonumber \\
\lim_{x\to - \infty} \phi(x) &= 0 \nonumber
\end{align}

:::

::: {.fragment .fade-in}

```{r}
#| label: fig-cdf-000-sli
#| out-width: 75%
#| fig-cap: A visual represenstation of the CDF for the normal distribution.
#| fig-align: center

knitr::include_graphics(here::here("chapter001","CDF_000.png"))

```

:::

:::

## Likelihood and Probability

::: {.content-visible when-profile="script"}

```{r}
#| label: fig-lp
#| out-width: 75%
#| fig-cap: The subtle difference between likelihood and probability.

knitr::include_graphics(here::here("chapter001","likelihood_prob.png"))

```

Likelihood
:   refers to the chance or plausibility of a particular event occurring given certain evidence or assumptions. It is often used in statistical inference, where it indicates how well a particular set of parameters (or hypotheses) explain the observed data. Likelihood is a measure of how compatible the observed data are with a specific hypothesis or model.

Probability 
:   represents the measure of the likelihood that an event will occur. It is a quantification of uncertainty and ranges from $0$ (indicating impossibility) to $1$ (indicating certainty). Probability is commonly used to assess the chances of different outcomes in various scenarios.

In summary, while both likelihood and probability deal with the chance of events occurring, likelihood is often used in the context of comparing different *hypotheses or models* based on *observed data*, while probability is more broadly used to quantify the chances of *events happening* in *general*.

:::

::: {.content-visible when-profile="slides"}

::: {.r-stack}

::: {.fragment .fade-out}

Likelihood
:   refers to the chance based on given evidence or assumptions, often used in statistical inference

Probability: 
:   is a measure of the chance an event will occur, ranging from $0$ to $1$.

:::

![](chapter001/likelihood_prob_001.svg){.fragment .fade-in-then-out width=85% fig-align="center" auto-animate=true}

![](chapter001/likelihood_prob_002.svg){.fragment .fade-in-then-out width=85% fig-align="center" auto-animate=true}

![](chapter001/likelihood_prob_003.svg){.fragment .fade-in-then-out width=85% fig-align="center" auto-animate=true}

![](chapter001/likelihood_prob_004.svg){.fragment .fade-in-then-out width=85% fig-align="center" auto-animate=true}

:::

:::

:::

### Exercise

::: {.fragment .fade-in-then-out style="font-size: 75%;"}

| Machine      | Sample 1 | Sample 2 | Sample 3 | Sample 4 | Sample 5 |
|--------------|----------|----------|----------|----------|----------|
| **Machine X**| 44       | 46       | 45       | 47       | 43       |
| **Machine Y**| 52       | 50       | 53       | 49       | 51       |
| **Machine Z**| 40       | 42       | 39       | 41       | 43       |

: Exercise Data

```{r}
#| label: tbl-exercise-z
#| tbl-cap: Exercise Data

dat <- data.frame(
  machine_x = c(44,46,45,47,43),
  machine_y = c(52,50,53,49,51),
  machine_z = c(40,42,39,41,43)
)

dat %>% 
  gt()

```


1. calculate $\bar{x}$ and $sd$
2. Compute z-scores
3. Flag anomalies
4. Recommend actions

:::

---

```{r}
#| label: tbl-exercise-z-res
#| tbl-cap: Results

dat_long <- dat |> 
  pivot_longer(cols = starts_with("machine"),names_to = "machine",values_to = "cycle_time_s") 

  dat_sum <- dat_long |> 
  group_by(machine) |>  
  summarise(
    mean_cycle_time = mean(cycle_time_s),
    sd_cycle_time = sd(cycle_time_s),
    var_cycle_time = sd_cycle_time^2
  ) 
  
  
  dat_sum |> gt()
```

---

```{r}
#| label: tbl-exercise-z-scores
#| tbl-cap: Results for z score
dat_z <- dat_long |> 
  group_by(machine) |>  
  mutate(
    scaled_data = scale(cycle_time_s))

dat_z |> gt()

```


## Chi^2^ - Distribution

::: {.content-visible when-profile="script"}

```{r}
#| layout: [[45,-10, 45], [100]]
#| label: fig-chi-2
#| fig-cap: What a $\chi^2$ distribution reprepresents and how it relates to a the normal distribution.
#| fig-subcap: 
#| - a normal distribution
#| - the standard normal variable square ($dof = 1$)
#| - the $\chi^2$ distributions with varying degrees of freedom
#| fig-pos: H


# create PRNG
set.seed(123)


# sample norms
norms <- rnorm(2^16)
norms_sq <- norms^2

data_nomal <- data.frame(x = seq(-3, 3, by = 0.01), 
                   y = dnorm(seq(-3, 3, by = 0.01))) %>% 
  mutate(
    x2 = x^2,
    y2 = y^2
  )

plt1 <- ggplot(data.frame(norms),aes(x = norms))+
  geom_ribbon(data = data_nomal,aes(x = x, ymax = y*19000,ymin = 0),fill = "azure3")+
  geom_line(data = data_nomal,aes(x = x, y = y*19000))+
  geom_histogram(alpha = 0.7,color = "white")+
  scale_x_continuous(
    expand = c(0,0,0,0),
    breaks = seq(-5,5)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
  )+
  labs(title = "normally distributed, random data points (n=65536)",
       x = "standard deviations",
       y = "counts")
  
plt2 <- ggplot(data.frame(norms_sq),aes(x = norms_sq))  +
  geom_histogram(aes(y = ..density..))+
  geom_density() +
  scale_x_continuous(
    expand = c(0,0,0,0),
    limits = c(0,10)
    # breaks = seq(-5,5)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
  )+
  labs(title = "squared normally distributed data",
       x = "standard deviations"
       )


plt3 <- data.frame(
  dof = seq(2,20,2)
  ) %>% 
  
  mutate(
  x = list(seq(0,50,0.1)),
  d = map2(x,dof,dchisq)
) %>% 
  mutate(dof = as.factor(dof)) %>% 
  unnest(cols = c(x,d)) %>% 
  ggplot(aes(x = x, y = d, linetype = dof))+
  geom_path()+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(title = "PDF of Chi square distribution with varying dof",
       )

plt1
plt2
plt3


```

The $\chi^2$ distribution is a continuous probability distribution that is widely used in statistics [@1981369198]. 
It is often used to test hypotheses about the independence of categorical variables.

\begin{align}
\chi^2 = \sum_{k = 1}^n \frac{(O_k - E_k)^2}{E_k}
\end{align}

The connection between the chi-squared distribution and sample variance holds significant importance in statistics. 

1. **Distribution of Sample Variance:** When calculating the sample variance from a dataset, it follows a chi-squared distribution. 
Specifically, for a random sample from a normally distributed population with mean [$\mu_0$](index.qmd#truemean-gloss) and variance [$\sigma_0^2$](index.qmd#truevariance-gloss), the sample variance (adjusted for bias) divided by [$\sigma_0^2$](index.qmd#truevariance-gloss) follows a $\chi^2$ distribution with $n-1$ {{< acr dof >}}, where $n$ is the sample size.

2. **Hypothesis Testing:** In statistical analysis, hypothesis testing is a common technique for making inferences about populations using sample data. The $\chi^2$ distribution plays a crucial role in hypothesis testing, especially when comparing variances between samples.

   - **$\chi^2$ Test for Variance:** The $\chi^2$ distribution is used to test whether the variance of a sample matches a hypothesized variance. This is applicable in various scenarios, such as quality control, to assess the consistency of a manufacturing process.

3. **Confidence Intervals:** When estimating population parameters like population variance, it's essential to establish confidence intervals. The $\chi^2$ distribution aids in constructing these intervals, allowing researchers to quantify the uncertainty associated with their parameter estimates.

4. **Model Assessment:** In regression analysis, the $\chi^2$ distribution is related to the F-statistic, which assesses the overall significance of a regression model. It helps determine whether the regression model is a good fit for the data.

In summary, the link between the chi-squared distribution and sample variance is fundamental in statistical analysis. It empowers statisticians and analysts to make informed decisions about population parameters based on sample data and evaluate the validity of statistical models. Understanding this relationship is essential for those working with data and conducting statistical investigations.


:::

::: {.r-stack}

::: {.fragment .fade-out}

* is a "new" distribution function
* arises from squaring a random variable
* is calculated with:

\begin{align}
\chi^2 = \sum_{k = 1}^n \frac{(O_k - E_k)^2}{E_k}
\end{align}

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 100%
#| 
plt1
```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 100%
plt2
```

:::

::: {.fragment .fade-in-then-out}

* handles categorical (nominal) data and is used in statistical inference in many ways
* the goal is to "convert" discrete (nominal) values to continous variables (counts in frequencies, making things measureable)

:::

::: {.fragment .fade-in-then-out}

1. Independence Test: Are two categorical variables independent of each other?
2. Distribution Test: Are the observed values of two categorical variables equal to the expected values?
3. Homogeneity Test:Are two or more samples from the same population (comparing frequencies)?

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 100%
plt3
```

:::

:::

### Classroom example: Fairness of a die

::: {.r-stack}

::: {.fragment .fade-out}

* each face should come up the same number of times
* each face represents a category, the order of $1 ... 6$ does not matter
* look at it as a way to collect live data on different categories
* Let's do it using 60 roles of the same die
* First, formulate the Null and alternative Hypothesis

:::

::: {.fragment .fade-in-then-out}

- $H_0$ The faces turn up with the same frequencies
- $H_a$ The faces do not turn up with the same frequencies

:::

::: {.fragment .incremental .smaller}

- $\chi^2 = \sum_{k = 1}^n \frac{(O_k - E_k)^2}{E_k}$
- $E_k = ?$
- $O_k = ?$ 
- $\alpha = ?$
- $dof = n_{categories}-1 = ?$
- [$\chi^2$-table](https://datatab.net/tutorial/chi-square-distribution) or `qchisq(1 - alpha, dof)` in `R`
- The Hypothesis as "translated" to quantitative measures?


:::

:::

### The drive shaft exercise - Chi^2^ Distribution

```{r}
#| label: fig-ds-chi
#| out-width: 95%
#| fig-cap: The $\chi^2$ disitribution of the drive shaft data.

sample_no <- seq(1,100) 

# dataset <- data.frame(sample_no = sample_no, 
#                       group01 = rnorm(n = length(sample_no),mean = 12, sd = 0.1), 
#                       group02 = rnorm(n = length(sample_no),mean = 12.3, sd = 0.1)) %>%
  

load (here("data","drive_shaft_data.Rdata"))

dataset <- drive_shaft %>%   
  pivot_wider(names_from = "group",values_from = "diameter") %>% 
  mutate(across(starts_with("group"),scale), 
         across(starts_with("group"),as.vector)) %>% 
    pivot_longer(cols = starts_with("group"),names_to = "group",values_to = "scld") 
  



dat_chisq <- dataset %>% 
  mutate(scld_sq= scld^2) 


dat_chisq %>% 
  ggplot(aes(x = scld_sq))+ 
  stat_theodensity( 
    aes( 
      y = after_stat(count), 
      fill = "theoretical" 
    ), 
    # color = NA, 
    alpha = 0.7, 
    geom = "area", 
    distri = "chisq", 
    show.legend = FALSE 
  )+ 
  geom_histogram( 
    aes( 
      # linetype = group, 
      fill = group), 
    position = "dodge", 
    color = "white" 
    )+ 
  geom_density( 
    aes(y = after_stat(count), 
        linetype = group), 
    key_glyph = draw_key_path, 
  )+ 
  scale_x_continuous( 
    limits = c(0.1,6), 
    expand = c(0,0,0,0) 
    )+ 
  scale_y_continuous( 
    expand = c(0,0,0,0), 
  )+ 
  scale_fill_grey()+ 
  coord_cartesian( 
    ylim = c(0,30) 
  )+ 
  facet_wrap(~group)+
  theme_few(base_size = 8)+ 
  labs( 
    title = latex2exp::TeX("Computed $sd^2$ for dataset"), 
    x = latex2exp::TeX("$sd^2$"), 
    y = "count", 
    linetype = "computed\ndensity", 
    fill = "empirical and theoretical\ndistributions" 
      )+ 
  theme( 
    legend.position = "bottom" 
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE),
         linetype = guide_legend(nrow = 3, byrow = TRUE),)

```

::: {.content-visible when-profile="script"}

In @fig-ds-chi the squared standad deviation for every datapoint (from the stanardized data) is shown as a histogram for every group with an overlayed (and scaled) density plot. 
In the background of every group the theoretical $\chi^2$-distribution with $dof = 1$ is plotted to visually compare the empirical distribution of the datapoints to the theorectial.

:::


## t - Distribution

::: {.content-visible when-profile="script"}

```{r}
#| label: fig-t-dist
#| out-width: 95%
#| fig-cap: PDF of t-distribution with varying $dof$

data.frame(
  dof = c(1,4,5,seq(10,100,10))
  ) %>% 
  
  mutate(
  x = list(seq(-4,4,0.01)),
  d = map2(dof,x,function(x,y)dt(y,x)),
  d_norm = map(x,dnorm,mean = 0, sd = 1)
) %>% 
  mutate(dof = as.factor(dof)) %>% 
  unnest(cols = c(x,d,d_norm)) %>% 
  ggplot(aes(x = x, y = d, linetype = dof))+
  geom_ribbon(aes(ymax = d_norm,ymin = 0),alpha = 0.4,fill = "azure3",show.legend = F)+
  geom_line()+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(title = "t-distribution with varying dof",
       )


```

The t-distribution, also known as the Student's t-distribution [@Student_1908], is a probability distribution that plays a significant role in statistics[^1]. 
It is a symmetric distribution with a bell-shaped curve, similar to the normal distribution, but with heavier tails. 
The key significance of the t-distribution lies in its application to inferential statistics, particularly in hypothesis testing and confidence interval estimation.

1. **Small Sample Sizes:** When dealing with small sample sizes (typically less than 30), the t-distribution is used to make inferences about population parameters, such as the mean. This is crucial because the normal distribution assumptions are often violated with small samples.

2. **Accounting for Variability:** The t-distribution accounts for the variability inherent in small samples. It provides wider confidence intervals and more conservative hypothesis tests compared to the normal distribution, making it more suitable for situations where sample size is limited.

3. **Degrees of Freedom:** The shape of the t-distribution is determined by a parameter called {{< acr dof >}}. As the {{< acr dof >}} increases, the t-distribution approaches the normal distribution. When df is small, the tails of the t-distribution are fatter, allowing for greater uncertainty in estimates.

Statisticians found that if they took samples of a constant size from a normal population, computed a statistic called a *t-score* for each sample, and put those into a relative frequency distribution, the distribution would be the same for samples of the same size drawn from any normal population. 
The shape of this sampling distribution of t’s varies somewhat as sample size varies, but for any $n$, it is always the same.
For example, for samples of $5$, $90\%$ of the samples have t-scores between $-1.943$ and $+1.943$, while for samples of $15$, $90\%$ have t-scores between $\pm 1.761$. 
The bigger the samples, the narrower the range of scores that covers any particular proportion of the samples \eqref{tscore} (Note the similarity to \eqref{zscore}).
Since the *t-score* is computed for every $x_i$ the resulting sampling distribution is called the *t-disitribution*.

\begin{align}
t_i = \frac{x_i - \mu_o}{sd/\sqrt{n}} \label{tscore}
\end{align}

In @fig-t-dist it is shown, that with increasing {{< acr dof >}} (in this case *sample size*), the *t-distribution* approximates a normal distribution (gray area).
@fig-t-dist also shows an example of the *t-distribution* in action.
Of all possible samples with 9 [$dof$](#dof) $0.025\;(2\frac{1}{2}\%)$ of those samples would have t-scores greater than $2.262$, and $.975\;(97.5\%)$ would have t-scores less than $2.262$.
The advantage of the *t-score* and *t-distribution* is clearly visible.
All these values can be computed from sampled data, the population can remain *estimated* \eqref{tscore}.

[^1]: William Sealy Gosset (June 13, 1876 - October 16, 1937) was a pioneering statistician known for developing the t-distribution, a key tool in modern statistical analysis.

:::

::: {.r-stack}

::: {.fragment .fade-out}

- accounts especially for small sample sizes ($n<30$)
- Shape: looks like normal, but with heavier tails
- Degrees of Freedom: related to sample size ($n-1$)
- Convergence to Normal: as the {{< acr dof >}} increases, the t distribution approaches the standard normal distribution. When we have more data, the estimate of the population standard deviation becomes more accurate.

<center>
We need the t-Distribution for **small sample sizes** with an **unknown population standard deviation**
</center>

:::



::: {.fragment .fade-in-then-out}

* is another "new" distribution function
* is a generalization of the standard normal distribution
* "connects" the sample with the population via the *t-score*
* does not need the population standard deviation for computation

\begin{align}
t_i = \frac{\bar{x} - \mu_0}{sd/\sqrt{n}} \label{tscore}
\end{align}

:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-t-dist-sl
#| out-width: 95%
#| fig-cap: PDF of t-distribution with varying $dof$

data.frame(
  dof = c(1,4,5,seq(10,100,10))
  ) %>% 
  
  mutate(
  x = list(seq(-4,4,0.01)),
  d = map2(dof,x,function(x,y)dt(y,x)),
  d_norm = map(x,dnorm,mean = 0, sd = 1)
) %>% 
  mutate(dof = as.factor(dof)) %>% 
  unnest(cols = c(x,d,d_norm)) %>% 
  ggplot(aes(x = x, y = d, linetype = dof))+
  geom_ribbon(aes(ymax = d_norm,ymin = 0),alpha = 0.4,fill = "azure3",show.legend = F)+
  geom_line()+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(title = "t-distribution with varying dof",
       )

```

:::

::: {.fragment .incremental}

* Applications of the *t-disitribution* include:

* Hypothesis testing (one sample t-test, two sample t-test, two sample t-test for paired samples)
* Computing confidence intervals
* Test significance of model parameters (against *null* model)

:::

:::

### Classroom example: Comparing means

::: {.r-stack}

::: {.fragment .fade-out}

* we want to know if student A and student B are rolling dice differently
* everyone rolls the die 15 times each
* look at it as a way to collect live data on different groups
* First, formulate the Null and alternative Hypothesis

:::

::: {.fragment .fade-in-then-out}

- $H_0$ student A and student B roll the die in the same fashion
- $H_a$ student A and student B roll the die in a different fashion

:::

::: {.fragment .incremental .smaller}

- $t_i = \frac{\bar{x} - \mu_0}{sd/\sqrt{n}}$
- $dof = ?$
- $\alpha = ?$
- $dof = ?$
- [$t$-table](https://datatab.de/tutorial/tabelle-t-verteilung) or `qt(alpha / 2, dof)` in `R`
- The Hypothesis as "translated" to quantitative measures?


:::

:::

### The drive shaft exercise - t-Distribution

::: {.content-visible when-profile="script"}

The t-score computation and the z-standardization look very familiar.
While the z-score calculation needs some population parameters, the t-score calculation does not need such.
It therefore allows us, to estimate population parameters based on a sample - a very frequent use case in statistics.

Suppose we have some data (maybe the drive shaft exercise?) with which calculations can be done.
First, the mean $\bar{x}$ and $sd$ is calculated according to \eqref{mean} and \eqref{sd}.
After this, the {{< acr cl >}} (we will get to this later in more detail) is specified.
A value of $95\%$ is a common choice of {{< acr cl >}}.

\begin{align}
ci &= 0.95 \quad \text{(for a 95\% confidence level)}
\end{align}

Then the {{< acr SE >}} is calculated using \eqref{se}, which takes the $sd$ and $n$ of a sample into account (notice, how we did not use any population estimation?).

\begin{align}
SE &= \frac{sd}{\sqrt{n}} \label{se}
\end{align}

In the next step, the critical *t-score* is calculated using the {{< acr cl >}} as shown in \eqref{tscore}.
*qt* in this case returns the value of the inverse {{< acr CDF >}} of the t-distribution given a certain random variable (or datapoint $x_i$) and $n-1$ {{< acr dof >}}.
Think of it as an automated look up in long statistical tables.

\begin{align}
% Step 5: Calculate the T-score
t_{score} &= qt\left(\frac{1 - ci}{2}, df = n - 1\right) \label{tscore}
\end{align}

With this, the *margin of error* can be calculated using the {{< acr SE >}} and the *t-score* as shown in \eqref{errormargin}.

\begin{align}
% Step 6: Calculate the margin of error
margin\;of\;error &= t_{score} \times SE \label{errormargin}
\end{align}

In the last step the [Confidence Interval](#ci) is calculated for the `lower` and the `upper` bound with \eqref{cilobound} and \eqref{cihibound}.

\begin{align}
% Step 7: Determine the confidence interval
lo &= \bar{x} - margin\;of\;error \label{cilobound} \\
hi &= \bar{x} + margin\;of\;error \label{cihibound}
\end{align}

It all looks and feels very similar to using the normal disitrbution.
Why this is the case, is shown in @fig-ds-t.
In @fig-ds-t-1 the raw dataset is shown with the underlayed specification limits for the manufacturing of the drive shaft.
For some groups the judgement if the drive shaft is wihtin specification is quite clear (`group 1`, `group 2` and `group 5`).
For the other groups, this can not be done so easily.
For the drive shaft data, we of course now some population data, therefore the *normal distribution* can be compared to the *t-distribution*.
This is done in @fig-ds-t-2.
On the `x-axis` the diameter is shown, the `y-axis` depicts the groups (as before).
The distribution on top of the estimated parameters is the population (normal distribution), the distribution on the bottom follow a *t-distribution*.
With $n>30$ (as for this dataset), the difference between distribution is very small, further showcasing the use of the *t-distribution* (also see @fig-t-dist for comparison).

:::

```{r}
#| label: fig-ds-t
#| out-width: 95%
#| fig-cap: The drive shaft data with normal disitribution, t-distribution and confidence intervalls using the t-distribution


load (here("data","drive_shaft_data.Rdata"))


drive_shaft <- drive_shaft %>% 
  group_nest(group) %>% 
  mutate(scaled = map(data, function(x) scale(x["diameter"]) %>% as.vector(.)),
         mean_d = map(data, function(x) mean(x %>% pull(diameter))),
         sd_d = map(data, function(x) sd(x %>% pull(diameter))),
         count_n = map(data, function(x) nrow(x))
  ) 
  
drive_shaft_scaled <- drive_shaft %>% 
  unnest(cols = c("data"))
         

drive_shaft_t_score <- drive_shaft %>% 
  unnest(cols = c("mean_d","sd_d","count_n"))

conf_level <- 0.95

spec_nom <- 12
spec_lo <- 11.9
spec_hi <- 12.1

drive_shaft_t_score <- drive_shaft_t_score %>% 
  mutate(se = mean_d/sqrt(count_n),
         t_score = qt(1-conf_level/2,df = count_n-1),
         margin_of_error = t_score*se,
         conf_int_lo = mean_d-margin_of_error,
         conf_int_hi = mean_d + margin_of_error)

drive_shaft_t_score %>% 
  ggplot()+
  geom_rect(aes(ymin = -Inf, ymax = Inf, xmin = spec_lo, xmax = spec_hi,fill = "spec"),
            alpha = 0.3)+
  stat_eye(
    aes(y = group, xdist = dist_normal(mean = mean_d,sd = sd_d),fill = "normal"),
    alpha = 0.4,
    side = "left"
    )+
  stat_eye(
    aes(y = group, xdist = dist_student_t(df = count_n-1,mu = mean_d,sigma = sd_d),fill = "t"),
    # fill = "red",
    alpha = 0.4,
    side = "right"
    )+
  geom_point(aes(x = mean_d,y = group))+
  # geom_errorbarh(aes(y = group, xmin = mean_d-sd_d, xmax = mean_d + sd_d),height = 0.3)+
  theme_few()+
  labs(
    title = "mean and sd plot of the drive shaft data",
    x = "diameter",
    y = "",
    fill = ""
  )+
  ggrepel::geom_text_repel(
    aes(x = mean_d, 
        y = group, 
        label = paste0("SE: ",round(se,digits = 2))),
    force_pull = -1)+
  scale_fill_grey()+
  geom_errorbarh(aes(y = group, xmin = conf_int_lo, xmax = conf_int_hi),width = 0.3)+
  # scale_fill_manual(
  #   values = c("spec" = "azure2","t" = "gray","normal" = "black")
  #   )+
  theme(legend.position = "bottom")


```

## F - Statistics

```{r,include=FALSE}

x_vec <-  seq(0.1,2,0.01)

comp_f <- function(dof1,dof2,x = seq(0.1,8,0.1)){
  
  d <- df(x = x, df1 = dof1, df2 = dof2)
  
}

tmp <- expand_grid(
  # dof1 = seq(2,30,3),
  dof1 = c(1,5,10,15,20,25,30,50,100), 
  dof2 = c(1,5,10,15,20,25,30,50,100),
  # dof2 = seq(2,30,3)
  ) %>% 
  mutate(
    x = list(x_vec),
    d = map2(.x = dof1, .y = dof2, function(x,y) comp_f(dof1 = x,dof2 = y, x = x_vec))
  ) %>% 
  mutate(
    dof1 = as.factor(dof1),
    dof2 = as.factor(dof2)) %>% 
  unnest(cols = c(d,x))



plt_dof1_min <- tmp %>% 
  filter(dof1==1) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof1==1) %>%
      group_by(dof2) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  geom_text(
    data = tmp %>% 
      filter(dof1==1) %>%
      group_by(dof2) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof2, dof1 = 1",
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof2,labeller = label_both)


plt_dof1_max <- tmp %>% 
  filter(dof1==100) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof1==100) %>%
      group_by(dof1) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "varying dof1, dof2 = 100",
  )+
  geom_text(
    data = tmp %>% 
      filter(dof1==100) %>%
      group_by(dof2) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof2, dof1 = 100",
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof2,labeller = label_both)


plt_dof2_min <- tmp %>% 
  filter(dof2==1) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof2==1) %>%
      group_by(dof1) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "varying dof1, dof2 = 1",
  )+
  geom_text(
    data = tmp %>% 
      filter(dof2==1) %>%
      group_by(dof1) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof1, dof2 = 1",
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof1,labeller = label_both)


plt_dof2_max <- tmp %>% 
  filter(dof2==100) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof2==100) %>%
      group_by(dof1) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "varying dof1, dof2 = 100",
  )+
  geom_text(
    data = tmp %>% 
      filter(dof2==100) %>%
      group_by(dof1) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof1, dof2 = 1",
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof1,labeller = label_both)


max_dat <- expand_grid(
  # dof1 = seq(2,30,3),
  dof1 = seq(1,100), 
  dof2 = seq(1,100),
  # dof2 = seq(2,30,3)
  ) %>% 
  mutate(
    x = list(x_vec),
    d = map2(.x = dof1, .y = dof2, function(x,y) comp_f(dof1 = x,dof2 = y, x = x_vec))
  ) %>% 
  mutate(
    dof1 = as.factor(dof1),
    dof2 = as.factor(dof2)) %>% 
  unnest(cols = c(d,x)) %>% 
  group_by(dof1,dof2) %>% 
  summarise(
    max_d = max(d),
    max_x = x[which.max(d)],
    mean_d = mean(d)
  ) %>% 
  mutate(
    dof1 = as.numeric(as.character(dof1)),
    dof2 = as.numeric(as.character(dof2)),
  ) 

plt_max <- max_dat %>% 
  ggplot(aes(x = dof1, y = dof2))+
  geom_tile(aes(fill = max_d))+
  geom_contour(aes(z = max_d,color = max_d),color = "white",bins = 20)+
  labs(
    title = "maximum density vs. dof1 and dof2"
  )+
  scale_fill_gradient2(low = "white", high = 'black')+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0,0))

plt_max_line <- tmp %>% 
  ggplot(aes(x = x, y = d))+
  geom_path()+
  geom_vline(
    data = tmp %>% 
      # filter(dof2==1) %>%
      group_by(dof1,dof2) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "F - density plots of dof1 vs. dof2"
  )+
  facet_grid(dof1~dof2)
  
```

::: {.content-visible when-profile="script"}

*F-statistics*, also known as the *F-test* or *F-ratio*, is a statistical measure used in {{< acr ANOVA >}} and regression analysis [@1981369198]. 
It assesses the ratio of two variances, indicating the extent to which the variability between groups or models is greater than the variability within those groups or models. 
The *F-statistic* plays a crucial role in hypothesis testing and model comparison.

Significance of F-statistics:
The significance of the F-statistic lies in its ability to help researchers determine whether the differences between group means or the goodness-of-fit of a regression model are statistically significant. 
In {{< acr ANOVA >}}, a high F-statistic suggests that at least one group mean differs significantly from the others, while in regression analysis, it indicates whether the regression model as a whole is a good fit for the data.

Applications of F-statistics:
1. **Analysis of Variance {{< acr ANOVA >}}:** F-statistics are extensively used in {{< acr ANOVA >}} to compare means across two or more groups. 
It helps determine whether there are significant differences among the means of these groups. 
For example, an {{< acr ANOVA >}} might be used to compare the mean test scores of students taught using different teaching methods.

2. **Regression Analysis:** F-statistics are used in regression analysis to assess the overall significance of a regression model. 
Specifically, in multiple linear regression, it helps determine whether the model, which includes multiple predictor variables, is better at explaining the variance in the response variable compared to a model with no predictors. 
It tests the null hypothesis that all coefficients of the model are equal to zero.

```{r}
#| label: fig-f-dist-01
#| out-width: 95%
#| fig-cap: F-distribution for $dof_1$ on the horizontal and $dof_2$ on the vertical axis

plt_max_line

```

The {{< acr dof >}} in an *F-distribution* refer to the two sets of numbers that determine the shape and properties of the distribution (@fig-f-dist-01). 

Numerator Degrees of Freedom ($dof_1$):
The numerator degrees of freedom, often denoted as $dof_1$, is associated with the variability between groups or models in statistical analyses (@fig-f-dist-01 - horizontal axis).
In the context of {{< acr ANOVA >}}, it represents the {{< acr dof >}} associated with the differences among group means.
In regression analysis, it is related to the number of predictors or coefficients being tested simultaneously.

Denominator Degrees of Freedom ($dof_2$):
The denominator degrees of freedom, often denoted as $dof_2$, is associated with the variability within groups or models (@fig-f-dist-02 - vertical axis). 
In {{< acr ANOVA >}}, it represents the degrees of freedom associated with the variability within each group.
In regression analysis, it is related to the error or residual degrees of freedom, indicating the remaining variability not explained by the model.

The F-distribution is used to compare two variances: one from the numerator and the other from the denominator. 
The F-statistic, calculated as the ratio of these variances, follows an F-distribution \eqref{fdist}.

\begin{align}
f(x; dof_1, dof_2) = \frac{{\Gamma\left(\frac{{dof_1 + dof_2}}{2}\right)}}{{\Gamma\left(\frac{{dof_1}}{2}\right)\Gamma\left(\frac{{dof_2}}{2}\right)}} \left(\frac{{dof_1}}{{dof_2}}\right)^{\frac{{dof_1}}{2}} \frac{{x^{\frac{{dof_1}}{2} - 1}}}{{\left(1 + \frac{{dof_1}}{{dof_2}}x\right)^{\frac{{dof_1 + dof_2}}{2}}}} \label{fdist} \\
F_{m,n} = \frac{\chi^2_m/m}{\chi^2_n/n} 
\end{align}

```{r}
#| label: fig-f-dist-02
#| layout-nrow: 2
#| out-width: 95%
#| fig-cap: the maximum density as a function of $dof_1$ and $dof_2$ in a continous parameter space


plt_max

```

In practical terms:
A higher numerator degrees of freedom ($dof_1$) suggests that there are more groups or predictors being compared, which may result in larger F-statistic values.
A higher denominator degrees of freedom ($dof_2$) implies that there is more data within each group or model, which may lead to smaller F-statistic values.
The F-distribution is right-skewed and always positive. 
It has different shapes depending on the values of $dof_1$ and $dof_2$ (@fig-f-dist-02). 
The exact shape is determined by these degrees of freedom and cannot be altered by changing sample sizes or data values (@fig-f-dist-02). 
Researchers use F-distributions to conduct hypothesis tests, such as F-tests in ANOVA and F-tests in regression, to determine if there are significant differences between groups or if a regression model is statistically significant.

In summary, {{< acr dof >}} in the F-distribution are critical in hypothesis testing and model comparisons. 
They help quantify the variability between and within groups or models, allowing statisticians to assess the significance of observed differences and make informed statistical decisions.


:::

::: {.r-stack}

::: {.fragment .fade-out}

\begin{align}
F_{m,n} = \frac{\chi^2_m/m}{\chi^2_n/n} \nonumber
\end{align}

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 100%
#| 
plt_max_line
```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 100%
plt_max
```

:::

:::


## Interconnections

::: {.content-visible when-profile="script"}

1. Normal Distribution
The **Normal Distribution** is characterized by its mean ($\mu$) and standard deviation ($\sigma$), see @fig-interconnections. 
It serves as the foundation for many statistical analyses.

2. Standardized Normal Distribution
The **Standardized Normal Distribution**, denoted as $Z \sim N(0, 1)$, is a special case of the normal distribution. 
It has a mean ($\mu$) of $0$ and a standard deviation ($\sigma$) of $1$. 
It is obtained by standardizing a normal distribution variable $X$: $Z = \frac{X - \mu}{\sigma}$ (@fig-interconnections).

3. t Distribution
The **t Distribution** is related to the normal distribution and depends on {{< acr dof >}}. 
As {{< acr dof >}} increases, the t-distribution approaches the standard normal distribution (@fig-interconnections).

4. Chi-Square Distribution
The **Chi-Square Distribution** is indirectly connected to the normal distribution through the concept of "sum of squared standard normals." 
When standard normal random variables ($Z$) are squared and summed, the resulting distribution follows a chi-square distribution.

5. F Distribution
The **F Distribution** arises from the ratio of two independent chi-square distributed random variables. 
It is used for comparing variances between groups in statistical tests like {{< acr ANOVA >}}.

:::

```{r}
#| label: fig-interconnections
#| out-width: 75%
#| fig-cap: The distributions are interconnected in several different ways.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter000","009_DistributionConnection.png"))
```



## Weibull - Distribution

::: {.content-visible when-profile="script"}

```{r}
#| label: fig-wbll-dist
#| out-width: 95%
#| fig-cap: The weibull distribution and the influence of $\beta$ and $\lambda$

x_vec <- seq(0.1,8,0.01)

wbll_scl <- seq(1,4,1)
wbll_shp <- c(seq(0.1,1,length.out = 4),seq(1,4,1))


wbll <- expand_grid(wbll_scl,wbll_shp) %>% 
  mutate(
    x = list(x_vec),
    d = map2(.x = wbll_scl,.y = wbll_shp, .f = function(x,y) dweibull(x_vec,y,x))
  ) %>% 
  unnest(cols = c(d,x)) %>% 
  mutate(shp_lbl = 
           case_when(
             wbll_shp<1~"beta < 1",
             wbll_shp==1~"beta == 1",
             TRUE~"beta > 1"
           ),
         scl_lbl = paste0("lambda == ",wbll_scl)
           )
  
plt_wbll <- wbll %>% 
  ggplot(aes(x = x, y = d, linetype = as.factor(wbll_shp)))+
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),geom = "area",alpha = 0.1,show.legend = FALSE)+
  geom_line()+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0.05,0))+
  facet_grid(shp_lbl~scl_lbl,labeller = label_parsed)+
  labs(
    title = bquote("The weibull distribution with scale"~(lambda)~"and shape"~(beta)~"parameter"),
    linetype = bquote(beta)
  )

plt_wbll

```

The Weibull distribution is a probability distribution frequently used in statistics and reliability engineering to model the time until an event, particularly failures or lifetimes. 
It is named after Wallodi Weibull[^3], who developed it in the mid-20th century [@Weibull_1951].

The Weibull distribution is characterized by two parameters:

**Shape Parameter ($\beta$):** This parameter determines the shape of the distribution curve and can take on values greater than 0. 
Depending on the value of $\beta$, the Weibull distribution can exhibit different behaviors:

If $\beta < 1$, the distribution has a decreasing failure rate, indicating that the probability of an event occurring decreases over time. This is often associated with "infant mortality" or early-life failures.
If $\beta = 1$, the distribution follows an exponential distribution with a constant failure rate over time.
If $\beta > 1$, the distribution has an increasing failure rate, suggesting that the event becomes more likely as time progresses. 
This is often associated with "wear-out" failures.

**Scale Parameter ($\lambda$):** This parameter represents a characteristic scale or location on the time axis. 
It influences the position of the distribution on the time axis. 
A larger $\lambda$ indicates that events are more likely to occur at later times.

**Applications:**
- Reliability Engineering: The Weibull distribution is extensively used in reliability engineering to assess the lifetime and failure characteristics of components and systems. 
Engineers can estimate the distribution parameters from data to predict product reliability, set warranty periods, and plan maintenance schedules.

- Survival Analysis: In medical research and epidemiology, the Weibull distribution is employed to analyze survival data, such as time until the occurrence of a disease or death. 
It helps in modeling and understanding the progression of diseases and the effectiveness of treatments.

- Economics and Finance: The Weibull distribution is used in finance to model the time between financial events, like market crashes or loan defaults. 
It can provide insights into risk assessment and portfolio management.

[^3]: Waloddi Weibull (1887–1979) was a Swedish engineer and statistician known for his work on the Weibull distribution, which is widely used in reliability engineering and other fields.

:::


::: {.r-stack}

::: {.fragment .fade-out}

* Time-to-event ([time to failure (TTF)](#TTF)) continuous distribution function
* Examples: time users spend on web pages, time until a product fails

:::

::: {.fragment .fade-in-then-out}

```{r}
#| label: fig-bathtub
#| out-width: 75%
#| fig-cap: The classical bathtub curve [@LienigUnknownTitle2017]

knitr::include_graphics(here::here("chapter001","bathtub.png"))

```

:::

::: {.fragment .fade-in-then-out style="font-size: 75%;"}

\begin{align}
f_X(x; \lambda, \beta) = \left\{ \begin{array}{cl}
    \frac{\beta}{\lambda}(\frac{x}{\beta})^{\beta-1} e^{-(x/\lambda)^\beta} & \ ; \ x \geq 0 \\
    0 & \ ; \ x < 0 \end{array} \right. 
\end{align}

* $\lambda$ scale parameter
* $\beta$ shape parameter
* Interpretation of Shape Parameter ($\beta$):
  * $\beta < 1$ Failure rate decreases over time (e.g., due to “infant mortality”).
  * $\beta = 1$ Constant failure rate (random external events causing mortality).
  * $\beta > 1$ Failure rate increases with time (aging process).

:::

::: {.fragment .fade-in-then-out}


```{r}
#| label: fig-wbll-dist-slides
#| out-width: 85%
#| fig-cap: The influence of scale ($\lambda$) and shape ($\beta$) parameter on the disitribution.

plt_wbll

```

:::

:::


#### Classroom example: Weibull Distribution

* 6 students roll a die
* 1 student records the number of rolls it takes until a $6$ is rolled ("the event")
* calculate the mean of the "times" as well as the standard distribution
* draw a histogram of the data

### The drive shaft exercise - Weibull distribution

::: {.content-visible when-profile="script"}

The Weibull distribution can be applied to estimate the probability of a part to fail after a given time.
Suppose there have been $n=100$ drive shafts produced.
In order to assure that the assembled drive shaft would last during their service time, they have been tested in a test-stand that mimics the mission profile[^4] of the product.
This process is called *qualification* and a big part of any product development [@Meyna2023].
The measured hours are shown in @fig-ds-wbll in a histogram of the data.
On the `x-axis` the `Time to failure`is shown, while the `y-axis` shows the number of parts that failed within the time.
They histogram plot is overlayed with an empirical density plot as a solid line, as well as the theoretical distribution as a dotted line (Luckily, we know the distribution parameters).

[^4]: A mission profile for parts is a detailed plan specifying how specific components in a system should perform, considering factors like environment, performance, safety, and compliance.

:::

```{r}
#| label: fig-ds-wbll
#| out-width: 95%
#| fig-cap: The measured hours how long the drive shafts lasted in the test stand.

load(here("data","drive_shaft_failures.Rdata"))

drive_shaft_failure %>% 
  ggplot(aes(x = Time_to_Failure))+
  geom_histogram(color = "white", alpha = 0.7) +
  geom_density(aes(y = after_stat(count)*70,linetype = "empirical"))+
  stat_theodensity(
    aes(y = after_stat(count)*70,linetype = "theoretical"),
    distri = "weibull",
    )+
  scale_x_continuous(
    expand = c(0,0,0,0),
    breaks = scales::pretty_breaks()
    )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
    breaks = scales::pretty_breaks()
    )+
  labs(title = "Drive Shaft Failure Times",
       x = "Time to Failure (Hours)",
       y = "Frequency") 

```

```{r, include=FALSE}

wbll_fit <- fitdistrplus::fitdist(drive_shaft_failure %>% pull("Time_to_Failure"),distr = "weibull")

time_threshold <- 500

probability_failure_after_500h <- 1 - pweibull(time_threshold, coef(wbll_fit)[["shape"]], coef(wbll_fit)[["scale"]])

probability_failure_after_500h

```


## Poisson - Distribution

::: {.content-visible when-profile="script"}

The Poisson distribution is a probability distribution commonly used in statistics to model the number of events that occur within a fixed interval of time or space, given a known average rate of occurrence. 
It is named after the French mathematician Siméon Denis Poisson[^5].

The Poisson distribution is an applicable probability model in such situations under specific conditions:

**1. Independence:** Events should occur independently of each other within the specified interval of time or space. 
This means that the occurrence of one event should not affect the likelihood of another event happening.

**2. Constant Rate:** The average rate (*lambda*, denoted as $\lambda$) at which events occur should be constant over the entire interval.
In other words, the probability of an event occurring should be the same at any point in the interval.

**3. Discreteness:** The events being counted must be discrete in nature. 
This means that they should be countable and should not take on continuous values.

**4. Rare Events:** The Poisson distribution is most appropriate when the events are rare, meaning that the probability of more than one event occurring in an infinitesimally small interval is negligible. 
This assumption helps ensure that the distribution models infrequent events.

**5. Fixed Interval:** The interval of time or space in which events are counted should be fixed and well-defined. 
It should not vary or be open-ended.

**6. Memorylessness:** The Poisson distribution assumes that the probability of an event occurring in the future is independent of past events. 
In other words, it does not take into account the history of events beyond the current interval.

**7. Count Data:** The Poisson distribution is most suitable for count data, where you are interested in the number of events that occur in a given interval.

In the context of a Poisson distribution, the parameter lambda ($\lambda$) represents the average rate of events occurring in a fixed interval of time or space. 
It is a crucial parameter that helps define the shape and characteristics of the Poisson distribution. 

**Average Rate:** $\lambda$ is a positive real number that represents the average or expected number of events that occur in the specified interval. 
It tells you, on average, how many events you would expect to observe in that interval.

**Rate of Occurrence:** $\lambda$ quantifies the rate at which events happen. 
A higher value of $\lambda$ indicates a higher rate of occurrence, while a lower value of $\lambda$ indicates a lower rate.

**Shape of the Distribution:** The value of $\lambda$ determines the shape of the Poisson distribution. Specifically:

When $\lambda$ is small, the distribution is skewed to the right and is more concentrated toward zero (@fig-pois).
When $\lambda$ is moderate, the distribution approaches a symmetric bell shape (@fig-pois).
When $\lambda$ is large, the distribution becomes increasingly similar to a normal distribution(@fig-pois).

[^5]: Siméon Denis Poisson (1781-1840) was a notable French mathematician, renowned for his work in probability theory and mathematical physics.

:::

```{r}
#| label: fig-pois
#| out-width: 75%
#| fig-cap: The poisson distribution with different $\lambda$ values.

# Create a data frame with different lambda values
lambda_values <- c(0.1,0.5,seq(1, 5, by = 1),10)
data <- expand.grid(x = 0:15, lambda = lambda_values)
data$probability <- dpois(data$x, lambda = data$lambda)

# Create the Poisson distribution plot
ggplot(data, aes(x = x, y = probability, linetype = as.factor(lambda))) +
  geom_line() +
  labs(title = bquote("Poisson Distribution with Different"~lambda~"Values"),
       y = "d",
       linetype = bquote(lambda)
       ) +
  scale_color_discrete(name = "Lambda")+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0.05,0))

```



