---
title: "Inferential Statistics"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(patchwork)
library(gghalves)
library(ggridges)
library(here)
library(ggthemes)
library(gt)
library(gtsummary)
library(broom)
library(pwr)
library(qqplotr)
library(ggpubr)
library(datasauRus)
library(car)
library(ggh4x)
library(gghighlight)
library(rstatix)
library(SensorLab)
library(gtExtras)
library(effectsize)
library(nortest)

ggplot2::theme_set(
  ggthemes::theme_few(base_size = 15)
  )

```

::: {.content-visible when-profile="script"}

Inferential statistics involves making predictions, generalizations, or inferences about a population based on a sample of data. 
These techniques are used when researchers want to draw conclusions beyond the specific data they have collected. 
Inferential statistics help answer questions about relationships, differences, and associations within a population. 



:::

## Hypothesis Testing - Basics

```{r}
#| label: fig-hypothesis
#| out-width: 85%
#| fig-cap: We are hypotheses.

knitr::include_graphics(here::here("chapter002","004_Hypothesis.png"))
```

::: {.content-visible when-profile="script"}

**{{< acr H0 >}}** This is the default or status quo assumption. 
It represents the belief that there is no significant change, effect, or difference in the production process. 
It is often denoted as a statement of equality (e.g., the mean production rate is equal to a certain value).

**{{< acr Ha >}}:** This is the claim or statement we want to test. 
It represents the opposite of the null hypothesis, suggesting that there is a significant change, effect, or difference in the production process (e.g., the mean production rate is not equal to a certain value).

:::

## Statistical errors

```{r}
#| label: fig-stat-errors
#| fig-cap: The statistical Errors (Type I and Type II).
#| out-width: 75%

knitr::include_graphics(here::here("chapter002","007_StatisticalErrors.png"))
```

::: {.content-visible when-profile="script"}

* Type I Error (False Positive, see @fig-stat-errors):

A Type I error occurs when a null hypothesis that is actually true is rejected.
In other words, it's a false alarm.
It is concluded that there is a significant effect or difference when there is none.
The probability of committing a Type I error is denoted by the [significance level $\alpha$](#sign-level). 
*Example:* Imagine a drug trial where the null hypothesis is that the drug has no effect (it's ineffective), but due to random chance, the data appears to show a significant effect, and you incorrectly conclude that the drug is effective (Type I error).

* Type II Error (False Negative, see @fig-stat-errors):

A Type II error occurs when a null hypothesis that is actually false is not rejected.
It means failing to detect a significant effect or difference when one actually exists.
The probability of committing a Type II error is denoted by the symbol $\beta$.
*Example:* In a criminal trial, the null hypothesis might be that the defendant is innocent, but they are actually guilty. 
If the jury fails to find enough evidence to convict the guilty person, it is a Type II error.

Type I Error is falsely concluding, that there is an effect or difference when there is none (false positive).
Type II Error failing to conclude that there is an effect or difference when there actually is one (false negative).

The relationship between *Type I* and *Type II* errors is often described as a trade-off. 
As the risk of Type I errors is reduced by lowering the significance level ($\alpha$), the risk of Type II errors ($\beta$) is typically increased (@fig-stat-rates). 
This trade-off is inherent in hypothesis testing, and the choice of significance level depends on the specific goals and context of the study. 
Researchers often aim to strike a balance between these two types of errors based on the consequences and costs associated with each. 
This balance is a critical aspect of the design and interpretation of statistical tests.

:::

## Significance

```{r}
#| label: fig-stat-rates
#| fig-cap: Type I and Type II error in the context of inferential statistics.
#| out-width: 75%

knitr::include_graphics(here::here("chapter002","008_Rates.png"))
```

::: {.content-visible when-profile="script"}

The p-value is a statistical measure that quantifies the evidence against a null hypothesis. 
It represents the probability of obtaining test results as extreme or more extreme than the ones observed, assuming the null hypothesis is true. 
In hypothesis testing, a smaller p-value indicates stronger evidence against the null hypothesis.
If the p-value is less than or equal to $\alpha$ ($p \leq \alpha$), you reject the null hypothesis.
If the p-value is greater than $\alpha$ ( $p > \alpha$ ), you fail to reject the null hypothesis.
A common threshold for determining statistical significance is to reject the null hypothesis when $p\leq\alpha$.

The p-value however does not give an assumption about the effect size, which can be quite insignificant [@Nuzzo_2014].
While the p-value therefore is the probability of accepting $H_a$ as true, it is not a measure of magnitude or relative importance of an effect.
Therefore the [CI](#ci) and the effect size should always be reported with a p-value.
Some Researchers even claim that most of the research today is false [@Ioannidis_2005].
In practice, especially in the manufacturing industry, the p-value and its use is still popular.
Before implementing any measures in a series production, those questions will be asked.
The confident and reliable engineer asks them beforehand and is always his own greatest critique.

:::





::: {.content-visible when-profile="slides"}

### The drive shaft exercise - Hypotheses

:::{.incremental .v-center-container}

{{< acr H0 >}}:
: The drive shaft diameter is within the specification.

{{< acr Ha >}}:
: The drive shaft diameter is not within the specification.

:::

:::

::: {.content-visible when-profile="script"}

### The drive shaft exercise - Hypotheses

During the {{< acr QC >}} of the drive shaft $n=100$ samples are taken and the diameter is measured with an accuracy of $\pm 0.01mm$.
Is the true mean of all produced drive shafts within the specification?

For this we can formulate the hypotheses.

{{< acr H0 >}}
: The drive shaft diameter is within the specification.

{{< acr Ha >}}:
: The drive shaft diameter is not within the specification.

In the following we will explore, how to test for these hypotheses.

:::


## {{< acr CI >}}

::: {.content-visible when-profile="script"}

A {{< acr CI >}} is a statistical concept used to estimate a range of values within which a population parameter, such as a population mean or proportion, is likely to fall. 
It provides a way to express the uncertainty or variability in our sample data when making inferences about the population. 
In other words, it quantifies the level of confidence we have in our estimate of a population parameter.

Confidence intervals are typically expressed as a range with an associated confidence level. 
The confidence level, often denoted as $1-\alpha$, represents the probability that the calculated interval contains the true population parameter. 
Common confidence levels include $90\%$, $95\%$, and $99\%$.

There are different ways of calculating {{< acr CI >}}).

1. For the population mean [$\mu_0$](index.qmd#truemean-gloss) when the population standard deviation [$\sigma_0^2$](index.qmd#truevariance-gloss) is known (\eqref{ci01}).

\begin{align}
CI = \bar{X} \pm t \frac{\sigma_0}{\sqrt{n}} \label{ci01}
\end{align}

* $\bar{X}$ is the sample mean.

* $Z$ is the critical value from the standard normal distribution corresponding to the desired confidence level (e.g., $1.96$ for a $95\%$ confidence interval).

* $\sigma_0$ is the populations standard deviation

* $n$ is the sample size

2.For the population mean [$\mu_0$](index.qmd#truemean-gloss) when the population standard deviation [$\sigma_0^2$](index.qmd#truevariance-gloss) is unknown (t-confidence interval), see \eqref{ci02}.

\begin{align}
CI = \bar{X} \pm t \frac{sd}{\sqrt{n}} \label{ci02}
\end{align}

* $\bar{X}$ is the sample mean.

* $t$ is the critical value from the t-distribution with $n-1$ degrees of freedom corresponding to the desired confidence level

* $sd$ is the sample standard deviation

* $n$ is the sample size

3. For a population proportion [p](index.qmd#popprop-gloss), see \eqref{ci03}.

\begin{align}
CI = \hat{p} \pm Z \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \label{ci03}
\end{align}

* $\hat{p}$ is the sample proportion

* $Z$ is the critical value from the standard normal distribution corresponding to the desired confidence level

* $n$ is the sample size

4. The method for calculating confidence intervals may vary depending on the estimated parameter. 
Estimating a population median or the differences between two population means, other statistical techniques may be used.

:::

```{r}
#| label: fig-ci-lit
#| fig-cap: Point estimates vs. confidence intervals [@statistical_ismaykim_2019].
#| out-width: 95%
#| fig-pos: "H"

knitr::include_graphics(here::here("SamplingMethods","point_estimate_vs_conf_int.png"))
```

### Some basics

\begin{align}
P(L \leq \theta \leq U) = 1- \alpha
\end{align}

... with $L$ and $U$ being computed from sample data.

::: {.fragment .fade-in}

A $95\%$ {{< acr CI >}} for the population mean $\mu_0$ implies that if we repeated the sampling process infinitely, $95\%$ of the computed intervals would contain $\mu$.

:::

### Components of a {{< acr CI >}}

1. **Point Estimate**: The sample statistic ($\bar{x},\hat{p}$)
2. **{{< acr SE >}}**: Measures the variability of the point estimate
3. **Critical Value**: Derived from the sampling distribution
4. **{{< acr ME >}}**: $ME = \text{Critical Value} \times SE$

\begin{align}
\text{CI} = \text{Point Estimate} \pm \text{ME}
\end{align}

### The drive shaft exercise - Confidence Intervals

```{r}
#| label: fig-ci
#| out-width: 75%
#| fig-cap: The 95% CI for the drive shaft data.

load(here("data","drive_shaft_data.Rdata"))

drive_shaft <- drive_shaft %>% 
  pivot_wider(names_from = group,values_from = diameter) %>% 
  select(group01,sample_no)

conf_interval <- confint(lm(group01~1,data = drive_shaft),level = 0.95) %>% 
  as_tibble() %>% 
  tidy()

conf_interval <- tidy(lm(group01~1,data = drive_shaft),level = 0.95,conf.int = TRUE) 

drive_shaft <- drive_shaft %>% 
  mutate(conf.low = conf_interval$conf.low,
         conf.high = conf_interval$conf.high,
         in_conf = case_when(
           (group01 > conf.low & group01 < conf.high) ~ TRUE,
           TRUE ~ FALSE
         )
         )

drive_shaft %>% 
  # ggplot(aes(x = factor("group01")))+
  ggplot()+
  geom_histogram(aes(x = group01),color = "white")+
  geom_rect(data = conf_interval,
            aes(xmin = conf.low, 
                xmax = conf.high, 
                ymin = -Inf, 
                ymax = Inf,
                fill = "CI"),
            alpha=0.5)+
  geom_vline(data = conf_interval,
            aes(xintercept = conf.low),
            color = "azure3",
            # alpha=0.5
            )+
  geom_vline(data = conf_interval,
            aes(xintercept = conf.high),
            color = "azure3",
            )+
  geom_density(aes(x = group01),color = "azure1") +
  scale_fill_manual(values = c("CI"="azure3"))+
  geom_vline(aes(xintercept = mean(group01),linetype = "mean"),color = "black",key_glyph = draw_key_path)+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_linetype_manual(values = c(mean = "dashed"))+
  theme(legend.position = "bottom")+
  labs(
    title = "The confidence interval for the drive shaft data",
    x = "diameter in mm",
    y = "count",
    fill = "",
    linetype = ""
    )+
  theme_few()+
  theme(legend.position = "bottom")
  
```

::: {.content-visible when-profile="script"}

The $95\%$ {{< acr CI >}} for the drive shaft data is shown in @fig-ci.
For comparison the histogram with an overlayed density curve is plotted.
The highlighted area shows the minimum and maximum {{< acr CI >}}, the calculated mean is shown as a dashed line.

:::

::: {.content-visible when-profile="script"}

## Significance Level 

The significance level $\alpha$ is a critical component of hypothesis testing in statistics. 
It represents the maximum acceptable probability of making a Type I error, which is the error of rejecting a null hypothesis when it is actually true. 
In other words, $\alpha$ is the probability of concluding that there is an effect or relationship when there isn't one.
Commonly used significance levels include $0.05 (5\%)$, $0.01 (1\%)$, and $0.10 (10\%)$. 
The choice of $\alpha$ depends on the context of the study and the desired balance between making correct decisions and minimizing the risk of Type I errors.

:::

## False negative - risk

The risk for a false negative outcome is called [$\beta$ - risk](#beta-risk).
Is is calculated using statistical power analysis. 
Statistical power is the probability of correctly rejecting a null hypothesis when it is false, which is essentially the complement of beta ($\beta$). 

\begin{align}
\beta = 1 - \text{Power}
\end{align}


## Power Analysis

::: {.content-visible when-profile="script"}

Statistical power is calculated using software, statistical tables, or calculators specifically designed for this purpose.
Generally speaking: The greater the statistical power, the greater is the evidence to accept or reject the $H_0$ based on the study.
Power analysis is also very useful in determining the sample size before the actualy experiments are conducted.
Below is an example for a power calculation for a two-sample t-test.

$$
\text{Power} = 1 - \beta = P\left(\frac{{|\bar{X}_1 - \bar{X}_2|}}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}} > Z_{\frac{\alpha}{2}} - \frac{\delta}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}\right)
$$

1. Effect Size: 
This represents the magnitude of the effect you want to detect. 
Larger effects are easier to detect than smaller ones.

2. Significance Level ($\alpha$): This is the predetermined level of significance that defines how confident you want to be in rejecting the null hypothesis (e.g., typically set at 0.05).

3. Sample Size ($n$): 
The number of observations or participants in your study. Increasing the sample size generally increases the power of the test.

4. Power ($1 - \beta$): 
This is the probability of correctly rejecting the null hypothesis when it is false. 
Higher power is desirable, as it minimizes the chances of a Type II error (failing to detect a true effect).

5. Type I Error ($\alpha$): 
The probability of incorrectly rejecting the null hypothesis when it is true. 
This is typically set at $0.05$ or $5\%$ in most studies.

6. Type II Error ($\beta$): 
The probability of failing to reject the null hypothesis when it is false. Power is the complement of $\beta$ ($Power = 1 - \beta$).

:::

### Why Power analysis

- How large should a sample be?
- What's the probability my experiment will succeed (as in reject $H_0$ when $H_0$ is false)
- Is my study even worth running given the effect size I expect

### Power defined

**Power** = Probability of rejecting $H_0$ when $H_a$ is true.

Mathematically:

\begin{align}
\text{Power} = P(\text{Reject }H_0|H_1 \text{ is true}) = 1 -\beta
\end{align}

- Significance level ($\alpha$): Threshold for rejecting $H_0$ (e.g. $0.05$)
- Effect Size: For the Coin Toss example $p = 0.6$ vs. $p = 0.5$
- Sample Size ($n$)

::: {.content-visible when-profile="script"}

### Power and $\beta$ error

The sample size $n = 23$, meaning $23$ coin flips means that the statistical power is $80\%$  at a $\alpha = 0.05$ significance level ($\beta = 1-power = 0.2 \approx 20\%$).
But what if the sample size varies?
This is the subject of @fig-pwr-calc.
On the `x-axis` the power is shown (or the $\beta$-risk on the upper `x-axis`), whereas the sample size `n` is depicted on the `y-axis`.
To increase the power by $10\%$ to be $90\%$ the sample sized must be increased by $11$.
A further power increase of $5\%$ would in turn mean an increase in sample size to be $n = 40$.
This highlights the non-linear nature of power calculations and why they are important for experimental planning.

:::

### Example: The Coin Toss

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| label: fig-ct-prob
#| fig-cap: The coin toss with the respective probabilites [@pwr].
#| out-width: 60%
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","023_CoinToss.png"))
```

:::

::: {.fragment .fade-in-then-out}

Define $H_0$ and $H_a$

:::

::: {.fragment .fade-in-then-out}

H0: The coin is fair and lands heads $50\%$ of the time.

Ha: The coin is loaded and lands heads more than $50\%$ of the time.

:::

::: {.fragment .fade-in-then-out}

Power computation for $n = 10,20,30,50$ with $\alpha = 0.05$

:::

::: {.fragment .fade-in-then-out}

Define the test statistic

:::

::: {.fragment .fade-in-then-out}

For a binomial test, the test statistic is the number of head $X$ under $H_0$, $X \sim \text{Binomial}(n,0.5)$

:::

::: {.fragment .fade-in-then-out}

Critical Value

:::

::: {.fragment .fade-in-then-out}

Reject $H_0$ if $x\geq c$, where $c$ is the smallest integer such that

$$P(X\geq c) | H_0) \leq \alpha$$

:::

::: {.fragment .fade-in-then-out}

Power calculation

:::

::: {.fragment .fade-in-then-out}

$$\text{Power} = P(X\geq c|H_1)$$

:::

:::

#### Power calculations in this example

- The *critical value* is the smallest integer where $P(X\geq c)\leq \alpha$
  - For $n = 10$ and $\alpha = 0.05$ `qbinom(0.95,10,0.5)` returns $8$, meaning we would reject $H_0$ if there are **8 or more heads** in 10 flips

##### Power is the probability of observing $X\geq c$ under $H_1$

  - `pbinom(c-1,n,p_h1)` computes $P(X\leq c-1 |H_1)$
  - `1-pbinom(...)` gives $P(X\geq c | H_1)$
  - $n=10$, $c = 8$ and $p_{H1} = 0.6$: `1-pbinom(7,10,0.6) = ` `r 1-pbinom(7,10,0.6)`  

### Using packages

```{r}
#| label: pwr-calc
#| echo: true

pwr::pwr.p.test(h = pwr::ES.h(p1 = 0.6, p2 = 0.50),
           sig.level = 0.05,
           power = 0.80,
           alternative = "greater")

```

### Power vs. sample size

```{r}
#| label: fig-pwr-calc
#| out-width: 95%
#| fig-cap: The power vs. the sample size

pwr_df <- data.frame(
  p1 = 0.75,
  p2 = 0.5,
  sig_level = 0.05,
  pwr = seq(0.2,0.999,0.001),
  alternative = "greater"
  ) %>% 
  rowwise() %>% 
  mutate(
    ES = ES.h(p1 = p1, p2 = p2),
    n = list(pwr.p.test(
      h = ES,
      sig.level = sig_level,
      power = pwr,
      alternative = "greater"
    )[["n"]] 
    )
  ) %>% 
  unnest(n)

pwr_df %>% 
  ggplot(aes(x = pwr, y = n))+
  geom_line()+
  scale_x_continuous(
    expand = c(0,0,0,0),
    limits = c(0.2,1),
    labels = scales::percent,
    breaks = seq(0,1,0.05),
    sec.axis = sec_axis(
      trans = ~ 1- . ,
      labels = scales::percent,
      name = bquote("" %<-% beta*" risk"),
      breaks = seq(0,1,0.05)
    )
  )+
  scale_y_continuous(
    breaks = seq(0,100,5),
    expand = c(0,0,0,0)
  )+
  labs(
    title = expression("Power and "*beta*" risk for the example"),
    x = bquote("" %->% " Power"),
    y = "sample size n"
  )+
  theme_few()

```

### Power and the different parameters


```{r}
#| label: fig-pwr-calc-params
#| out-width: 95%
#| fig-cap: The power vs. the sample size for different effect sizes

p1 <- c(seq(0.6,0.8, 0.1),0.65)
p2 <- c(0.5)
sig_level <- seq(0.01,0.05, 0.01)
pwr = seq(0.3,0.95,0.01)

pwr_df_params <- expand_grid(p1,p2,sig_level,pwr) %>% 
  add_column(alternative = "greater") %>% 
  rowwise() %>% 
  mutate(
    ES = ES.h(p1 = p1, p2 = p2),
    n = list(pwr.p.test(
      h = ES,
      sig.level = sig_level,
      power = pwr,
      alternative = "greater"
    )[["n"]] 
    )
  ) %>% 
  unnest(n)

pwr_df_params %>% 
  ggplot(aes(x = pwr, y = n, linetype = as.factor(sig_level)))+
  geom_path()+
  geom_vline(xintercept = 0.8, color = "gray")+
  scale_x_continuous(
    expand = c(0,0,0,0),
    limits = c(0.3,0.95),
    labels = scales::percent,
    breaks = seq(0,1,0.1),
    sec.axis = sec_axis(
      trans = ~ 1- . ,
      labels = scales::percent,
      name = bquote("" %<-% beta*" risk"),
      breaks = seq(0,1,0.1)
    )
  )+
  scale_y_continuous(
    # breaks = seq(0,100,5),
    expand = c(0,0,0,0)
  )+
  facet_wrap(
    ~as.factor(p1),
    # scales = "free"
  )+
  labs(
    title = expression("Power and "*beta*" risk for the example"),
    x = bquote("" %->% " Power"),
    y = "sample size n"
  )+
  theme_few()

```



## Effect Sizes

After computing statistical power, we must ask: *"How meaningful is the effect we’re powered to detect?"*
Effect sizes quantify the **magnitude** of a phenomenon—separate from sample size or significance.

### What is an effect size?

- A **standardized** measure of the strength of a relationship or difference.
- Answers: *"How large is the effect in the population?"* (not just *"Is it nonzero?"*).
- **Key Property**: Independent of sample size (unlike *p*-values).

**Analogy**:
Power tells you if your telescope is strong enough to see a star (*significance*).
Effect size tells you how bright the star is (*meaningfulness*).

### Types of effect sizes

```{r}
#| label: tbl-effectsize-calc
#| tbl-cap: Some parametric and non-parametric statistical tests.

# Create the effect size table data
effect_size_table <- tribble(
  ~"Analysis Type",       ~"Common Effect Size Metric", ~"Interpretation",                     ~"R Function (tidyverse)",
  "Mean Difference",      "Cohen’s *d*",                "Small: 0.2, Medium: 0.5, Large: 0.8", "`effectsize::cohen_d()`",
  "Correlation",          "Pearson’s *r*",              "Small: 0.1, Medium: 0.3, Large: 0.5", "`correlation::correlation()`",
  "Binary Outcome",       "Odds Ratio (OR) / Risk Ratio (RR)", "OR = 2: 2x higher odds in group A", "`effectsize::odds_ratio()`",
  "ANOVA (Group Diffs)", "Eta-squared (η²) / Omega-squared (ω²)", "Proportion of variance explained", "`effectsize::eta_squared()`",
  "Regression",           "Standardized β (beta)",     "Change in SD units per 1-SD predictor change", "`parameters::standardize_parameters()`"
)

# Format the table with gt
effect_size_gt <-
  effect_size_table %>%
  gt() %>%
  # Add a title and subtitle
  tab_header(
    title = md("**Common Effect Size Metrics in Statistical Analysis**"),
    subtitle = md("*Interpretation guidelines and tidyverse functions*")
  ) %>%
  # Format columns for clarity
  fmt_markdown(columns = everything()) %>%
  # Center-align text for readability
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body()
  ) 

effect_size_gt

```

### Computing effect sizes in R

```{r}
#| echo: true


# Simulate data: Treatment vs. Control
set.seed(123)
data_effect <- tibble(
  group = rep(c("Treatment", "Control"), each = 50),
  score = c(rnorm(50, mean = 85, sd = 10), rnorm(50, mean = 80, sd = 10))
)

# Compute Cohen's d (correct formula syntax)
effectsize::cohens_d(score ~ group, data = data_effect) 

```

### Visualization

```{r}
#| label: fig-effectsize-data
#| out-width: 95%
#| fig-cap: The visualization of the data

data_effect |> 
  ggplot(
    aes(
      x = group,
      y = score
    )
  )+
  geom_boxplot()+
  geom_jitter()+
  stat_summary(
    fun = "median",
    geom = "line",
    aes(
      group = 1
    )
  )
  

```

### Simple example

- Old process: $\text{Mean diameter:}\;10.02mm,\; sd = 0.1mm\;(n = 50)$
- New process: $\text{Mean diameter:}\;10.00mm,\; sd = 0.05mm\;(n = 50)$

What is the effect size ($d$) of changing to the new process

### Visualizations

```{r}
#| label: fig-effectsize-simple-example
#| out-width: 95%
#| fig-cap: Distribution of the process data

d_data <- data.frame(
  old = rnorm(50,mean = 10.02, sd = 0.1),
  new = rnorm(50,mean = 10,sd = 0.05)
) |> 
  pivot_longer(
    cols = c("old","new"),
    values_to = "rod_diameter",
    names_to = "process"
  )


d_data |> 
  ggplot(
    aes(
      x = rod_diameter,
      y = process
    )
  )+
  geom_density_ridges()+
  labs(
    title = "Is the new process really better?"
  )


```

### Calculate Cohens $d$ (Mean Difference)

\begin{align}
d = \frac{\bar{x}_{new}-\bar{x}_{old}}{sd_{pooled}}
\end{align}

Pooled sd:

\begin{align}
sd_{pooled} = \sqrt{\frac{(n_1-1)sd_{1}^2+(n_2-1)sd_{2}^2}{n_1+n_2-1}}\approx 0.076
\end{align}

Effect Size ($d$):

$$d = \frac{10.00-10.02}{0.076} = \frac{-0.02}{0.076}\approx -0.26$$

## Parametric and Non-parametric Tests {.r-stretch}

::: {.content-visible when-profile="script"}

Parametric and non-parametric tests in statistics are methods used for analyzing data. 
The primary difference between them lies in the assumptions they make about the underlying data distribution:

1. Parametric Tests:
   - These tests assume that the data follows a specific probability distribution, often the normal distribution.
   - Parametric tests make assumptions about population parameters like means and variances.
   - They are more powerful when the data truly follows the assumed distribution.
   - Examples of parametric tests include t-tests, ANOVA, regression analysis, and parametric correlation tests.

2. Non-Parametric Tests:
   - Non-parametric tests make minimal or no assumptions about the shape of the population distribution.
   - They are more robust and can be used when data deviates from a normal distribution or when dealing with ordinal or nominal data.
   - Non-parametric tests are generally less powerful compared to parametric tests but can be more reliable in certain situations.
   - Examples of non-parametric tests include the Mann-Whitney U test, Wilcoxon signed-rank test, Kruskal-Wallis test, and Spearman's rank correlation.

The choice between parametric and non-parametric tests depends on the nature of the data and the assumptions. 
Parametric tests are appropriate when data follows the assumed distribution, while non-parametric tests are suitable when dealing with non-normally distributed data or ordinal data.
Some examples for parametric and non-parametric tests are given in @tbl-ParamTestsvsNonParamTests.

:::


```{r}
#| label: tbl-ParamTestsvsNonParamTests
#| tbl-cap: Some parametric and non-parametric statistical tests.

ParamTests_NonParamTests <- 
  data.frame(
    `param` = 
      c("One-sample t-test", "Paired t-test", "Two-sample t-test", "One-Way ANOVA"),
    `non_param` = 
      c("Wilcoxon signed rank test", "Mann-Whitney U test", "Kruskal Wallis test", "Welch Test")
  )

ParamTests_NonParamTests %>% 
  gt() %>% 
  cols_label(param = md("**Parametric Tests**"),
             non_param = md("**Non-Parametric Tests**")) %>% 
  tab_options(
    table.width = pct(100),
    table.font.size = "20px",
    table.align = "center"
    )

```

### Ranks

A *rank* is a position of a data point when sorted in ascending/descending order.

Ties are handled using the *average* rank (two 3rd places, both get ranked $3.5$)

- Data: `[5,2,8,2,10]`
- Sorted: `[2,2,5,8,10]`
- Ranks: `[2,2,3,4,5]`
- Ranks with ties: `[1.5,1.5,3,4,5]` 

### Why ranks for non-parametric tests {.incremental}

1. **Invariance to Monotonic Transformations**: Ranks preserve order, so log/root transformations do not change them

2. **Focus on relative Differences**: Tests like Wilcoxon (rank-sum) or Kruskal-Wallis compare *ranks distributions* across groups

3. **Exact p-values per Permutation**: Ranks allow exact tests by enumerating all possible rank orderings (for small samples)

### Common Pitfalls and Misconceptions

1. **Loss of Information**: Ranks discard magnitude, a trade-off for robustness

2. **Ties Reduce Power**: Many ties weaken tests

3. **Not "Assumption-Free"**: Nonpaeametric $\neq$ no assumptions (independence, distribution shapes)

#### Exercise on ranks

```{r}
#| label: tbl-ranks-exercise
#| tbl-cap: Reaction times for two groups of participants

reaction_data <- tibble(
  group = c("A", "A", "B", "B", "A", "B", "A", "B"),
  time  = c(12.3, 10.1, 14.0, 13.2, 10.1, 15.5, 11.8, 13.2)
)

reaction_data |> gt()

```

#### Sort the data

::: {.fragment .fade-in}

```{r}
#| label: tbl-ranks-exercise02
#| tbl-cap: sorted reaction times for two groups of participants

sorted_data <- reaction_data %>%
  arrange(time) %>%
  mutate(sorted_position = row_number())

sorted_data |> gt()
         
```

:::

#### Assign Ranks

::: {.fragment .fade-in}

```{r}
#| label: tbl-ranks-exercise03
#| tbl-cap: sorted reaction times for two groups of participants

ranked_data <- reaction_data %>%
  mutate(rank = rank(time, ties.method = "average")) |> 
  arrange(group,time)

ranked_data |> gt()


```

:::

#### Sum of ranks

::: {.fragment .fade-in}

```{r}
#| label: tbl-ranks-exercise04
#| tbl-cap: sorted reaction times for two groups of participants

sum_ranks <- ranked_data %>%
  group_by(group) %>%
  summarise(
    sum_rank = sum(rank),
    count = n(),
    mean_rank = mean(rank)
  )

sum_ranks |> gt()

```

:::

## Paired and Independent Tests

```{r}
#| label: fig-pairedvsindep
#| out-width: 75%
#| fig-cap: The difference between paired and independent Tests.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","018_Paired_Independent.png"))

```

::: {.content-visible when-profile="script"}

1. Paired Statistical Test:
- Paired tests are used when there is a natural pairing or connection between two sets of data points. 
This pairing is often due to repeated measurements on the same subjects or entities.
- They are designed to assess the difference between two related samples, such as before and after measurements on the same group of individuals.
- The key idea is to reduce variability by considering the differences within each pair, which can increase the test sensitivity.

2. Independent Statistical Test:
- Independent tests, also known as unpaired or two-sample tests, are used when there is no inherent pairing between the two sets of data.
- These tests are typically applied to compare two separate and unrelated groups or samples.
- They assume that the data in each group is independent of the other, meaning that the value in one group doesn't affect the value in the other group.

An example for a paired test is, if two groups of data are to be compared in two different points in time (see @fig-pairedvsindep).

:::

{{< pagebreak >}}

## Distribution Tests

::: {.content-visible when-profile="script"}

The importance of testing for normality (or other distributions) lies in the fact that various statistical techniques, such as parametric tests (e.g., t-tests, ANOVA), are based on the assumption of for example normality. 
When data deviates significantly from a normal distribution, using these parametric methods can lead to incorrect conclusions and biased results. 
Therefore, it is essential to determine how a dataset is approximately distributed before applying such techniques.

Several tests for normality are available, with the most common ones being the Kolmogorov-Smirnov test, the Shapiro-Wilk test, and the Anderson-Darling test. 
These tests provide a quantitative measure of how well the data conforms to a normal distribution. 

In practice, it is important to interpret the results of these tests cautiously. 
Sometimes, a minor departure from normality may not affect the validity of parametric tests, especially when the sample size is large. 
In such cases, using non-parametric methods may be an alternative. 
However, in cases where normality assumptions are crucial, transformations of the data or choosing appropriate non-parametric tests may be necessary to ensure the reliability of statistical analyses.

Tests for normality do not free you from the burden of thinking for yourself.

:::

### Quantile-Quantile plots {#sec-qq-plot}

::: {.content-visible when-profile="script"}

[Quantile-Quantile](#qq) plots are a graphical tool used in statistics to assess whether a dataset follows a particular theoretical distribution, typically the normal distribution. 
They provide a visual comparison between the observed quantiles[^7] of the data and the quantiles expected from the chosen theoretical distribution. 

A neutral explanation of how [QQ](#qq) plots work:

[^7]: A quantile is a statistical concept used to divide a dataset into equal-sized subsets or intervals. 

:::

#### Sample data

::: {.content-visible when-profile="script"}

In @tbl-qq-smpl-dat $n=10$ datapoints are shown as a sample dataset.

:::

```{r}
#| label: tbl-qq-smpl-dat
#| tbl-cap: 10 datapoints for the creation of the QQ-plot


qq_smpl <- data.frame(sample_data = c(3,1,5,2,4)) 

qq_smpl %>% 
  gt() 


```


#### Data Sorting 

::: {.content-visible when-profile="script"}

To create a [QQ](#qq) plot, the data must be sorted in ascending order.

:::

```{r}
#| label: tbl-qq-smpl-sort
#| tbl-cap: The sorted data points.

qq_smpl_srt <- qq_smpl %>% 
  arrange(sample_data)  
  
qq_smpl_srt %>% gt()


```

#### Empirical Quantiles

::: {.content-visible when-profile="script"}

Theoretical quantiles are calculated based on the chosen distribution (e.g., the normal distribution). 
These quantiles represent the expected values if the data perfectly follows that distribution.

:::

```{r}
#| label: tbl-qq-smpl-emprcl
#| tbl-cap: The empirical quantiles


qq_smple_srt_emp <-  qq_smpl_srt %>% 
  mutate(
    sample_data_emp_prob = (1:n()-0.5)/n()
    # x_thrtcl = pnorm(x_norm)
  )

qq_smple_srt_emp %>% 
  gt()

```

#### Theoretical Quantiles

```{r}
#| label: tbl-qq-smpl-thrtcl
#| tbl-cap: The calculated theoretical quantiles


qq_smple_srt_theo <-  qq_smple_srt_emp |> 
  mutate(
   quant_theo = qnorm(sample_data_emp_prob,  mean  = mean(sample_data),sd = sd(sample_data))
  )

qq_smple_srt_theo %>% 
  gt()

```

#### Plotting Points 


```{r}
#| label: fig-qq-pts
#| out-width: 95%
#| fig-cap: The QQ points as calculated before.
#| fig-pos: "H"

qq_smple_srt_theo %>% 
  ggplot()+
  geom_point(
    aes(
      x = quant_theo,
      y = sample_data
      )
  )+
  labs(
    title = "The drawn QQ points",
    x = "theoretical quantiles",
    y = "sample data"
  )

```

::: {.content-visible when-profile="script"}

For each data point, a point is plotted in the [QQ](#qq) plot. 
The x-coordinate of the point corresponds to the theoretical quantile, and the y-coordinate corresponds to the observed quantile from the data, see @fig-qq-pts.

:::


#### Perfect Normal Distribution

```{r}
#| label: fig-qq-abline
#| out-width: 95%
#| fig-cap: A perfect normal distribution would be indicated if all points would fall on this straight line.
#| fig-pos: "H"

qq_smple_srt_theo %>% 
  ggplot(
    aes(
      x = quant_theo,
      y = sample_data))+
  geom_point()+
  geom_abline(
    aes(
    intercept = mean(quant_theo) - sd(quant_theo)*mean(quant_theo),
    slope = sd(quant_theo),
    linetype = "fitted qq line")
  )+
  geom_abline(
    aes(
      intercept = 0,
      slope = 1,
      linetype = "perfect normal"
    )
  )+
  labs(
    title = "A perfect normal distribution line.",
    linetype = "",
    x = "theoretical quantiles",
    y = "sample data"
  )+
  theme(
    legend.position = "bottom"
  )

  

```

::: {.content-visible when-profile="script"}

In the case of a perfect normal distribution, all the points would fall along a straight line at a 45-degree angle. 
If the data deviates from normality, the points may deviate from this line in specific ways, see @fig-qq-abline.

Deviations from the straight line suggest departures from the assumed distribution. 
For example, if points curve upward, it indicates that the data has heavier tails than a normal distribution. 
If points curve downward, it suggests lighter tails. 
S-shaped curves or other patterns can reveal additional information about the data's distribution.
In @fig-qq-line the QQ-points are shown together with the respective QQ-line and a line of perfectly normal distributed points.
Some deviations can be seen, but it is hard to judge, if the data is normally distributed or not.

:::

#### QQ plot with probabilities

```{r}
#| label: fig-qq-abline-probgrid
#| out-width: 95%
#| fig-cap: A QQ Plot with the assigned probabilities on the axis
#| fig-pos: "H"

probs <- c( 0.05, seq(0.1, 0.9, by = 0.1), 0.95)
qprobs<-qnorm(probs)

qq_smple_srt_theo_scaled <- qq_smple_srt_theo |> 
  mutate(
    sample_data_scaled = scale(sample_data),
    quant_theo_scaled = qnorm(sample_data_emp_prob,  mean  = mean(sample_data_scaled),sd = sd(sample_data_scaled))
  )

qq_smple_srt_theo_scaled |> 
  ggplot(
    aes(
      x = quant_theo_scaled,
      y = sample_data_scaled))+
  geom_point()+
  geom_abline(
    aes(
    intercept = mean(quant_theo_scaled) - sd(quant_theo_scaled)*mean(quant_theo_scaled),
    slope = sd(quant_theo_scaled),
    # linetype = "fitted qq line"
    )
  )+
  labs(
    title = "A QQ plot with probabilities",
    x = "theoretical probabilities",
    y = "sample probabilities"
  )+
  theme(
    panel.grid.major = element_line(color = "gray")
  )+
  scale_y_continuous(limits=range(qprobs), breaks=qprobs, labels = 100*probs)+
  scale_x_continuous(limits=range(qprobs), breaks=qprobs, labels = 100*probs)

```



#### Confidence Interval

```{r}
#| label: fig-qq-bands
#| out-width: 75%
#| fig-cap: The QQ plot with confidence bands.
#| fig-pos: "H"

qq_smpl %>% 
  ggplot(aes(sample = sample_data))+
  stat_qq_band()+
  stat_qq_point()+
  stat_qq_line()+
  scale_x_continuous(
    # breaks = seq(-3,3,0.5),
    # limits = c(-1.5,1.5)
    expand = c(0,0.05,0,0.05)
  )+
  scale_y_continuous(
    # breaks = seq(-3,3,0.5),
    # limits = c(-1.5,1.5)
    expand = c(0,0,0,0)
  )+
  labs(
    title = "The confidence bands for the QQ plot.",
    x = "theoretical",
    y = "sample"
  )

```

::: {.content-visible when-profile="script"}

Because it is hard to judge from @fig-qq-line if the points are normally distributed, it makes sense to get limits for normally distributed points.
This is shown in @fig-qq-bands.
The gray area depicts the ($95\%$) confidence bands for a normal distribution.
All the points fall into the area, as well as the line.
This shows, that the points are likely to be normally distributed.

:::

#### Expanding to non-normal disitributions

```{r}
#| label: fig-qq-wbll
#| layout-ncol: 2
#| out-width: 75%
#| fig-cap: The QQ-plot can easily be extended to non-normal distributions.
#| fig-subcap: 
#| - the QQ-plot for the weibull distribution using the drive shaft failure time data
#| - a detrended QQ-plot

load(here("data","drive_shaft_failures.Rdata"))

# n <- 100  # Number of drive shafts
# shape <- 2  # Shape parameter
# scale <- 500  # Scale parameter (in hours)

di <- "weibull"
dp <- list(shape = 2, scale = 500)
de <- TRUE

plt1 <- drive_shaft_failure %>% 
  ggplot(aes(sample = Time_to_Failure))+
  stat_qq_band(
    distribution = di,
    dparams = dp
    )+
  stat_qq_point(
    distribution = di,
    dparams = dp
  )+
  stat_qq_line(
    distribution = di,
    dparams = dp
    )+
  labs(
    title = "QQ-plot for the weibull distribution",
    caption = "drive shaft failure data",
    x = "theoretical",
    y = "sample"
  )+
  theme_few()
  

plt2 <- drive_shaft_failure %>% 
  ggplot(aes(sample = Time_to_Failure))+
  stat_qq_band(
    distribution = di,
    dparams = dp,
    detrend = de
    )+
  stat_qq_point(
    distribution = di,
    dparams = dp,
    detrend = de
  )+
  stat_qq_line(
    distribution = di,
    dparams = dp,
    detrend = de
    )+
  labs(
    title = "QQ-plot for the weibull distribution (drive shaft failure data)",
    caption = "drive shaft failure data",
    x = "theoretical",
    y = "sample"
  )+
  theme_few()

plt1
plt2

```

::: {.content-visible when-profile="script"}

The [QQ](#qq)-plot can easily be extended to non-normal disitributions as well.
This is shown in @fig-qq-wbll.
In @fig-qq-wbll-1 a classic [QQ](#qq)-plot for @fig-ds-wbll is shown.
The same rules as before still apply, they are *only* extended to the weibull distribution.
In @fig-qq-wbll-2 a *detrended* [QQ](#qq)-plot is shown in order to account for visual bias.
It is of course known, that the data follows a *weibull* disitribution with a shape parameter $\beta=2$ and a scale parameter $\lambda = 500$, but such distributional parameters can also be estimated [@fitdistrplus].

:::


#### The drive shaft exercise

```{r}
#| label: fig-qq-ds
#| out-width: 75%
#| fig-cap: The QQ plots for each drive shaft group shown in subplots.
#| fig-pos: "H"

load(here("data","drive_shaft_data.Rdata"))

drive_shaft %>% 
  ggplot(aes(sample = diameter))+
  stat_qq_band()+
  stat_qq_line(
    aes(linetype = "qq-line from data")
  )+
  stat_qq_point()+
  geom_abline(
    aes(
      linetype = "perfect normal distribution",
      slope = 1,
      intercept = 0),
    key_glyph = draw_key_path,
    lwd = 1
    )+
  labs(
    title = "QQ-Plots of the drive shaft data with confidence bands.",
    linetype = ""
  )+
  
  facet_wrap(~group,scale = "free")+
  theme_few(base_size = 10)+
  theme(
    panel.grid.major = element_line(color = "gray"),
    panel.grid.minor = element_line(color = "gray"),
    legend.position = "bottom"
  )
  

```
::: {.content-visible when-profile="script"}

The [QQ](#qq) plot method is extended to the drive shaft exercise in @fig-qq-ds.
In each subplot the plot for the respective group is shown together with the QQ-points, the QQ-line and the respective confidence bands.
The scaling for each plot is different to enhance visibility of every subplot.
A line for the perfect normal distribution is also shown in solid linestyle.
From group $1 \ldots 4$ all points fall into the QQ confidence bands.
Group05 differs however.
The points from visible categories, which is a strong indicator, that the measurement system may be to inaccurate.

:::


### Quantitative Methods {#sec-ks-test}

#### The data

```{r}
#| label: fig-dist-test-data
#| out-width: 95%
#| fig-cap: The example data to show all quantitative distribution tests.


# Beispiel-Daten (leicht rechtsschief)
set.seed(123)
daten <- tibble(
  werte = c(rnorm(15, mean = 10, sd = 2), rnorm(5, mean = 15, sd = 3))
)

# Visualisierung
daten |> 
  ggplot(
    aes(
      x = werte
      )
    ) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 7, 
    fill = "steelblue",
    color = "white"
    ) +
  geom_rug()+
  stat_function(
    fun = dnorm, 
    args = list(
      mean = mean(daten$werte), 
      sd = sd(daten$werte)),
    # color = "red", 
    linewidth = 1) +
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    title = "Verteilung der Beispiel-Daten vs. Normalverteilung"
    )

```


#### Kolmogorov Smirnov 

Measures the distance from the {{< acr ECDF >}} to the {{< acr CDF >}}.

{{< acr H0 >}}: The data comes from the assumed distribution.

```{r}
#| label: fig-ks-smpl
#| out-width: 75%
#| fig-cap: A visualisation of the KS test using the 10 datapoints from before
#| fig-pos: "H"

# Perform the KS test
ks_result <- ks.test(daten$werte, "pnorm")

# Create a data frame for the ECDF
ecdf_data <- data.frame(x = daten$werte, ecdf = ecdf(daten$werte)(daten$werte))

# Create a data frame for the theoretical normal CDF
normal_data <- data.frame(x = seq(min(daten$werte), max(daten$werte), length = 1000),
                          cdf = pnorm(seq(min(daten$werte), max(daten$werte), length = 1000),
                                      mean = mean(daten$werte), sd = sd(daten$werte)))

# Create a ggplot object to visualize the ECDF
ggplot(ecdf_data, aes(x = x, y = ecdf)) +
  geom_step() +
  geom_line(data = normal_data, aes(x = x, y = cdf), color = "red") +
  labs(title = "Kolmogorov-Smirnov Test for Normality",
       subtitle = paste("D =", round(ks_result$statistic, 2), "p =", signif(ks_result$p.value, digits = 3))) 

```

```{r}
#| label: tbl-kstest-result
#| tbl-cap: The ks test result


ks_result |> broom::tidy() |> gt()

```
#### Summary ks test

Pro

- Generally applicable (not only normal distribution).
- Easy to interpret (maximum deviation between empirical and theoretical distribution).

Con

- Conservative (low power with small samples).
- Sensitive to outliers.

::: {.content-visible when-profile="script"}

The [Kolmogorov-Smirnov](#KS) test for normality, often referred to as the [KS](#KS) test, is a statistical test used to assess whether a dataset follows a normal distribution. 
It evaluates how closely the [cumulative distribution function](#cdf) of the dataset matches the expected [CDF](#cdf) of a normal distribution. 

1. **Null Hypothesis (H0):** The null hypothesis in the KS test states that the sample data follows a normal distribution.

2. **Alternative Hypothesis (Ha):** The alternative hypothesis suggests that the sample data significantly deviates from a normal distribution.

3. **Test Statistic (D):** The KS test calculates a test statistic, denoted as *D* which measures the maximum vertical difference between the empirical [CDF](#cdf) of the data and the theoretical [CDF](#cdf) of a normal distribution. 
It quantifies how far the observed data diverges from the expected normal distribution.
A visualization of the [KS](#KS)-test is shown in @fig-ks-smpl.
The red line denotes a perfect normal distribution, whereas the step function shows the empirical [CDF](#cdf) of the data itself.

4. **Critical Value:** To assess the significance of D, a critical value is determined based on the sample size and the chosen significance level ($\alpha$). 
If D exceeds the critical value, it indicates that the dataset deviates significantly from a normal distribution.

5. **Decision:** If D is greater than the critical value, the null hypothesis is rejected, and it is concluded that the data is not normally distributed. 
If D is less than or equal to the critical value, there is not enough evidence to reject the null hypothesis, suggesting that the data may follow a normal distribution.

It is important to note that the KS test is sensitive to departures from normality in both tails of the distribution. 
There are other normality tests, like the *Shapiro-Wilk test* and *Anderson-Darling test*, which may be more suitable in certain situations. 
Researchers typically choose the most appropriate test based on the characteristics of their data and the assumptions they want to test.

:::

#### Anderson Darling Test

Similar to ks test, but weighs the tails heavier.

{{< acr H0 >}}: The data comes from the assumed distribution.

```{r}
#| label: tbl-adtest-result
#| tbl-cap: The AD test result

ad.test(daten$werte) |> broom::tidy() |> gt()

```

#### summary AD test

Pro:

- Generally applicable (not only normal distribution).
- asy to interpret (maximum deviation between empirical and theoretical distribution).

Cons:

- Conservative (low power with small samples).
- Sensitive to outliers.

#### Shapiro Wilk

- Special Case for the normal Distribution.
- used for $n<5000$
- Test Statistik ranges between $0,\ldots,1$ with $1$ being normally distributed.

{{< acr H0 >}}: The data comes from the assumed distribution.


#### Mathemtatical Intuition

- evaluates whether a sample $X_1,X_2,\ldots,X_n$ comes from a normal distribution by comparing the ordered sample values to the expected order.

1. Order Statistics: $X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}$

2. Expected Normal Order: taken from $\mathrm{N}(0,1)$ via $m_i=\Phi^{-1}\frac{i}{n+1}$ with $\Phi$ being the {{< acr CDF>}}

3. Weights ($a_i$): assign weights to $a_i$ derived from the covariance matrix to account for the variability in different parts of the distribution (tails vs. center)

#### Connection to linear Regression

The Shapiro Wilk test can be framed as a regression problem:

- Dependent variable: The ordered sample $X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}$
- Independent variable: The expected normal order statistics $m_1,m_2,\ldots,m_n$
- Weights: The inverse covariance $\mathrm{V}^{-1}$ accounts for the non-independence of order statistics

The test statistic $W$ is then the coefficient of determination $r^2$ of this weighted regression.

$$W = r^2 = \frac{\text{Explained Variance}}{\text{Total Variance}}$$

#### The test

```{r}
#| label: tbl-sw-result
#| tbl-cap: The shapiro Wilk test result


shapiro.test(daten$werte) |> broom::tidy() |> gt()
```

### Comparison of test results

```{r}
#| label: tbl-quant-norm-test
#| tbl-cap: Comparison of all quantitative test results on the same data



sw_out <- shapiro.test(daten$werte) |> broom::tidy() |> add_column(alternative=NA)
ad_out <- ad.test(daten$werte) |> broom::tidy() |> add_column(alternative=NA)
ks_out <- ks_result |> broom::tidy() 


q_norm_test <- bind_rows(
  sw_out,ad_out,ks_out
)
  q_norm_test |> gt()

```

### sample size influence

```{r}

set.seed(123)
daten <- tibble(
  werte = c(rnorm(150, mean = 0, sd = 1),
            rnorm(150, mean = 15, sd = 3)
            )
)

sw_out <- shapiro.test(daten$werte) |> broom::tidy() |> add_column(alternative=NA)
ad_out <- ad.test(daten$werte) |> broom::tidy() |> add_column(alternative=NA)
ks_out <- ks.test(daten$werte, "pnorm") |> broom::tidy() 


q_norm_test <- bind_rows(
  sw_out,ad_out,ks_out
)
  q_norm_test |> gt()

```


## Test 1 Variable

```{r}
#| label: fig-tests-OneVar
#| out-width: 75%
#| fig-cap: Statistical tests for one variable.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","019_StatisticalTests_OneVar.png"))

```


### One Proportion Test 

Scenario:
A semiconductor manufacturer claims that their production line yields no more than $2\%$ defective chips (i.e., the defect rate $p_0=0.02$ ). 
To verify this, a quality engineer randomly samples 500 chips and finds 14 defectives. 
Is there sufficient evidence at $\alpha=0.05$  to conclude that the defect rate exceeds the claimed $2\%$?

::: {.content-visible when-profile="script"}

The one proportion test is used on categorical data with a binary outcome, such as success or failure. 
Its prerequisite is having a known or hypothesized population proportion that the sample proportion shall be compared to. 
This test helps determine if the sample proportion significantly differs from the population proportion, making it valuable for studies involving proportions and percentages.

:::

#### Define the Hypothesis {.incremental}

{{< acr H0 >}}:
: Defect rate is < 2 \%

{{< acr Ha >}}:
: Defect rate is >2 \%

This is a **right-tailed** test

#### The z-test for proportions

The underlying distribution for the z-test is the *binomial* distribution.

The test itself relies on the *normal* approximation to the *binomial*

$$X\sim\text{Binomial}(n,p)$$

- $n$: sample size

- $p$: true population proportion

- Sample proportion: The sample proportion $\hat{p} = \frac{X}{n}$ is the estimator for $p$

#### The Bridge to Normality

Acc. to the {{< acr CLT >}} for large $n$

\begin{align}
\hat{p} \sim N(p,\sqrt{\frac{p(1-p)}{n}})
\end{align}

1. Mean: $\hat{p}$ is the unbiased estimator of $p$
2. {{< acr SE >}}: The $sd$ of $\hat{p}$ is $\sqrt{\frac{p(1-p)}{n}}$
3. Normality Condition: $np\geq10$ and $n(1-p)\geq 10$

#### The Z-Statistic

Under $H_0$ assume $p = p_0$. The Z-Statistic standardizes $\hat{p}$ to

\begin{align}
Z = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}
\end{align}

#### Check Assumptions

. . .

- Independence: Chips are randomly sampled

. . .

- Sample Size: $n \cdot p_0\geq 10$ and $n(1-p_0)\geq10$

. . .
  
  - $500 \times 0.02 = 10 \geq 10$
  
. . .  

  - $500 \times 0.98 = 490 \geq 10$
  
#### Calculate Test Statistic

- $\hat{p} = \frac{\text{defective}}{n} = \frac{14}{500} = 0.028$
- $p_0 = 0.02, n = 500$

. . .

$$z = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}} = \frac{0.028-0.02}{\sqrt{\frac{0.02\times 0.98}{500}}}\approx 1.28$$

#### Compute the p-value

$$P(Z>1.28) = 1-P(Z\leq 1.28)\approx 1-0.8997 = 0.1003$$

```{r}
#| label: fig-z-test
#| out-width: 95%
#| fig-cap: The z-test statistic
#| fig-pos: "H"


prob_z <- expand_grid(
  x = seq(0,5,0.01)
  ) %>%
  mutate(
    p = pnorm(x, lower.tail = FALSE)
    )
    

  prob_z |> 
  ggplot(
    aes(
      x = x,
      y = p
    )
  )+
  # geom_line()+
  geom_area(
    fill = "gray70"
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  geom_vline(
    xintercept =  qnorm(1-0.05,lower.tail = TRUE),
    linetype = "dashed"
  )+
  geom_vline(xintercept = 0.1003)+
    geom_text(
      x = 0.1003,
      y = 0.3,
      label = "Actual Z statistic",
      angle = 90,
      vjust = 1,
      hjust = 0.5
      )+
  geom_text(
    x = qnorm(1-0.05,lower.tail = TRUE),
    y = 0.3,
    label = "Critical z value",
    angle = 90,
    vjust = 1,
    hjust = 0.5
  )+
  labs(
    title = "CDF of z-test",
    y = "probability",
    x = "Z-Statistic"
  ) 

```

### Chi^2^ goodness of fit test {.incremental}

Scenario: A semiconductor manufacturer produces wafers using three machines (A, B, C). 
The QA team suspects that defect rates differ across machines. They collect data over a week:

```{r}
#| label: tbl-chi-gof-data
#| tbl-cap: The raw data for the gof $\chi^2$ test.


defect_data <- tibble(
  Machine = rep(c("A", "B", "C"), each = 2),
  Status   = rep(c("Defective", "Non-Defective"), 3),
  Count    = c(45, 505, 30, 520, 60, 490)
)

defect_data |> gt()

```

#### Define the Hypotheses

::: {.fragment}

{{< acr H0 >}}:
: Defect rates are independent of machine (no difference).

{{< acr Ha >}}:
: Defect rates depend on the machine.

:::

#### Calculate Test Statistic

$$\chi^2 = \sum \frac{(O_{ij}-E_{ij})^2}{E_{ij}}$$

- $O_{ij}$ observed frquency in cell $i,j$ (Row $i$, column $j$)
- $E_{ij}$ expected frquency in cell $i,j$ under the $H_0$

#### Contingency tables

A contingency table (also called a cross-tabulation or two-way table) is a tabular representation of the relationship between two or more categorical variables. 
It displays the frequency distribution of observations across different categories, allowing us to examine associations, independence, or dependencies between variables.

#### Observed Contingency table

```{r}
#| label: tbl-chi-cont-ob
#| tbl-cap: The tabulated results for the observed frequencies

obs_conttable <- xtabs(Count~Machine+Status,data = defect_data) |> as.data.frame.matrix() 

obs_conttable |> 
  rownames_to_column("Machine") |> 
  rowwise() |> 
  mutate(
    `Row Total` = sum(Defective+`Non-Defective`)
  ) |> 
  ungroup() |> 
  gt() |> 
  grand_summary_rows(
    columns = c(Defective,`Non-Defective`, `Row Total`),
    fns = "sum"
  )
  

```

#### Calculate Expected Frequencies

For each cell, calculate $E_{ij}$:

1. Machine A, Defective:

$$E_{A,Defective} = \frac{\text{Row Total}_A \times \text{Column Total}_{Defective}}{\text{Grand Total}} = \frac{550\times135}{1650} = 45$$

2. Machine A, Non-Defective:

$$E_{A,Non-Defective} = \frac{\text{Row Total}_A \times \text{Column Total}_{Non Defective}}{\text{Grand Total}} = \frac{550\times1515}{1650} = 505$$

#### Expected table

```{r}
#| label: tbl-chi-cont-ex
#| tbl-cap: The tabulated results for the expected frequencies

exp_conttable <- data.frame(
  Machine  = c("A","B","C"),
  Defective = c(45,45,45),
  Non_Defective = c(505,505,505)
)

exp_conttable |> gt()

```


#### The test result

```{r}
#| label: tbl-chi-gof-res
#| tbl-cap: The test results for the gof $\chi^2$ test.


chi_test <- chisq.test(
  xtabs(Count~Machine+Status, data = defect_data)
)

chi_test |> 
  broom::tidy() |> 
  select(statistic, p.value, method) |> 
  gt() |> 
  fmt_number(decimals = 3)

```


::: {.content-visible when-profile="script"}

The $\chi^2$ goodness of Fit Test ([gof](#gof)) is applied on categorical data with expected frequencies. 
It is suitable for analyzing nominal or ordinal data. 
This test assesses whether there is a significant difference between the observed and expected frequencies in your dataset, making it useful for determining if the data fits an expected distribution.

:::



### One-sample t-test

::: {.content-visible when-profile="script"}

The one-sample t-test is designed for continuous data when you have a known or hypothesized population mean that you want to compare your sample mean to. 
It relies on the assumption of normal distribution, making it applicable when assessing whether a sample's mean differs significantly from a specified population mean.

The test can be applied in various settings.
One is, to test if measured data comes from a population with a certain mean (for exampe a test against a specification).
To show the application, the *drive shaft data* is employed.
In @tbl-t-one-data the *per group* summarised data of the dirve shaft data is shown.

:::

```{r}
#| label: tbl-t-one-data
#| tbl-cap: The raw data for the one sample t-test.

load(here("data","drive_shaft_data.Rdata"))

drive_shaft %>% 
  group_by(group) %>% 
  summarise(
    mean_diameter = mean(diameter),
    sd_diameter = sd(diameter)
  ) %>% 
  gt()  %>% 
  fmt_number(decimals = 3)%>% 
  tab_options(
    table.font.size = 15
  )
  

```

::: {.content-visible when-profile="script"}

One important prerequisite for the One sample t-test normally distributed data.
For this, graphical and numerical methods have been introduced in previous chapters.
First, a classic [QQ](#qq)-plot is created for every group (see @fig-t-one-qq).
From a first glance, the data appears to be normally distributed.

:::

#### Normality Check

```{r}
#| label: fig-t-one-qq
#| fig-cap: The qq-plot for the drive shaft data
#| out-width: 95%

load(here("data","drive_shaft_data.Rdata"))


drive_shaft %>% 
  ggplot(aes(sample = diameter))+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()+
  geom_abline()+
  facet_wrap(~group)+
  labs(
    title = "The graphcial check for normality"
  )+
  theme_few()

```

::: {.content-visible when-profile="script"}

A more quantitative approach to tests for normality is shown in @tbl-t-one-ks-test.
Here, each group is tested with the [KS](#KS)-test for normality.
[H0](#H0) is accepted (the data is normal distributed) because the computed p-value is larger than the significance level ($\alpha  = 0.05$). 

:::

#### Quantitative Normality Check

```{r}
#| label: tbl-t-one-ks-test
#| tbl-cap: The results for the one KS normality test for each group.

source("ks_helpers.R")

load(here("data","drive_shaft_data.Rdata"))

drive_shaft %>% 
  group_nest(group) %>% 
  mutate(
    ks_res = map(data, ks_drive_shaft),
    ks_tidy = map(ks_res, tidy)
  ) %>% 
  select(-data,-ks_res) %>%
  unnest(ks_tidy) %>% 
  mutate(
    method = str_wrap(method, width  = 1),
    method = str_replace_all(method, pattern = "\n", replacement = "<br />")
  ) %>% 
  gt()  %>% 
  fmt_markdown(
    columns = everything()
  ) %>% 
  fmt_number(decimals = 3)%>% 
  tab_options(
    table.font.size = 15
  ) %>% 
  as_raw_html()

```

::: {.content-visible when-profile="script"}

There is sufficient evidence to assume normal distributed data within each group.
The next step is, to test if the data comes from a certain population mean ([$\mu_0$](#truemean-gloss)). 
In this case, the population mean is the specification of the drive shaft at a diameter $=12mm$.

:::

#### Test result

```{r}
#| label: tbl-t-one-res
#| tbl-cap: The results for the one sample t-test (against mean = 12mm).

drive_shaft %>% 
  group_nest(group) %>% 
  mutate(
    t_one_sample = map(data, function(x) t.test(x["diameter"],mu = 12)),
    t_tidy = map(t_one_sample, function(x) tidy(x))
  ) %>% 
  unnest(t_tidy) %>% 
  select(-data,-t_one_sample) %>% 
  mutate(
    method = str_wrap(method, width  = 1),
    method = str_replace_all(method, pattern = "\n", replacement = "<br />")
    ) %>% 
  gt() %>% 
  fmt_markdown(
    columns = everything()
  ) %>% 
  fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 15
  )  %>% 
  as_raw_html()


```


### One sample Wilcoxon test {#sec-wilcox-rank-sum}

Scenario:

A manufacturing plant produces bolts with a target diameter of 10.0 mm. The quality control team measures 7 randomly selected bolts and obtains the following diameters (in mm): 8, 9.8, 9.9, 10.0, 10.1, 10.2, 15.0

The team wants to determine if the median diameter differs from the target at a significance level of $\alpha=0.05$ .


#### The data

```{r}
#| label: fig-wilcox-one-hist
#| fig-cap: The wear and tear rating data histograms.
#| out-width: 95%
 

bolt_data <- data.frame(
  bolt_id = seq(1,7),
  bolt_diameter =  c(8, 9.8, 9.9, 10.0, 10.1, 10.2, 15.0)
)

bolt_data |> 
  ggplot(
    aes(
     sample = bolt_diameter)
  )+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()

```

#### The hypotheses

::: {.fragment .fade-in}

$$H_0: \text{Median diameter} = 10.00mm$$

$$H_a: \text{Median diameter} \neq 10.00mm$$

:::

#### Signed Ranks

$D_i = X_1 - 10.00$ and Rank $|D_i|$

::: {.fragment .fade-in}

```{r}
#| label: tbl-signed-rank
#| tbl-cap: Signed rank calculation


bolt_ranks <- bolt_data |> 
  mutate(
    Di = bolt_diameter-10,
    abs_Di = abs(Di),
    Di_rank = dense_rank(abs_Di),
    signed_rank = sign(Di)*Di_rank
  )

sum_signed_ranks <- bolt_ranks |> 
  mutate(
    pos_sign = case_when(
      signed_rank>0~"positive",
      signed_rank<0~"negative")
  )

sum_rank_pos <- sum_signed_ranks |> 
  filter(pos_sign == "positive")  |> 
  pull(Di_rank) |> 
  sum()
    
sum_rank_neg <- sum_signed_ranks |> 
  filter(pos_sign == "negative")  |> 
  pull(Di_rank) |> 
  sum()    

bolt_ranks |> 
  gt() |> 
  cols_label(
    bolt_id = "Bolt ID",
    bolt_diameter= ("Bolt\ndiameter"),
    Di = md("$D_i$"),
    abs_Di = md("$|D_i|$"),
    Di_rank = md("ranked $D_i$"),
    signed_rank = md("signed rank of $D_i$")
  )


```

:::

#### Sum of signed ranks

- Sum of *positive ranks*: `r sum_rank_pos`
- Sum of *negative ranks*: `r sum_rank_neg`

#### Decision at α Level

- Reject $H_0$ if $W \leq 2$
- Here $W = 9$ (use the smaller sum), so we fail to reject $H_0$

#### W?

The Wilcoxon signed-rank test statistic $W$ is the smaller of the two rank sums. 
Under $H_0$, the distribution of $W$ is symmetric, and we can enumerate all possible outcomes.

1. Total number of outcomes: Each bolt can have a positive or negative outcome (ignoring $0$) $2^7=128$.

2. Constructing the Distribution: For each combination, compute rank sum for $\pm$ differences. The critical value is the largest $W$ such that the cumulative probability $P(W\leq w)\leq\alpha/2$ (two tailed test)

3. For $n = 7$ the cumulative probabilities for $W$ are precomputed in tables. 

#### W value at significance level 5\% and n = 7

{{< acr H0 >}}: The median of differences is $0$ (or a certain value like $10.00mm$)

Possible ranks of differences: 

For $n = 7$ the ranks of the absolute differences are *always* $1,2,\ldots,7$ (independent of data, because for $H_0$ all sign combinations are of interest)

$$W_{max} = \sum_{k=1}^7 k = 28$$

All possible sign combinations: $2^7 = 128$. Every combination is one **possible realization of $W$**

#### Calculation of PMF

- $W = 0$: all differences are negative
- $W = 1$: exactly the difference with rank 1 is positive ($7$ combinations, because every difference of the $7$ could be the one with rank 1)
- $W = 2$: 
  - the difference with rank $2$ is positive (and all others are negative) **or**
  - the differences with rank $1$ and rank $2$ are positive ($7$ combinations)
  
#### table for $n = 7$

```{r}
#| include: false

wilcox_n7 <- dwilcox_signed_rank(7)

pmf_n7 <- wilcox_n7$pmf

pmf_n7 <- pmf_n7 |> 
  mutate(
    cum_prob = cumsum(density)
  )

pmf_n7_gt_fn <- function(x){ 
  gt(x) |> 
  cols_label(
    density = "frequency",
    cum_prob = "cumulative probability"
  )
}

gt_pmf_n7_01 <- pmf_n7 |> 
  slice(1:15) |> 
  pmf_n7_gt_fn()

gt_pmf_n7_02 <- pmf_n7 |> 
  slice(16:29) |> 
  pmf_n7_gt_fn()

```

:::: {.columns}

::: {.column width='40%'}

```{r}
#| label: tbl-wilcox-cdf-n7-01
#| tbl-cap: The exact cumulative probabilities... 

gt_pmf_n7_01

```


:::

::: {.column width='60%'}

```{r}
#| label: tbl-wilcox-cdf-n7-02
#| tbl-cap: ... for the wilcoxon sigend rank test

gt_pmf_n7_02

```


:::

::::

#### CDF for $n = 7$

```{r}
#| label: fig-wilcox-cdf-n7
#| fig-cap: The exact cumulative probabilities for the wilcoxon sigend rank test
#| out-width: 95%


pmf_n7 |> 
  ggplot(
    aes(
      x = W,
      y = cum_prob
    )
  )+
  geom_col()+
  scale_y_continuous(
    expand = c(0,0,0,0.05)
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  labs(
    title = "Cumulative probabilities",
    subtitle = "for the wilcoxon signed rank test at n=7",
    y = "cumulative probabilites"
  )

```

#### Estimation of W for small n

```{r}
#| include: false

# Dataframe erstellen
wilcoxon_critical <- tibble(
  `sample size (n)` = 5:20,
  `critical value (W, two-sided, α=0.05)` = c(0, 0, 2, 3, 5, 8, 10, 13, 17, 21, 25, 30, 35, 41, 47, 53),
  `Interpretation (reject H₀ if...` = c(
    "W ≤ 0",
    "W ≤ 0",
    "W ≤ 2",
    "W ≤ 3",
    "W ≤ 5",
    "W ≤ 8",
    "W ≤ 10",
    "W ≤ 13",
    "W ≤ 17",
    "W ≤ 21",
    "W ≤ 25",
    "W ≤ 30",
    "W ≤ 35",
    "W ≤ 41",
    "W ≤ 47",
    "W ≤ 53"
  )
)

wilcox_critical_gt_fn <- function(x) {
  gt(x) %>%
    tab_header(
    title = "critical value for the Wilcoxon-signed-rank-test (two-sided, α=0.05)",
    subtitle = "for small sample sizes (n ≤ 20)"
  ) %>%
  fmt_number(columns = 2, decimals = 0) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_options(
    heading.title.font.size = 18,
    heading.subtitle.font.size = 12
  )
}

wilcox_tbl_01 <- wilcoxon_critical |> 
  slice(1:8) |> 
  wilcox_critical_gt_fn()

wilcox_tbl_02 <- wilcoxon_critical |> 
  slice(9:16) |> 
  wilcox_critical_gt_fn()


# wilcox_dbl_tbl <- gt_double_table(wilcoxon_critical, wilcox_critical_gt_fn, nrows = nrow(wilcoxon_critical) / 2)

```


:::: {.columns}

::: {.column width='50%'}

```{r}
#| label: tbl-wilcox-small-n-1
#| tbl-cap: The critical W values for small n

wilcox_tbl_01

```


:::

::: {.column width='50%'}

```{r}
#| label: tbl-wilcox-small-n-2
#| tbl-cap: The critical W values for small n

wilcox_tbl_02

```

:::

::::



#### The Wilcox PMF

```{r}
#| label: fig-wilcox-exact-pmf
#| fig-cap: The exact PMF for the wilcoxon sigend rank test
#| out-width: 95%


exact_wilcox_data <- data.frame(
  n = seq(5,13)
) |> 
  rowwise() |> 
  mutate(
    pmf = list(dwilcox_signed_rank(n)$pmf),
    lower_crit = dwilcox_signed_rank(n)$lower,
    upper_crit = dwilcox_signed_rank(n)$upper
  ) |> 
  unnest("pmf")
  
sum_exact_wilcox_data <-  exact_wilcox_data |> 
  group_by(n) |> 
  summarise(
    lower_crit = mean(lower_crit),
    upper_crit = mean(upper_crit),
  )

# Plot mit kritischen Werten

exact_wilcox_data |> 
  ggplot(
    aes(
      x = W,
      y = density,
      group = 1
    )
  )+
  geom_area(
    fill = "gray60"
  )+
  scale_y_continuous(
    expand = c(0,0,0,0.01)
  )+
  scale_x_continuous(
    expand = c(0,0,0,0.0)
  )+
  geom_vline(
    aes(
      xintercept = lower_crit,
      linetype = "lower crit val"
    ),
    key_glyph = "path"
  )+
  geom_vline(
    aes(
      xintercept = upper_crit,
      linetype = "upper crit val"
    ),
    key_glyph = "path"
  )+
  geom_label(
    data = sum_exact_wilcox_data,
    aes(
      x = lower_crit,
      y = 0.1,
      label = lower_crit
    )
  )+
  geom_label(
    data = sum_exact_wilcox_data,
    aes(
      x = upper_crit,
      y = 0.1,
      label = upper_crit
    )
  )+
  labs(
    title = "exact PMF fot the wilcox sigend rank",
    linetype = "critical values"
  )+
  facet_wrap(
    ~n,
    scales = "free_x"
    )+
  theme(
    legend.position = "bottom"
  )

```


#### The Wilcox cumulative probabilities

```{r}
#| label: fig-wilcox-cum-prob
#| fig-cap: The exact PMF for the wilcoxon sigend rank test
#| out-width: 95%


wilcox_cumprob <- exact_wilcox_data |> 
  group_by(n) |> 
  mutate(
    cum_prob = cumsum(density)
  )
  
sum_exact_wilcox_data <-  exact_wilcox_data |> 
  group_by(n) |> 
  summarise(
    lower_crit = mean(lower_crit),
    upper_crit = mean(upper_crit),
  )

# Plot mit kritischen Werten

wilcox_cumprob |> 
  ggplot(
    aes(
      x = W,
      y = cum_prob,
      group = n
    )
  )+
  geom_area(
    fill = "gray60"
  )+
  scale_y_continuous(
    expand = c(0,0,0,0.01)
  )+
  scale_x_continuous(
    expand = c(0,0,0,0.0)
  )+
  geom_vline(
    aes(
      xintercept = lower_crit,
      linetype = "lower crit val"
    ),
    key_glyph = "path"
  )+
  geom_vline(
    aes(
      xintercept = upper_crit,
      linetype = "upper crit val"
    ),
    key_glyph = "path"
  )+
  geom_label(
    data = sum_exact_wilcox_data,
    aes(
      x = lower_crit,
      y = 0.1,
      label = lower_crit
    )
  )+
  geom_label(
    data = sum_exact_wilcox_data,
    aes(
      x = upper_crit,
      y = 0.1,
      label = upper_crit
    )
  )+
  labs(
    title = "exact cumulative",
    linetype = "critical values"
  )+
  facet_wrap(
    ~n,
    scales = "free_x"
    )+
  theme(
    legend.position = "bottom"
  )

```

## Test 2 Variable (Qualitative or Quantitative)

```{r}
#| label: fig-tests-TwoVar-same
#| out-width: 75%
#| fig-cap: Statistical tests for two variables.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","020_StatisticalTests_TwoVar_same.png"))

```

### Chi^2^ test of independence

::: {.content-visible when-profile="script"}

This test is appropriate when you have two categorical variables, and you want to determine if there is an association between them. 
It is useful for assessing whether the two variables are dependent or independent of each other.

In the context of the drive shaft production the example assumes a dataset with categorical variables like "Defects" (Yes/No) and "Operator" (Operator A/B). 

:::

#### Sezenario

A manufacturing plant produces three types of components (A, B, C) using two different assembly lines (Line 1 and Line 2). 
The quality control team suspects that the defect rates may differ between the two lines. 
They collect data over a week to test whether component type and defect occurrence are independent of the assembly line used.

#### Hypotheses

{{< acr H0 >}}: Component type and defect occurrence are independent of the assembly line (no association).

{{< acr Ha >}}: Component type and defect occurrence are dependent on the assembly line (association exists).

$$H_0: P(\text{Defect}|Line) = P(\text{Defect})$$

#### The data

```{r}
#| label: tbl-chi2-indep-data
#| tbl-cap: The contingency table of the obeservations.


# Create a tibble with the observed counts
defect_data <- tribble(
  ~Component, ~Line, ~DefectStatus, ~Count,
  "A",        "Line1", "Defect",     12,
  "A",        "Line1", "NoDefect",  188,
  "A",        "Line2", "Defect",     20,
  "A",        "Line2", "NoDefect",  180,
  "B",        "Line1", "Defect",      8,
  "B",        "Line1", "NoDefect",  242,
  "B",        "Line2", "Defect",     15,
  "B",        "Line2", "NoDefect",  235,
  "C",        "Line1", "Defect",     25,
  "C",        "Line1", "NoDefect",  175,
  "C",        "Line2", "Defect",     10,
  "C",        "Line2", "NoDefect",  240
)

# Create a contingency table (Component x Line x DefectStatus)
contingency_table <- defect_data %>%
  pivot_wider(
    names_from = c(Line, DefectStatus),
    values_from = Count,
    values_fill = 0
  )

contingency_table |> gt()

```

Tipp: Revisit $\chi^2$ goodness of fit.

#### The test - Component A

```{r}
#| label: tbl-chi2-indep-data-comp-A
#| tbl-cap: The test result for Line A

# Filter data for Component A
component_A <- defect_data %>%
  filter(Component == "A")

# Create a 2x2 contingency table (Line vs. DefectStatus)
table_A <- component_A %>%
  count(Line, DefectStatus) %>%
  pivot_wider(names_from = DefectStatus, values_from = n) %>%
  column_to_rownames("Line")

# Chi-square test
chisq.test(table_A) |> broom::tidy() |> gt()

```

#### The test - Line vs. Defect Status (Ignoring Component)

```{r}
#| label: tbl-chi2-indep-data-comp-overall
#| tbl-cap: The test result for the overall data

# Aggregate data across all components
overall_table <- defect_data %>%
  count(Line, DefectStatus) %>%
  pivot_wider(names_from = DefectStatus, values_from = n) %>%
  column_to_rownames("Line")

# Chi-square test
chisq.test(overall_table) |> broom::tidy() |> gt()


```

#### Visualization

```{r}
#| label: fig-chi2-indep-vis
#| out-width: 95%
#| fig-cap: Viualitzation of the data


# Create a long-format table for plotting
plot_data <- defect_data %>%
  group_by(Line, DefectStatus) %>%
  summarise(Count = sum(Count), .groups = "drop") %>%
  mutate(DefectStatus = factor(DefectStatus, levels = c("NoDefect", "Defect")))

# Plot
ggplot(plot_data, aes(x = Line, y = DefectStatus, fill = Count)) +
  geom_tile() +
  geom_label(
    aes(
      label = Count
    )
  )+
  # scale_fill_gradient(low = "white", high = "red") +
  labs(title = "Defect Count by Assembly Line",
       x = "Assembly Line",
       y = "Defect Status",
       fill = "Count") +
  theme_minimal()


```


### Correlation

```{r}
#| label: fig-corr
#| out-width: 75%
#| fig-cap: Correlation between two variables and the quantification thereof.

knitr::include_graphics(here::here("chapter002","024_Correlation.png"))

```

::: {.content-visible when-profile="script"}

Correlation refers to a statistical measure that describes the relationship between two variables.
It indicates the extent to which changes in one variable are associated with changes in another.

Correlation is measured on a scale from -1 to 1:

- A correlation of 1 implies a perfect positive relationship, where an increase in one variable corresponds to a proportional increase in the other.

- A correlation of -1 implies a perfect negative relationship, where an increase in one variable corresponds to a proportional decrease in the other.

- A correlation close to 0 suggests a weak or no relationship between the variables.

Correlation doesn't imply causation; it only indicates that two variables change together but doesn't determine if one causes the change in the other.

:::


### Pearson Corrrelation

The pearson correlation $R$ or $r$ coefficient is a normalized version of the covariance.

#### Covariance

Covariance is a measure of joint variability.

Covariance is a generalized formulation of the variance.

\begin{align}
\mathrm{Cov}(X,Y) = \frac{1}{n}\sum{(X_i - \bar{X})(Y_i - \bar{Y})}
\end{align}

#### Example

```{r}
#| label: tbl-example-data-cov
#| tbl-cap: Example data on the calculation of the covariance for two variables.


cov_data <- data.frame(
  temp_c = c(20,22,24,26,28),
  rod_length_mm = c(100.2,100.5,100.9,101.3,101.6)
)

cov_data |>
  gt() |>
  cols_label(
    temp_c = "Temperature in C",
    rod_length_mm = "Rod Length in mm"
  ) |>
  cols_align(
    align = "center",
    columns = everything()
  ) |>
  tab_options(
    table.width = pct(100),
    table.font.size = pct(80)
  )

```

#### Computing means

\begin{align}
\bar{X} = \frac{`r cov_data$temp_c[1]`+`r cov_data$temp_c[2]`+`r cov_data$temp_c[3]`+`r cov_data$temp_c[4]`+`r cov_data$temp_c[5]`}{`r nrow(cov_data)`} = `r mean(cov_data$temp_c)`
\end{align}

\begin{align}
\bar{X} = \frac{`r cov_data$rod_length_mm[1]`+`r cov_data$rod_length_mm[2]`+`r cov_data$rod_length_mm[3]`+`r cov_data$rod_length_mm[4]`+`r cov_data$rod_length_mm[5]`}{`r nrow(cov_data)`} = `r mean(cov_data$rod_length_mm)`
\end{align}

#### Computing covariance

```{r}
#| label: tbl-example-data-cov-calc
#| tbl-cap: Hand calculation of the XY-covariance

cov_data_calc <- cov_data |>
  mutate(
    Xi_X = temp_c-mean(temp_c),
    Yi_Y = rod_length_mm-mean(rod_length_mm),
    products = Xi_X*Yi_Y
  )

cov_data_calc |>
  gt() |>
  cols_label(
    temp_c = "Temperature in C",
    rod_length_mm = "Rod Length in mm",
    Xi_X = "$X_i - \\bar{X}$",
    Yi_Y = "$Y_i - \\bar{Y}$",
    products = "$(X_i - \\bar{X}) \\cdot (Y_i - \\bar{Y})$"
  ) |>
  cols_align(
    align = "center",
    columns = everything()
  ) |>
  tab_options(
    table.width = pct(100),
    table.font.size = pct(70)
  )
```

#### Sum the products

\begin{align}
\sum(X_i - \bar{X})(Y_i - \bar{Y}) = `r cov_data_calc$products[1]` + `r cov_data_calc$products[2]` + `r cov_data_calc$products[3]` + `r cov_data_calc$products[4]` + `r cov_data_calc$products[5]` = `r sum(cov_data_calc$products)`
\end{align}

Compute Covariance

\begin{align}
\mathrm{Cov}(X,Y) = \frac{`r sum(cov_data_calc$products)`}{`r nrow(cov_data_calc)`} = `r  sum(cov_data_calc$products)/nrow(cov_data_calc)`
\end{align}

#### Interpretation:

* **Covariance** $= 1.44\;°C\cdot mm$
* It's **positive**, meaning:
  * When the machine runs hotter, the rods are longer (thermal expansion)
* It's **not standardized**
  * The number $1.44$ is not "large" or "small" until compared with variance or a **correlation** is computed

#### detailed Interpretation

* $(X_i-\bar{X}) \rightarrow$ How different is this temperature from average?
* $(Y_i-\bar{Y}) \rightarrow$ How different is this rod length from average?
* Multiply them:
  * Positive $x$ Positive $\rightarrow$ rods are longer at higher temp $\rightarrow$ $+$ contribution
  * Negative $x$ Negative $\rightarrow$ rods are shorter at lower temp $\rightarrow$ $+$ contribution
  * Different signs $\rightarrow$ one high, one low $\rightarrow$ $-$ contribution

<center>
The average of all contributions is the **Covariance**
</center>

#### Correlation equation

\begin{align}
R = \frac{\mathrm{Cov}(X,Y)}{\sigma_x \sigma_y}
\end{align}

* Covariance is sensitive to **scale** ($mm$ vs. $cm$)
* Pearson correlation **removes** units, allowing for meaningful comparisons across datasets


#### drive shaft data

```{r}
#| label: fig-drive-shaft-pearson-qq
#| out-width: 75%
#| fig-cap: The QQ-plot of both variables. There is strong evidence that they are normally distributed.

load(here("data","drive_shaft_rpm_dia.Rdata"))

drive_shaft_rpm_dia %>%
  pivot_longer(cols = everything(),names_to = "variable",values_to = "value") %>%
  ggplot(aes(sample = value))+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()+
  facet_wrap(~variable,scales = "free")+
  labs(
    title = "QQ-plot of diameter and rpm",
  )

```
We have data on the the `diameter`  vs. `rpm` and want to know, if there is a correlation between the two variables.

#### The correlation

```{r}
#| label: fig-drive-shaft-corr-pear
#| out-width: 75%
#| fig-cap: Correlation between rpm of lathe machine and the diameter of the drive shaft.

load(here("data","drive_shaft_rpm_dia.Rdata"))

drive_shaft_rpm_dia %>%
  ggplot(aes(x = rpm, y = diameter))+
  geom_point()+
  stat_cor()+
  geom_smooth(
    method = "lm"
  )+
  labs(
    title = "Drive shaft diameter vs. rpm of lathe machine",
    x = "rpm",
    y = "diameter in mm"
  )

```

#### Testing and computing it

```{r}

load(here("data","drive_shaft_rpm_dia.Rdata"))

cor.test(drive_shaft_rpm_dia$rpm,drive_shaft_rpm_dia$diameter)

```

### Pearson correlation coefficient summary

- $2$ continuous variables, measure the strength and direction of their linear relationship: Pearson correlation [@PCC]

- normally distributed data

\begin{align}
R = \frac{\sum_{i = 1}^{n}(x_i - \bar{x}) \times (y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\times \sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}} \label{pearcorr}
\end{align}

### Spearman Correlation

::: {.content-visible when-profile="script"}

Spearman [@Spearman1904] correlation is a non-parametric alternative to Pearson correlation.
It is used when the data is not normally distributed or when the relationship between variables is monotonic but not necessarily linear.

\begin{align}
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)} \label{spearcorr}
\end{align}

In @fig-drive-shaft-corr-spear the example data for a drive shaft production is shown.
The `Production_Time` and the `Defects` seem to form a relationship, but the data does not appear to be normally distributed.
This can also be seen in the QQ-plots of both variables in @fig-drive-shaft-spear-qq.

:::

The **spearman** correlation coefficient ($\rho$) is based on the pearson correlation, but applied to **ranked** data

#### data with outliers

```{r}
#| label: tbl-spear-ex-raw
#| tbl-cap: The data is now subject to outliers.

spear_dat <- data.frame(
  obs = seq(1,8,1),
  temp_c = c(20,22,24,26,28,30,32,34),
  rod_length_mm = c(100.2,100.4,100.7, 101.0, 101.3, 110, 120.0, 130.0)
) |>
  add_column(
    comment = c("normal","normal","normal","normal","normal","steep increase","very steep","extreme")
  )

spear_dat |>
  gt() |>
  cols_label(
    obs = "Observation",
    temp_c = "Temperature in C",
    rod_length_mm = "Rod Length in mm",
    comment = "Comment"
  ) |>
  cols_align(
    align = "center",
    columns = everything()
  ) |>
  tab_options(
    table.width = pct(100),
    table.font.size = pct(70)
  ) |>
    tab_style(
    style = cell_fill(color = 'grey'),
    locations = cells_body(
      columns = c(rod_length_mm),
      rows = c(6,7,8)
    ))

```

#### The ranks of the data

```{r}
#| label: tbl-spear-ex-calc
#| tbl-cap: The steps in calculating the spearman correlation.

spear_dat_calc <- spear_dat |>
  mutate(
    rnk_temp = dense_rank(temp_c),
    rnk_leng = dense_rank(rod_length_mm),
    di = rnk_temp-rnk_leng,
    di2 = di^2
  )

spear_dat_calc |>
  gt() |>
  cols_label(
    obs = "Obersvation",
    temp_c = "Temperature in C",
    rod_length_mm = "Rod Length in mm",
    comment = "Comment",
    rnk_temp = "Rank $X$",
    rnk_leng = "Rank $Y$",
    di = "$d = R_X-R_Y$",
    di2 = "$d^2$"
  ) |>
  cols_align(
    align = "center",
    columns = everything()
  ) |>
  tab_options(
    table.width = pct(100),
    table.font.size = pct(70)
  ) |>
    tab_style(
    style = cell_fill(color = 'grey'),
    locations = cells_body(
      columns = c(rod_length_mm),
      rows = c(6,7,8)
    ))

```

#### The spearman rho

\begin{align}
\rho = 1- \frac{6\sum{d^2}}{n(n^2-1)} = 1 - \frac{0}{8(64-1)} = 1
\end{align}

#### The data in a plot

```{r}
#| label: fig-spear-dat
#| fig-width: 15
#| fig-height: 8
#| out-width: 95%
#| fig-cap: The data for which the spearman correlation is computed

spear_dat_calc |>
  ggplot(
    aes(
      x = temp_c,
      y = rod_length_mm
    )
  )+
  geom_point(
    shape  = 4,
    size = 5
  )+
  geom_line()+
  # geom_smooth(method = "lm",se = TRUE)+
  labs(
    title = "The data for the correlation computation",
    x = "Temperature in C",
    y = "Rod Length in mm"
  )+
  stat_cor(method = "spearman", label.x = 20, label.y = 130)



```

#### Background


The spearman $\rho$ calculates the pearson correlation, but between the *rank* difference of the variables.

::: {style="font-size: 80%;"}

| Part                  | Meaning                                                                       |
| --------------------- | ----------------------------------------------------------------------------- |
| $\sum d_i^2$          | Total squared difference in ranks (rank disagreement)                         |
| $6$                   | Derived from algebraic simplification of Pearson’s r using rank variance      |
| $n(n^2 - 1)$          | Comes from variance of ranks and number of comparisons                        |
| $1 - \text{fraction}$ | Ensures perfect agreement yields $\rho = 1$; increasing $d_i^2$ lowers $\rho$ |

:::

### Comparison on outlier data

```{r}

corr_comp <- 
  data.frame(
    cor.test(spear_dat_calc$temp_c,spear_dat_calc$rod_length_mm, method = "pearson") |> broom::tidy()) |> 
  add_row(
    cor.test(spear_dat_calc$temp_c,spear_dat_calc$rod_length_mm, method = "spearman")|> broom::tidy()) 
  
corr_comp |> gt()  

```


### Correlation - methodogical limits

::: {.content-visible when-profile="script"}

While correlation analysis and summary statistics are certainly useful, one must always consider the raw data.
The data taken from @datasauRus showcases this.
The summary statistics in @tbl-corr-limits are practically the same, one would not suspect different underlying data.
When the raw data is plotted though (@fig-corr-limits), it can be seen that the data appears to be highly non linear, forming different shapes as well as different categories etc.

Always check the raw data.

:::

```{r}
#| label: tbl-corr-limits
#| tbl-cap: The datasauRus data and the respective summary statistics.

datasaurus_dozen %>%
    group_by(dataset) %>%
    summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
    ) %>%
  gt() %>%
   fmt_number(decimals = 3) %>%
  tab_options(
    # table.font.size = 25
  ) %>%
  as_raw_html()

```


### Plot of the raw data

```{r}
#| label: fig-corr-limits
#| out-width: 75%
#| fig-cap: The raw data from the datasauRus packages shows, that summary statistics may be misleading.

ggplot(datasaurus_dozen, aes(x = x, y = y))+
    geom_point()+
    theme_void(base_size = 10)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```


## Test 2 Variables (2 Groups)

```{r}
#| label: fig-tests-TwoVar-TwoGrps
#| out-width: 75%
#| fig-cap: Statistical tests for two variable.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","021_StatisticalTests_TwoVar_TwoGrps.png"))

```

### Test for equal variance (homoscedasticity)

```{r}
#| label: fig-tst-var
#| fig-cap: The variances ($sd^2$) for the drive shaft data.
#| out-width: 95%

load(here("data","drive_shaft_data.Rdata"))

ds_sum <- drive_shaft %>% 
  group_by(group) %>% 
  summarise(
    mean_diameter = mean(diameter),
    sd_diameter = sd(diameter),
    var_diameter = var(diameter)
  )

ds_sum %>% 
  ggplot(aes(x = group, y = var_diameter))+
  geom_col()+
  scale_y_continuous(expand = c(0,0,0.05,0))+
  labs(
    title = "The variances of the drive shaft data",
    x = "Group",
    y = "Variance (sd²)"
  )+
  theme_few()

```

::: {.content-visible when-profile="script"}

Tests for equal variances, also known as tests for homoscedasticity, are used to determine if the variances of two or more groups or samples are equal. 
Equal variances are an assumption in various statistical tests, such as the t-test and analysis of variance ([ANOVA](#anova)). 
When the variances are not equal, it can affect the validity of these tests. Two common tests for equal variances are:

Certainly, here are bullet points outlining the null hypothesis, prerequisites, and decisions for each of the three tests:

:::


#### F-Test [@Hahs_Vaughn_2013] {#sec-f-test .smaller}

:::: {.columns}

::: {.column wwidth="50%"}
- **Null Hypothesis:** The variances of the different groups or samples are equal.
- **Prerequisites:**
  - Independence
  - Normality
  - Number of groups $= 2$

:::  

::: {.column wwidth="50%"}

- **Decisions:**
  - $p> \alpha \rightarrow$ fail to reject [H0](#H0)
  - $p< \alpha \rightarrow$ reject [H0](#H0)

:::

::::

```{r}
#| output: true
#| echo: false

ds_wide <- drive_shaft %>% 
  pivot_wider(names_from = group, values_from = diameter) 

Group01VsGroup03 <- var.test(ds_wide$group01,ds_wide$group03) %>% print()

```

#### Bartlett Test [@Bartlett1937] {#sec-bartlett .smaller}

:::: {.columns}

::: {.column wwidth="50%"}
- **Null Hypothesis:** The variances of the different groups or samples are equal.
- **Prerequisites:**
  - Independence
  - Normality
  - Number of groups $> 2$

:::  

::: {.column wwidth="50%"}

- **Decisions:**
  - $p> \alpha \rightarrow$ fail to reject [H0](#H0)
  - $p< \alpha \rightarrow$ reject [H0](#H0)

:::

::::

```{r}
#| output: true
#| echo: false

out_bartlett <- bartlett.test(diameter ~ group, data = drive_shaft) %>% print()

```

#### Levene Test [@0-8047-0596-8] {.smaller}

:::: {.columns}

::: {.column wwidth="50%"}
- **Null Hypothesis:** The variances of the different groups or samples are equal.
- **Prerequisites:**
  - Independence
  - Number of groups $> 2$

:::  

::: {.column wwidth="50%"}

- **Decisions:**
  - $p> \alpha \rightarrow$ fail to reject [H0](#H0)
  - $p< \alpha \rightarrow$ reject [H0](#H0)

:::

::::

```{r}
#| output: true
#| echo: false

ds_levene <- drive_shaft %>% 
  mutate(group = as_factor(group))

out_levene <- leveneTest(diameter ~ group, data = ds_levene) %>% print()

```

### t-test for independent samples

::: {.content-visible when-profile="script"}

The independent samples t-test is applied when you have continuous data from two independent groups. 
It evaluates whether there is a significant difference in means between these groups, assuming a normal distribution of the data.

:::

#### Prerequisites

- **Null Hypothesis:** The means of the two samples are equal.
- **Prerequisites:**
  - Independence
  - Normal Distribution
  - Number of groups $=2$
  - equal Variances of the groups
  
#### Hypotheses

{{< acr H0 >}}: $\mu_1 = \mu_2$ (two tailed)

{{< acr Ha >}}: $\mu_1 > \mu_2$ or $\mu_1 < \mu_2$ (one-tailed)

#### One sided test

```{r}
#| label: fig-t-test-oneside
#| fig-cap: The idea for a one-sided (t-)test
#| out-width: 95%
#| 
# Parameters
alpha <- 0.05
df_small <- 5    # Small sample size (heavy tails)
df_large <- 20   # Larger sample size (closer to normal)
x_range <- seq(-4, 4, length.out = 1000)

# Create data frames for t-distributions
df_t <- tibble(
  x = x_range,
  y_small = dt(x, df = df_small),
  y_large = dt(x, df_large)
)

# Critical values for one-sided and two-sided tests
crit_one_sided_small <- qt(1 - alpha, df = df_small)
crit_two_sided_small <- qt(1 - alpha/2, df = df_small)
crit_one_sided_large <- qt(1 - alpha, df = df_large)
crit_two_sided_large <- qt(1 - alpha/2, df = df_large)

# One-Sided Plot (with both small and large df)
p1 <- ggplot() +
  # Small df (red curve)
  geom_line(
    data = df_t, aes(x = x, y = y_small, color = "Small n (df=5)"),
    linewidth = 1, show.legend = TRUE
  ) +
  geom_area(
    data = df_t %>% filter(x >= crit_one_sided_small),
    aes(x = x, y = y_small),
    fill = "red", alpha = 0.3
  ) +
  geom_vline(
    xintercept = crit_one_sided_small,
    linetype = "dashed", color = "red", linewidth = 0.8
  ) +
  # Large df (blue curve)
  geom_line(
    data = df_t, aes(x = x, y = y_large, color = "Large n (df=20)"),
    linewidth = 1, show.legend = TRUE
  ) +
  geom_area(
    data = df_t %>% filter(x >= crit_one_sided_large),
    aes(x = x, y = y_large),
    fill = "blue", alpha = 0.3
  ) +
  geom_vline(
    xintercept = crit_one_sided_large,
    linetype = "dashed", color = "blue", linewidth = 0.8
  ) +
  labs(
    title = "One-Sided t-Test (Right-Tailed)",
    subtitle = "Influence of Sample Size (df) on Critical Values and Distribution Shape",
    x = "t-Statistic", y = "Density",
    color = "Sample Size"
  ) +
  scale_color_manual(values = c("red", "blue")) +
  theme(legend.position = "bottom")

p1

```

#### Two sided test

```{r}
#| label: fig-t-test-twoside
#| fig-cap: The idea for a two-sided (t-)test
#| out-width: 95%
#| 
# Two-Sided Plot (with both small and large df)
p2 <- ggplot() +
  # Small df (red curve)
  geom_line(
    data = df_t, aes(x = x, y = y_small, color = "Small n (df=5)"),
    linewidth = 1, show.legend = TRUE
  ) +
  geom_area(
    data = df_t %>% filter(x <= -crit_two_sided_small | x >= crit_two_sided_small),
    aes(x = x, y = y_small),
    fill = "red", alpha = 0.3
  ) +
  geom_vline(
    xintercept = c(-crit_two_sided_small, crit_two_sided_small),
    linetype = "dashed", color = "red", linewidth = 0.8
  ) +
  # Large df (blue curve)
  geom_line(
    data = df_t, aes(x = x, y = y_large, color = "Large n (df=20)"),
    linewidth = 1, show.legend = TRUE
  ) +
  geom_area(
    data = df_t %>% filter(x <= -crit_two_sided_large | x >= crit_two_sided_large),
    aes(x = x, y = y_large),
    fill = "blue", alpha = 0.3
  ) +
  geom_vline(
    xintercept = c(-crit_two_sided_large, crit_two_sided_large),
    linetype = "dashed", color = "blue", linewidth = 0.8
  ) +
  labs(
    title = "Two-Sided t-Test",
    subtitle = "Influence of Sample Size (df) on Critical Values and Distribution Shape",
    x = "t-Statistic", y = "Density",
    color = "Sample Size"
  ) +
  scale_color_manual(values = c("red", "blue")) +
  theme(legend.position = "bottom")

p2

```

#### The test statistic

The t-statistic quantifies how far the observed difference in means ($\bar{x_1}-\bar{x_2}$) is from {{< acr H0 >}} in units of standard error.

\begin{align}
t = \frac{\text{Observed Difference}}{\text{Standard Error of the Difference}} = \frac{\bar{x_1}-\bar{x_2}}{SE_{\bar{x_1}-\bar{x_2}}}
\end{align}

Where the standard error depends on:

- Sample Sizes ($n_1,n_2$)

- Sample variances ($sd_1^2,sd_2^2$)

- Whether the sample variances are equal (pooled variance) or not (Welch's t-test)

#### Standard error for equal variances

\begin{align}
SE_{pooled} &= \sqrt{sd_p^2(\frac{1}{n_1}+\frac{1}{n_2})} \\
sd_p^2 &= \frac{(n_1-1)sd_1^2+(n_2-1)sd_2^2}{n_1+n_2-2}
\end{align}

#### Variances

First, the variances are compared in order to check if they are equal using the F-Test (as described in @sec-f-test).

```{r}
#| warning: false

load (here("data","drive_shaft_data.Rdata"))

group01 <- drive_shaft %>% filter(group == "group01")
group03 <- drive_shaft %>% filter(group == "group03")

var.test(group01 %>% pull("diameter"),group03%>% pull("diameter"))
```

With $p>\alpha = 0.05$ the $H_0$ is accepted, the variances are equal.

#### Normality

The next step is to check the data for normality using the KS-test (as described in @sec-ks-test).

```{r}
#| warning: false

ks.test(group01 %>% pull("diameter"), 
        y= "pnorm",
        mean(group01 %>% pull("diameter")),
        sd(group01 %>% pull("diameter"))
        )
ks.test(group03 %>% pull("diameter"), 
        y= "pnorm",
        mean(group03 %>% pull("diameter")),
        sd(group03 %>% pull("diameter"))
        )


```

With $p>\alpha = 0.05$ the $H_0$ is accepted, the data seems to be normally distributed.

#### Visualization

```{r}
#| label: fig-t-tst-two-indep
#| fig-cap: The data within the two groups for comparing the sample means using the t-test for independent samples.
#| out-width: 95%
#| fig-pos: "H"

load (here("data","drive_shaft_data.Rdata"))

tmp <- drive_shaft %>% 
  filter(group %in% c("group01","group03"))

tmp %>% 
  ggplot(aes(x = diameter,linetype = group))+
  stat_theodensity(
    aes(y = after_stat(count)/20),
    distri = "norm",
    geom = "area",
    show.legend = FALSE
  )+
  geom_histogram(
    color = "white",
    alpha = 0.5,
    show.legend = FALSE
  )+
  geom_boxplot(
    aes(y = 30,
        fill = group),
    linetype = "solid",
    varwidth = FALSE,
    width = 2.5,
    position = "identity"
    )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
    breaks = seq(0,50,5)
  )+
  scale_x_continuous(
    breaks = seq(0,20,0.1)
  )+
  scale_fill_grey(
    start = 0.5,
    end = 0.8
  )+
  labs(
    title = "The groups for comparing the sample means.",
    y = "count",
    x = "diameter",
    fill = ""
  )+
  theme_few()+
  theme(
    legend.position = "bottom"
  )

```

#### Testing

The formal test is then carried out.
With $p<\alpha=0.05$ $H_0$ is rejected, the data comes from populations with different means.

```{r}
#| warning: false

load (here("data","drive_shaft_data.Rdata"))

group01 <- drive_shaft %>% filter(group == "group01")
group02 <- drive_shaft %>% filter(group == "group02")

t.test(group01 %>% pull(diameter),group03%>% pull(diameter),var.equal = TRUE)

```



### Welch t-test for independent samples

::: {.content-visible when-profile="script"}

Similar to the independent samples t-test, the Welch t-test is used for continuous data with two independent groups [@Welch1947]. 
However, it is employed when there are unequal variances between the groups, relaxing the assumption of equal variances in the standard t-test.

:::

#### Prerequisites

- **Null Hypothesis:** The means of the two samples are equal.
- **Prerequisites:**
  - Independence
  - Normal Distribution
  - Number of groups $=2$


#### Variance Check

First, the variances are compared in order to check if they are equal using the F-Test (as described in @sec-f-test).

```{r}
#| warning: false

load (here("data","drive_shaft_data.Rdata"))

group01 <- drive_shaft %>% filter(group == "group01")
group02 <- drive_shaft %>% filter(group == "group02")

var.test(group01 %>% pull("diameter"),group02%>% pull("diameter"))
```

With $p<\alpha = 0.05$ $H_0$ is rejected and $H_a$ is accepted. 
The variances are different.

#### Normality Check

Using the KS-test (see @sec-ks-test) the data is checked for normality.

```{r}
#| warning: false

ks.test(group01 %>% pull("diameter"), 
        y= "pnorm",
        mean(group01 %>% pull("diameter")),
        sd(group01 %>% pull("diameter"))
        )
ks.test(group02 %>% pull("diameter"), 
        y= "pnorm",
        mean(group02 %>% pull("diameter")),
        sd(group02 %>% pull("diameter"))
        )


```

With $p>\alpha = 0.05$ $H_0$ is accepted, the data seems to be normally distributed.

#### The data

```{r}
#| label: fig-welch-tst-two-indep
#| fig-cap: The data within the two groups for comparing the sample means using the Welch-test for independent samples.
#| out-width: 75%,
#| fig-pos: "H"

load (here("data","drive_shaft_data.Rdata"))

tmp <- drive_shaft %>% 
  filter(group %in% c("group01","group02")) %>% 
  mutate(
    y_pos = case_when(
      group == "group01" ~ 25,
      group == "group02" ~ 30,
    )
  )

tmp %>% 
  ggplot(aes(x = diameter,linetype = group))+
  stat_theodensity(
    aes(y = after_stat(count)/20,
        alpha = group),
    distri = "norm",
    geom = "area",
    show.legend = FALSE
  )+
  geom_histogram(
    color = "white",
    alpha = 0.5,
    show.legend = FALSE
  )+
  geom_boxplot(
    aes(y =y_pos,
        fill = group),
    linetype = "solid",
    varwidth = FALSE,
    width = 2.5,
    position = "identity"
    )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
    breaks = seq(0,50,5)
  )+
  scale_x_continuous(
    breaks = seq(0,20,0.1)
  )+
  scale_fill_grey(
    start = 0.5,
    end = 0.8
  )+
  labs(
    title = "The groups for comparing the sample means.",
    y = "count",
    x = "diameter",
    fill = ""
  )+
  theme_few()+
  theme(
    legend.position = "bottom"
  )

```

#### Calculation for unequal variances

The {{< acr SE >}} of the variances for unequal variances is calculated:

\begin{align}
SE_{Welch} = \sqrt{\frac{sd_1^2}{n_1}+\frac{sd_2^2}{n_2}}
\end{align}

#### The test

Then, the formal test is carried out.

```{r}
#| warning: false

load (here("data","drive_shaft_data.Rdata"))

group01 <- drive_shaft %>% filter(group == "group01")
group02 <- drive_shaft %>% filter(group == "group02")

t.test(group01 %>% pull(diameter),group02%>% pull(diameter),var.equal = FALSE)

```

With $p<\alpha = 0.05$ we reject $H_0$, the data seems to be coming from different population means, even though the variances are overlapping (and different).

### Mann-Whitney U test

::: {.content-visible when-profile="script"}

For non-normally distributed data or small sample sizes, the Mann-Whitney U test serves as a non-parametric alternative to the independent samples t-test [@MannWhitney1947]. 
It assesses whether there is a significant difference in medians between two independent groups.

:::

#### Prerequisites

Based on the Wilcoxon rank sum test (see @sec-wilcox-rank-sum).

- **Null Hypothesis:** The medians of the two samples are equal.
- **Prerequisites:**
  - Independence
  - no specific distribution (non-parametric)
  - Number of groups $=2$

#### Hypotheses

{{< acr H0 >}}: The two groups have equal medians

{{< acr Ha >}}: The two groups have unequal medians

#### The data

```{r}
#| label: fig-mannu-tst-data
#| fig-cap: The data within the two groups for comparing the sample medians using the Mann-Whitney-U Test.
#| out-width: 95%
#| fig-pos: "H"

load(here("data","drive_shaft_mann_u.Rdata"))

drive_shaft_mann_u %>% 
  ggplot(aes(x = diameter))+
  geom_density_ridges(
    aes(y = group),
    scale = 0.95,
    quantile_lines = TRUE
    )+
  scale_y_discrete(
    expand = c(0,0,0,0)
  )+
  scale_x_continuous(
    breaks = seq(0,20,0.1)
  )+
  labs(
    title = "Ridgeline plots of two groups of drive shaft diameters",
    y = ""
  )
  
```

#### Check for Normality

::: {.content-visible when-profile="script"}

This time a graphical method to check for normality is employed (QQ-plot, see @sec-qq-plot).
From the @fig-mannu-tst-qq it is pretty clear, that the data is not normally distributed.
Furthermore, the variances seem to be unequal as well.

:::

```{r}
#| label: fig-mannu-tst-qq
#| fig-cap: The data within the two groups for comparing the sample medians using the Mann-Whitney-U Test.
#| out-width: 75%
#| fig-pos: "H"

load(here("data","drive_shaft_mann_u.Rdata"))

drive_shaft_mann_u %>% 
  ggplot(aes(sample = diameter))+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()+
  scale_x_continuous(
    breaks = seq(0,20,0.2)
  )+
  facet_wrap(
    ~group,
    # scales = "free"
    )+
  labs(
    title = "The QQ-plots for the data to check for normality."
  )
  
```

#### The test

Then, the formal test is carried out.
With $p<\alpha = 0.05$ $H_0$ is rejected, the true location shift is not equal to $0$.

```{r}

load(here("data","drive_shaft_mann_u.Rdata"))

wilcox.test(diameter~group,data = drive_shaft_mann_u)

cohens_d(diameter~group,data = drive_shaft_mann_u)

```



### t-test for paired samples

::: {.content-visible when-profile="script"}

The paired samples t-test is suitable when you have continuous data from two related groups or repeated measures. 
It helps determine if there is a significant difference in means between the related groups, assuming normally distributed data.

:::

#### Prerequisites

- **Null Hypothesis:** True mean difference is not equal to 0.
- **Prerequisites:**
  - Paired Data
  - Normal Distribution
  - equal variances
  - Number of groups $=2$
  
#### Special characteristics

The paired t-test focuses on the differences between paired observations.

- ($X_{1i},X_{2i}$) may be the $i$th pair of observations ($i = 1, \ldots, n$)
- Differences: $D_i = X_{1i}-X_{2i}$

It is then evaluated whether the mean differences ($\mu_D$) is zero.

#### Hypotheses

{{< acr H0 >}}: $\mu_D = 0$

{{< acr Ha >}}: $\mu_D \neq 0$

with the test statistic being

\begin{align}
t = \frac{\bar{D}}{sd_D/\sqrt{n}}
\end{align}

#### Variances

Using the F-Test, the variances are compared.

```{r}
load(here("data","drive_shaft_treatment.Rdata"))

var.test(diameter~timepoint, data = drive_shaft_treatment)

```

With $p>\alpha = 0.05$ $H_0$ is accepted, the variances are equal.

#### Normality

Using a QQ-plot the data is checked for normality.

```{r}

load(here("data","drive_shaft_treatment.Rdata"))

drive_shaft_treatment %>% 
  ggplot(aes(sample = diameter))+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()+
  facet_wrap(~timepoint)+
  labs(
    title = "QQ-plots of the treament data at the different timepoints"
  )

```

#### Visualization

```{r}
#| label: fig-t-tst-paired
#| fig-cap: A boxplot of the data, showing the connections between the datapoints.
#| out-width: 75%
#| fig-pos: "H"

load(here("data","drive_shaft_treatment.Rdata"))

drive_shaft_treatment %>% 
  ggplot(aes(x = timepoint, y = diameter))+
  geom_boxplot(width = 0.1,fill = "gray")+
  geom_point()+
  geom_segment(data = drive_shaft_treatment %>% pivot_wider(names_from = timepoint, values_from = diameter),
               aes(x = "t0",
                   xend = "t1",
                   y = t0, 
                   yend = t1)
               )+
  ggrepel::geom_label_repel(aes(label = smpl_idx))+
  labs(
    title = "Paired t-test data with connections between samples.",
    x = "t0 = initial, t1 = after treatment",
    y = "diameter in mm"
  )
  

```

#### The test

The formal test is then carried out.

```{r}

drive_shaft_treatment |> 
  rstatix::t_test(
    formula = diameter ~ timepoint,
    paired = TRUE
  )

```

With $p<\alpha = 0.05$ $H_0$ is rejected, the treatment changed the properties of the product.


### Wilcoxon signed rank test

::: {.content-visible when-profile="script"}

For non-normally distributed data or situations involving paired samples, the Wilcoxon signed rank test is a non-parametric alternative to the paired samples t-test. 
It evaluates whether there is a significant difference in medians between the related groups.

:::

#### Prerequisites

- **Null Hypothesis:** True mean difference is not equal to 0.
- **Prerequisites:**
  - Paired Data
  - Number of groups $=2$

#### Check for Normality

```{r}

load(here("data","drive_shaft_wilcox_signed_rank.Rdata"))

drive_shaft_wilcox_signed_rank %>% 
  ggplot(aes(sample = diameter))+
  stat_qq_band(conf = 0.5)+
  stat_qq_line()+
  stat_qq_point()+
  facet_wrap(~timepoint)+
  theme_few()


```

#### Special characteristic

- ($X_{1i},X_{2i}$) may be the $i$th pair of observations ($i = 1, \ldots, n$)
- Differences: $D_i = X_{1i}-X_{2i}$

It is then evaluated whether the median differences is zero

#### The data

```{r}

load(here("data","drive_shaft_wilcox_signed_rank.Rdata"))

drive_shaft_wilcox_signed_rank %>% 
  ggplot(aes(x = timepoint,y = diameter))+
  geom_boxplot()+
  geom_point()+
  geom_segment(data = drive_shaft_wilcox_signed_rank %>% pivot_wider(names_from = timepoint, values_from = diameter),
               aes(x = "t0",
                   xend = "t1",
                   y = t0, 
                   yend = t1)
               )+
  ggrepel::geom_label_repel(aes(label = smpl_idx))+
  theme_few()


```

#### The test

```{r}

drive_shaft_wilcox_signed_rank |> 
  rstatix::wilcox_test(
    formula = diameter ~ timepoint,
    paired = TRUE
  )

```

### Exercise on paired samples

A manufacturing plant has 10 machines producing steel rods. 
After recalibration, the diameters of rods from each machine are measured again. 
The goal is to determine if the recalibration significantly changed the mean diameter.

#### The data

```{r}
#| label: tbl-exercise-paired-data
#| tbl-cap: The data for the paired test example

set.seed(42)
before <- c(
  rnorm(5, mean = 7, sd = 1),    # Small cluster
  rnorm(3, mean = 110, sd = 5),   # Outliers
  rlnorm(2, meanlog = 2, sdlog = 0.1) # Extra skew
) |> round(digits = 2)

# Generate 'after' by subtracting a normal shift
shift <- rnorm(10, mean = 1.5, sd = 0.4)  |> round(digits = 2)# Normal differences
after <- before - shift


diameter_data <- data.frame(
  before,
  after
)

diameter_data|> 
  gt()

```

#### Your plan

How do we go on?

#### Solution: Differences

```{r}
#| label: tbl-exercise-paired-data-sol01
#| tbl-cap: The data for the paired test example

diameter_data <- diameter_data |> 
  mutate(
    D = before-after
  )

diameter_data|> 
  gt()

```

#### Solution: Check Normality of data

```{r}
#| label: fig-data-qq
#| fig-cap: The distribution of the data
#| out-width: 95%

diameter_data |> 
  pivot_longer(
    cols = c("before","after"),
    names_to = "group",
    values_to = "diameter"
  ) |> 
  ggplot(
    aes(
      sample = diameter
    )
  )+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()+
  facet_wrap(~group)


```

#### Solution: Check Normality of the differences

```{r}
#| label: fig-D-diff
#| fig-cap: The distribution of the differences
#| out-width: 95%

diameter_data |> 
  ggplot(
    aes(
      sample = D
    )
  )+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()


```

#### The test

```{r}
#| label: tbl-exercise-paired-data-solution-final
#| tbl-cap: The data for the paired test example


diameter_data_long <- diameter_data |> 
  select(before,after) |> 
  pivot_longer(
    cols = c("before","after"),
    names_to = "group",
    values_to = "diameter")

res_paired <- diameter_data_long |> 
  t_test(diameter ~ group,paired = TRUE) |> 
  add_column("is_paired" = "yes")
res_standard <- diameter_data_long |> 
  t_test(diameter ~ group,paired = FALSE)|> 
  add_column("is_paired" = "no")
    

res_both <- data.frame(
  res_paired
)   |> 
  add_row(
    res_standard
  )
  
res_both |> gt()

```


## Test 2 Variables (> 2 Groups)

```{r}
#| label: fig-tests-TwoVar-nGrps
#| out-width: 75%
#| fig-cap: Statistical tests for one variable.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter000","022_StatisticalTests_TwoVars_nGrps.png"))

```

### Analysis of Variance (ANOVA) - Basic Idea

::: {.content-visible when-profile="script"}
[ANOVA](#anova)'s ability to compare multiple groups or factors makes it widely applicable across diverse fields for analyzing variance and understanding relationships within data.
In the context of engineering sciences the application of ANOVA include:

1. **Experimental Design and Analysis:** 
Engineers often conduct experiments to optimize processes, test materials, or evaluate designs.
ANOVA aids in analyzing these experiments by assessing the effects of various factors (like temperature, pressure, or material composition) on the performance of systems or products. 
It helps identify significant factors and their interactions to improve engineering processes.

2. **Product Testing and Reliability:** 
Engineers use ANOVA to compare the performance of products manufactured under different conditions or using different materials. 
This analysis helps ensure product reliability by identifying which factors significantly impact product quality, durability, or functionality.

3. **Process Control and Improvement:** 
ANOVA plays a crucial role in quality control and process improvement within engineering. 
It helps identify variations in manufacturing processes, such as assessing the impact of machine settings or production methods on product quality. 
By understanding these variations, engineers can make informed decisions to optimize processes and minimize defects.

4. **Supply Chain and Logistics:** 
In engineering logistics and supply chain management, ANOVA aids in analyzing the performance of different suppliers or transportation methods. 
It helps assess variations in delivery times, costs, or product quality across various suppliers or logistical approaches.

5. **Simulation and Modeling:** 
In computational engineering, ANOVA is used to analyze the outputs of simulations or models. 
It helps understand the significance of different input variables on the output, enabling engineers to refine models and simulations for more accurate predictions.

:::

```{r}
#| label: fig-ANOVA-basic-idea
#| out-width: 95%
#| fig-cap: The basic idea of an ANOVA.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","025_ANOVA_nd.png"))

```

::: {.content-visible when-profile="script"}

Across such fields ANOVA is often used to:

**Comparing Means:** 
ANOVA is employed when comparing means between three or more groups. 
It assesses whether there are statistically significant differences among the means of these groups. 
For instance, in an experiment testing the effect of different fertilizers on plant growth, ANOVA can determine if there's a significant difference in growth rates among the groups treated with various fertilizers.

**Modeling Dependencies:** 
ANOVA can be extended to model dependencies among variables in more complex designs. 
For instance, in factorial ANOVA, it's used to study the interaction effects among multiple independent variables on a dependent variable.
This allows researchers to understand how different factors might interact to influence an outcome.

**Measurement System Analysis (MSA):** 
ANOVA is integral in MSA to evaluate the variation contributed by different components of a measurement system. 
In assessing the reliability and consistency of measurement instruments or processes, ANOVA helps in dissecting the total variance into components attributed to equipment variation, operator variability, and measurement error.

As with statistical tests before, the applicability of the ANOVA depends on various factors.

:::


#### Sum of squared error (SSE)

::: {.content-visible when-profile="script"}

The [sum of squared errors](#sse) is a statistical measure used to assess the goodness of fit of a model to its data. 
It is calculated by squaring the differences between the observed values and the values predicted by the model for each data point, then summing up these squared differences. 
The SSE indicates the total variability or dispersion of the observed data points around the fitted regression line or model. 
Lower SSE values generally indicate a better fit of the model to the data.

\begin{align}
SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \label{sse}
\end{align}

:::

```{r}
#| label: fig-sse
#| out-width: 70%
#| fig-cap: A graphical depiction of the SSE.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","027_SSE.png"))

```

#### Mean squared error (MSE) 

::: {.content-visible when-profile="script"}

The [mean squared error](#mse) is a measure used to assess the average squared difference between the predicted and actual values in a dataset. 
It is frequently employed in regression analysis to evaluate the accuracy of a predictive model. 
The MSE is calculated by taking the average of the squared differences between predicted values and observed values. 
A lower MSE indicates that the model's predictions are closer to the actual values, reflecting better accuracy.

:::

::: {.v-center-container}

\begin{align}
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \label{mse}
\end{align}

:::

### One-way ANOVA 

::: {.content-visible when-profile="script"}

The one-way analysis of variance (ANOVA) is used for continuous data with three or more independent groups. 
It assesses whether there are significant differences in means among these groups, assuming a normal distribution.

:::

#### Prerequisites

- **Null Hypothesis:** True mean difference is equal to 0.
- **Prerequisites:**
  - equal variances
  - Number of groups $>2$
  - One response, one predictor variable

#### One-way AOVA - basic idea

```{r}
#| label: fig-one-way-ANOVA-basic-idea
#| out-width: 75%
#| fig-cap: The basic idea of a One-way ANOVA.
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","026_One-Way-ANOVA.png"))

```

#### Check Variances

The most important prerequisite for a One-way ANOVA are equal variances.
Because there are more than two groups, the Bartlett test (as introduced in @sec-bartlett) is chosen (data is normally distributed).

```{r}

bartlett.test(diameter ~ group,data = drive_shaft)

```



Because $p<\alpha = 0.05$ the variances are different.

#### Where are the variances equal?

```{r}
#| label: fig-one-way-ANOVA-boxplot
#| out-width: 95%
#| fig-cap: The groups with equal variance are highlighted.
#| fig-pos: "H"

drive_shaft %>% 
  ggplot(aes(x = group, y = diameter,fill = group))+
  geom_boxplot()+
  gghighlight(group %in% c("group01","group04","group03"))+
  scale_fill_brewer(palette = "Set1")+
  theme_few()+
  theme(legend.position = "bottom")
```

#### More tests


```{r}
bartlett.test(diameter ~ group,data = drive_shaft %>% filter(group %in% c("group01","group04","group03")))
```

With $p>\alpha=0.05$ $H_0$ is accepted, the variances of `group01`, `group02` and `group03` are equal.

#### The whole game

Of course, many software package provide an automated way of performing a One-way ANOVA, but the first will be explained in detail.
The general model for a One-way ANOVA is shown in \eqref{onewayanova}.

\begin{align}
Y \sim X + \epsilon \label{onewayanova}
\end{align}

- $H_0$: All population means are equal.
- $H_a$: Not all population means are equal.

For a One-way ANOVA the [predictor variable](#X) $X$ is the mean ($\bar{x}$) of all datapoints $x_i$.

::: {.content-visible when-profile="script"}

First the SSE and the MSE is calculated for the complete model ($H_a$ is true), see @tbl-anova-ow-complete.
The complete model means, that every mean, for every group is calculated and the [$SSE$](#sse) according to \eqref{sse} is calculated.

:::

#### Complete Model

```{r}
#| label: fig-anova-mdls-comp
#| fig-cap: Computation of error for the complete model (mean per group as model)
#| out-width: 95%

load (here("data","drive_shaft_data.Rdata"))

grp_dat <- drive_shaft %>% filter(group %in% c("group01","group04","group03")) %>% 
  ungroup() %>% 
  group_by(group) %>% 
  mutate(
    fit = mean(diameter),
    error = fit-diameter,
    sq_error = error^2
  ) 

mean_dat <- drive_shaft %>% filter(group %in% c("group01","group04","group03")) %>% 
  mutate(
    fit = mean(diameter),
    error = fit-diameter,
    sq_error = error^2
  )

mean_grp_dat <-  grp_dat %>% 
  group_by(group) %>% 
  summarise(
    mean_diameter = mean(diameter)
  )

grp_dat %>% 
  ggplot()+
  geom_segment(
    aes(
      x = sample_no,
      xend = sample_no,
      y = fit,
      yend = diameter
    )
  )+
  geom_point(
    aes(x = sample_no,y = diameter)
  )+
  geom_hline(
    data = mean_grp_dat,
    aes(
      yintercept = mean_diameter
      )
    )+
  facet_wrap(
    ~group)+
  labs(
    title = "computations of error for the complete model",
    subtitle = "model = mean per group",
    x = "sample no",
    y = "diameter"
  )
```

#### Reduced Model

```{r}
#| label: fig-anova-mdls-red
#| fig-cap: Computation of error for the reduced model (overall mean as model)
#| out-width: 95%

mean_dat %>% 
  ggplot()+
  geom_segment(
    aes(
      x = sample_no,
      xend = sample_no,
      y = fit,
      yend = diameter
    )
  )+
  geom_point(
    aes(x = sample_no,y = diameter)
  )+
  geom_hline(
    # aes(
      yintercept = mean_dat$fit[1],
      # linetype = "overall mean"
    # )
  )+
  facet_wrap(
    ~group
  )+
  labs(
    title = "computations of error for the reduced model",
    subtitle = "model = overall mean",
    x = "sample no",
    y = "diameter"
  )


```

#### SE and MSE

```{r}
#| label: tbl-anova-ow-complete
#| tbl-cap: The SSE and MSE for the complete model.

load (here("data","drive_shaft_data.Rdata"))

aov_ow_tmp_complete <- drive_shaft %>% filter(group %in% c("group01","group04","group03")) %>% 
  ungroup() %>% 
  group_by(group) %>% 
  mutate(
    fit = mean(diameter),
    error = fit-diameter,
    sq_error = error^2
  ) 

aov_ow_complete <- aov_ow_tmp_complete %>% 
  ungroup() %>% 
  summarise(
    n = n(), 
    sse = sum(sq_error)
    ) %>% 
  mutate(p = 3) %>%  #number of groups
  mutate(
    df = n - p
    ) %>% 
  select(sse, df, n, p) %>% 
  mutate(mse = sse / df) 

aov_ow_complete %>% 
  gt() %>% 
    fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 25
  ) %>% 
  as_raw_html()

```

#### SE and MSE comparison

Then, the SSE and the MSE is calculated for the reduced model ($H_0$ is true).
In the reduced model, the mean is not calculated per group, the overall mean is calculated (results in @tbl-anova-ow-reduced).

```{r}
#| label: tbl-anova-ow-reduced
#| tbl-cap: The SSE and MSE from the reduced model.

aov_ow_tmp_reduced <- drive_shaft %>% filter(group %in% c("group01","group04","group03")) %>% 
  mutate(
    fit = mean(diameter),
    error = fit-diameter,
    sq_error = error^2
  ) %>% 
  ungroup()

aov_ow_reduced <- aov_ow_tmp_reduced %>% 
  ungroup() %>% 
  summarise(
    n = n(), 
    sse = sum(sq_error)
    ) %>% 
  mutate(p = 1) %>%  #number of groups
  mutate(
    df = n - p
    ) %>% 
  select(sse, df, n, p) %>% 
  mutate(mse = sse / df) # mse = sigma squared 

aov_ow_reduced %>% 
  gt()%>% 
    fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 25
  ) %>% 
  as_raw_html()

```

#### Explained 

The $SSE$, $df$ and $MSE$ explained by the complete model are calculated:

```{r}

explained_sse <- aov_ow_reduced$sse - aov_ow_complete$sse

explained_df <- aov_ow_reduced$df - aov_ow_complete$df

explained_mse <- explained_sse / explained_df 

```

\begin{align}
SSE_{explained} &= SSE_{reduced}-SSE_{complete} = `r round(explained_sse,digits = 2)` \\
df_{explained} &= df_{reduced} - df_{complete} = `r round(explained_df,digits = 2)` \\
MSE_{explained} &= \frac{SSE_{explained}}{df_{explained}} = `r round(explained_mse,digits = 2)`
\end{align}

#### Ratio of Variances

The ratio of the  variance (MSE) as explained by the complete model to the reduced model is then calculated.
The probability of this statistic is afterwards calculated (if $H_0$ is true).

```{r}
f <- explained_mse/aov_ow_complete$mse

p_aov_ow <- pf(f, explained_df, aov_ow_complete$df, lower.tail = F) %>% print()

aov_ow_data <- drive_shaft %>% 
  filter(group %in% c("group01","group04","group03")) %>% 
  mutate(group = as.factor(group))

aov_res <- aov(diameter~group,data =aov_ow_data)

```

The probability of a F-statistic with $pf = `r round(f,digits = 3)`$ is $`r round(p_aov_ow,digits = 2)`$.

#### Crosscheck

A crosscheck with a automated solution (`aov`-function) yields the results shown in @tbl-anova-ow-r-sol.

```{r}
#| label: tbl-anova-ow-r-sol
#| tbl-cap: The ANOVA results from the aov function.

aov_res %>% 
  tidy() %>% 
  gt() %>% 
    fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 25
  ) %>% 
  as_raw_html()

```

#### Sanity Checks

Some sanity checks are of course required to ensure the validity of the results.
First, the variance of the residuals must be equal along the groups (see @fig-anova-ow-variance).

```{r}
#| label: fig-anova-ow-variance
#| out-width: 75%
#| fig-cap: The variances of the residuals.
#| fig-pos: "H"

aov_augment <- augment(aov_res)

aov_augment %>% 
  ggplot(
    aes(x = group,
        y = .resid)
  )+
  geom_boxplot()+
  labs(
    title = "The residuals of the ANOVA models"
  )

```

#### residuals

Also, the residuals from the model must be normally distributed (see @fig-anova-ow-qq).

```{r}
#| label: fig-anova-ow-qq
#| out-width: 75%
#| fig-cap: The distribution of the residuals.
#| fig-pos: "H"

aov_augment <- augment(aov_res)

aov_augment %>% 
  ggplot(
    aes(sample = .resid)
  )+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()+
  labs(
    title = "The distribution of the residuals"
  )

```

#### final conclusion

The model seems to be valid (equal variances of residuals, normal distributed residuals).

With $p<\alpha = 0.05$ $H_0$ can be rejected, the means come from different populations.

::: {.content-visible when-profile="slides"}

![](img/thumbs-up.png){width=20%}

:::



### Welch ANOVA

::: {.content-visible when-profile="script"}

**Welch ANOVA:**
Similar to one-way ANOVA, the Welch ANOVA is employed when there are unequal variances between the groups being compared. It relaxes the assumption of equal variances, making it suitable for situations where variance heterogeneity exists.

:::

#### Prerequisites

- **Null Hypothesis:** True mean difference is not equal to 0.
- **Prerequisites:**
  - Number of groups $>2$
  - One response, one predictor variable

#### Variance test

The Welch ANOVA drops the prerequisite of equal variances in groups.
Because there are more than two groups, the Bartlett test (as introduced in @sec-bartlett) is chosen (data is normally distributed).

```{r}

load (here("data","drive_shaft_data.Rdata"))

bartlett.test(diameter ~ group,data = drive_shaft)

```

With $p<\alpha = 0.05$ $H_0$ can be rejected, the variances are not equal.

#### ANOVA table

The ANOVA table for the Welch ANOVA is shown in @tbl-anova-ow-welch.

```{r}
#| label: tbl-anova-ow-welch
#| tbl-cap: The ANOVA results from the ANOVA Welch Test (not assuming equal variances).

load (here("data","drive_shaft_data.Rdata"))


aov_welch_res <- oneway.test(diameter~group, data = drive_shaft, var.equal = FALSE)

aov_welch_res %>% 
  tidy() %>% 
  mutate(
    method = str_wrap(method, width  = 25),
    method = str_replace_all(method, pattern = "\n", replacement = "<br />")
  ) %>% 
  gt() %>% 
  fmt_markdown() %>% 
  fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 15
  ) %>% 
  as_raw_html()

```


### Kruskal Wallis

::: {.content-visible when-profile="script"}

**Kruskal-Wallis Test:**
When dealing with non-normally distributed data, the Kruskal-Wallis test is a non-parametric alternative to one-way ANOVA. 
It is used to evaluate whether there are significant differences in medians among three or more independent groups.

In this example the drive strength is measured using three-point bending.
Three different methods are employed to increase the strength of the drive shaft.
:::

#### Scenario

```{r}
#| label: fig-kruskal-three-point
#| out-width: 75%
#| fig-cap: The mechanical Background for a three-point bending test
#| fig-pos: "H"

knitr::include_graphics(here::here("chapter002","028_three_point_bending.png"))

```

* Method A: baseline material
* Method B: different geometry
* Method C: different material

::: {.content-visible when-profile="script"}

In @fig-kruskal-raw-dat the raw drive shaft strength data for Method A, B and C is shown.
At first glance, the data does not appear to be normally distributed.

:::

#### The data

```{r}
#| label: fig-kruskal-raw-dat
#| out-width: 95%
#| fig-cap: The raw data from the drive shaft strength testing.
#| fig-pos: "H"
#| 
load(file = here("data","drive_shaft_kruskal_wallis.Rdata"))

kw_shaft_data %>% 
  ggplot(aes(x = group, y = strength))+
  geom_half_boxplot()+
  geom_half_violin(side = "r")+
  scale_y_continuous(
    limits = c(0,NA),
    expand = c(0,0,0.05,0)
  )+
  labs(
    title = "Drive shaft strength vs. production method",
    x = "",
    y = bquote("strength in "~frac(N,mm^2))
  )

```


#### Check Disitribution

::: {.content-visible when-profile="script"}

In @fig-kruskal-qq the visual test for normal distribution is performed.
The data does not appear to be normally distributed.

:::

```{r}
#| label: fig-kruskal-qq
#| out-width: 95%
#| fig-cap: The qq-plot for the drive shaft strength testing data.
#| fig-pos: "H"
 
kw_shaft_data %>% 
  ggplot(aes(sample = strength))+
  stat_qq_band()+
  stat_qq_line()+
  stat_qq_point()+
  facet_wrap(~group)

```

#### The test

::: {.content-visible when-profile="script"}

The Kruskal-Wallis test is then carried out.
With  $p< \alpha = 0.05$ it is shown, that the groups come from populations with different means.
The next step is to find which of the groups are different using a post-hoc analysis.

:::

```{r}

kruskal.test(strength~group, data = kw_shaft_data)

```

#### multiple tests

The Kruskal-Wallis Test (as the ANOVA) can only tell you, if there is a signifcant difference between the groups, not what groups are different.
Post-hoc tests are able to determine such, but must be used with a correction for multiple testing (see [@Tamhane1977])


```{r}

pairwise.wilcox.test(kw_shaft_data$strength, kw_shaft_data$group, p.adjust.method = "bonferroni") 

```

Because  $p<\alpha = 0.05$ it can be concluded, that all means are different from each other.

### repeated measures ANOVA

::: {.content-visible when-profile="script"}

**Repeated Measures ANOVA:**
The repeated measures ANOVA is applicable when you have continuous data with multiple measurements within the same subjects or units over time. 
It is used to assess whether there are significant differences in means over the repeated measurements, under the assumptions of sphericity and normal distribution.

In this example, the diameter of $n = 20$ drive shafts is measured after three different steps.

- Before Machining
- After Machining
- After Inspection

:::

#### The data

```{r}
#| label: fig-rep-meas-raw
#| out-width: 95%
#| fig-cap: The raw data for the repeated measures ANOVA.
#| fig-pos: "H"

load(file = here("data","drive_shaft_repeated_measures.Rdata"))

level_order <- c("Before_Machining","After_Machining","After_Inspection")

rep_meas_ds %>% 
  ggplot(
    aes(x = factor(timepoint, level = level_order),
        y = diameter
        )
    )+
  geom_line(
    aes(
      group = Subject_ID
    ),
    color = "grey",
    position = position_jitter(width = 0.1, seed = 123),
  )+
  geom_half_boxplot(
    outlier.shape = NA,
    fill = "steelblue",
    alpha = 0.5
    )+
  geom_half_violin(
    side = "r",
    fill = "steelblue",
    alpha = 0.5
    )+
  geom_point(
    aes(
      group = Subject_ID
      ),
    position = position_jitter(width = 0.1, seed = 123),
    shape = 21,
    fill = "white",
    color = "black",
    size = 2,
    stroke = 1
    )+
  ggrepel::geom_label_repel(
    aes(label = Subject_ID),
    position = position_jitter(width = 0.1,seed = 123),
    max.overlaps = 100
    )+
  labs(
    title = "repeatedly measured diameters at different timepoints",
    x = "",
    y = "diameter"
  )

```

#### Outlier detection

First, outliers are identified.
There is no strict rule to identify outliers, in this case a classical measure is applied according to \eqref{outlierrule}

\begin{align}
\text{outlier} &=
\begin{cases}
x_i & >Q3 + 1.5 \cdot IQR \\
x_i & <Q1 - 1.5 \cdot IQR
\end{cases}
\label{outlierrule}
\end{align}

```{r}

detected_outliers <- rep_meas_ds %>% 
  group_by(timepoint) %>% 
  identify_outliers(diameter) %>% 
  print()

rep_meas_ds <- rep_meas_ds %>% 
  filter(Subject_ID != detected_outliers$Subject_ID)

```

#### Check Distributions

A check for normality is done employing the Shapiro-Wilk test [@shapiro1965analysis].

```{r}

rep_meas_ds %>% 
  group_by(timepoint) %>% 
  shapiro_test(diameter) %>% 
  gt() %>% 
  fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 15
  )  %>% 
  as_raw_html()

```

#### Sphericity

The next step is to check the dataset for sphericity, meaning to compare the variance of the groups among each other in order to determine the equality thereof.
For this the Mauchly Test for sphericity is employed [@Mauchly1940].


```{r}

out <- rep_meas_ds %>% 
  anova_test(
    dv = diameter,
    wid = Subject_ID,
    within = timepoint
  )

print(out[[2]])
  

```

With $p>\alpha = 0.05$ $H_0$ is accepted, the variances are equal.
Otherwise sphericity corrections must be applied [@Greenhouse1959].

::: {.content-visible when-profile="script"}

The next step is to perform the repeated measures ANOVA, which yields the following results.

:::

#### The test

```{r}

out %>% get_anova_table() %>% 
  gt() %>% 
  fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 15
  ) %>% 
  as_raw_html()

```

With  $p<\alpha = 0.05$ $H_0$ is rejected, the different timepoints yield different diameters. 
Which groups are different is then determined using a post-hoc test, including a correction for the significance level [@bonferroni1936].


::: {.content-visible when-profile="script"}

In this case, the assumptions for a t-test are met, the pairwise t-test can be used.

:::

#### p-value adjustment

```{r}

rep_meas_ds %>% 
  pairwise_t_test(diameter~timepoint, paired = TRUE, p.adjust.method = "bonferroni") %>% 
  select(-.y.) %>% 
  rename("signif"="p.adj.signif") %>% 
  gt() %>% 
  fmt_number(
    columns = c(statistic,p,p.adj),
    decimals = 3
    ) %>% 
  tab_options(
    table.font.size = 15
  ) %>% 
  as_raw_html()

```

with  $p<\alpha = 0.05$ $H_0$ is rejected for the comparison `Before_Machining - After_Machining` and `After_Inspection - Before_Machining`.
It can therefore be concluded that the machining has a significant influence on the diameter, whereas the inspection has none.

### Friedman test

::: {.content-visible when-profile="script"}

The Friedman test is a non-parametric alternative to repeated measures ANOVA [@Friedman1937]. 
It is utilized when dealing with non-normally distributed data and multiple measurements within the same subjects. 
This test helps determine if there are significant differences in medians over the repeated measurements.

:::

#### The test

The same data as for the repeated measures ANOVA will be used.

```{r}
load(file = here("data","drive_shaft_repeated_measures.Rdata"))

out_fried <- rep_meas_ds %>% 
  friedman_test(
    diameter~timepoint|Subject_ID
  ) 
out_fried %>% 
  gt() %>% 
  fmt_number(decimals = 3) %>% 
  tab_options(
    table.font.size = 15
  ) %>% 
  as_raw_html()


```

With $p<\alpha = 0.05$ $H_0$ is rejected, the timepoints play a vital role for the drive shaft parameter.

::: {.content-visible when-profile="slides"}

## References

::: {#refs}

:::

:::

