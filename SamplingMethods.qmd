---
title: "Sampling Methods"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(gtExtras)
library(here)
library(ggthemes)

```

## Sample Size

### Standard Error

```{r}
#| label: fig-se
#| layout-ncol: 2
#| out-width: 95%
#| fig-height: 7 
#| fig-cap: The SE for varying sample sizes $n$
#| fig-subcap: 
#|   - maximum sample size $n = 200$
#|   - sample size for $n = 5 \ldots 50$

se_dat <- data.frame(
  sd = 1,
  n = seq(1,200)
) %>% 
  mutate(
    se = sd/sqrt(n)
  )

plt1 <- se_dat %>% 
  ggplot(
    aes(x = n, y = se)
  )+
  geom_line()+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0,0),
    limits = c(0,NA)
  )+
  theme_few(
    base_size = 20
  )+
  labs(
    title = "The SE vs. the sample size"
  )


tmp_n_1020 <- se_dat %>% filter(n == 10 | n == 20) %>% 
  rowid_to_column() %>%
  pivot_wider(
    names_from = rowid,
    values_from = c(n,se)
  )

tmp_n_1030 <- se_dat %>% filter(n == 10 | n == 30) %>% 
  rowid_to_column() %>%
  pivot_wider(
    names_from = rowid,
    values_from = c(n,se)
  )
  
tmp_n_1040 <- se_dat %>% filter(n == 10 | n == 40) %>% 
  rowid_to_column() %>%
  pivot_wider(
    names_from = rowid,
    values_from = c(n,se)
  )


tmp <- bind_rows(tmp_n_1020,tmp_n_1030,tmp_n_1040) %>% 
  mutate(
    diff_se = se_1 - se_2
  )

plt2 <- se_dat %>% 
  filter(n>5&n<50) %>% 
  ggplot(
    aes(x = n, y = se)
  )+
  geom_line()+
  scale_x_continuous(
    expand = c(0.05,0,0.05,0),
    # limits = c(5,50),
    breaks = seq(0,50,5)
  )+
  scale_y_continuous(
    expand = c(0,0.01,0.01,0),
    # limits = c(0,0.5)
  )+
  geom_point(
    data = se_dat %>% filter(n == 10 | n == 20 | n == 30 | n == 40),
    shape = 1,
    size = 4
  )+
  geom_segment(
    data = tmp,
    aes(x = n_2,
        xend = n_2,
        y = se_1,
        yend = se_2)
  )+
  geom_segment(
    data = tmp,
    aes(x = n_1,
        xend = n_2,
        y = se_1,
        yend = se_1)
  )+
  geom_text(
    data = se_dat %>% filter(n == 10 | n == 20 | n == 30 | n == 40),
    aes(
      label = se %>% round(.,digits = 3),
      ),
    nudge_x = -1,
    nudge_y = -0.01
  )+
  geom_label(
    data = tmp,
    aes(
      x = n_2,
      y = (se_1 + se_2)/2,
      label = paste0(diff_se %>% round(.,digits = 3)*100,"%")
    )
  )+
  theme_few(
    base_size = 15
  )+
  labs(
    title = "The SE vs. the sample size (n = 5 ... 50 and sd = 1)"
  )

plt1
plt2

```

::: {.content-visible when-profile="script"}

[Standard error](#se) is a statistical measure that quantifies the variation or uncertainty in sample statistics, particularly the mean (average). 
It is a valuable tool in inferential statistics and provides an estimate of how much the sample mean is expected to vary from the true population mean.

\begin{align}
SE = \frac{sd}{\sqrt{n}}
\end{align}

A smaller [standard error](#se) indicates that the sample mean is likely very close to the population mean, while a larger standard error suggests greater variability and less precision in estimating the population mean. 
[Standard error](#se) is crucial when constructing confidence intervals and performing hypothesis tests, as it helps in assessing the reliability of sample statistics as estimates of population parameters.

**Variance vs. Standard Deviation:** The standard error formula is based on the standard deviation of the sample, not the variance. 
The standard deviation is the square root of the variance. 

**Scaling of Variability:** The purpose of the standard error is to measure the variability or spread of sample means. 
The square root of the sample size reflects how that variability decreases as the sample size increases. 
When the sample size is larger, the sample mean is expected to be closer to the population mean, and the standard error becomes smaller to reflect this reduced variability.

**Central Limit Theorem:** The inclusion of $\sqrt{n}$ in the standard error formula is closely tied to the Central Limit Theorem, which states that the distribution of sample means approaches a normal distribution as the sample size increases. 
$\sqrt{n}$ helps in this context to ensure that the standard error appropriately reflects the distribution's properties.

:::

## Random Sampling {.smaller}

:::{.r-stack}

:::{.fragment .fade-in-then-out}

```{r}
#| label: fig-rand-smpl
#| out-width: 85%
#| fig-cap: The idea of random sampling (Dan Kernler).
#| fig-pos: "H"

knitr::include_graphics(here("SamplingMethods","Simple_random_sampling.png"))

```
:::

:::{.fragment .fade-in-then-out}

- **Definition:** Selecting a sample from a population in a purely random manner, where every individual has an equal chance of being chosen.
- **Advantages:**
  - Eliminates bias in selection.
  - Results are often representative of the population.
- **Disadvantages:**
  - Possibility of unequal representation of subgroups.
  - Time-consuming and may not be practical for large populations.

:::

:::

## Stratified Sampling {.smaller}

:::{.r-stack}

:::{.fragment .fade-in-then-out}

```{r}
#| label: fig-strat-smpl
#| out-width: 85%
#| fig-cap: The idea of stratified sampling (Dan Kernler)
#| fig-pos: "H"

knitr::include_graphics(here("SamplingMethods","Stratified_sampling.png"))

```
:::

:::{.fragment .fade-in-then-out}

- **Definition:** Dividing the population into subgroups or strata based on certain characteristics and then randomly sampling from each stratum.
- **Advantages:**
  - Ensures representation from all relevant subgroups.
  - Increased precision in estimating population parameters.
- **Disadvantages:**
  - Requires accurate classification of the population into strata.
  - Complexity in implementation and analysis.

:::

:::

## Systematic Sampling {.smaller}

:::{.r-stack}

:::{.fragment .fade-in-then-out}

```{r}
#| label: fig-syst-smpl
#| out-width: 75%
#| fig-cap: The idea of systematic sampling (Dan Kernler)
#| fig-pos: "H"

knitr::include_graphics(here("SamplingMethods","Systematic_sampling.png"))

```
:::

:::{.fragment .fade-in-then-out}

- **Definition:** Choosing every kth individual from a list after selecting a random starting point.
- **Advantages:**
  - Simplicity in execution compared to random sampling.
  - Suitable for large populations.
- **Disadvantages:**
  - Susceptible to periodic patterns in the population.
  - If the periodicity aligns with the sampling interval, it can introduce bias.

:::

:::

## Cluster Sampling {.smaller}

:::{.r-stack}

:::{.fragment .fade-in-then-out}

```{r}
#| label: fig-clust-smpl
#| out-width: 75%
#| fig-cap: The idea of clustered sampling (Dan Kernler).
#| fig-pos: "H"

knitr::include_graphics(here("SamplingMethods","Cluster_sampling.png"))

```
:::

:::{.fragment .fade-in-then-out}

- **Definition:** Dividing the population into clusters, randomly selecting some clusters, and then including all individuals from the chosen clusters in the sample.

- **Advantages:**
  - Cost-effective, especially for geographically dispersed populations.
  - Reduces logistical challenges compared to other methods.
- **Disadvantages:**
  - Increased variability within clusters compared to other methods.
  - Requires accurate information on cluster characteristics.

:::

:::

## Example - The Star Wars dataset

```{r}
#| include: false

data(starwars) 
starwars <- starwars |> select(-films, -vehicles, -starships)   
head(starwars)                                            
 
set.seed(984362)                                         
```

### Get to know the data

```{r}
#| label: tbl-starwarsdata
#| tbl-cap: The starwars dataset

starwars |> summary()
```

### Simple Random Sampling

```{r}
#| echo: true
#| output: true


starwars_srswor <- starwars %>%                         
  sample_n(size = 5)
starwars_srswor                                          
```

### Simple Random Sampling with replacment

```{r}
#| echo: true
#| output: true


starwars_srswr <- starwars %>%                            
  sample_n(size = 5,
           replace = TRUE)
starwars_srswr                                           
```

### Sampling with replacment, sample larger than original data

```{r}
#| echo: true
#| output: true


starwars_srswr2 <- starwars %>%                           
  sample_n(size = 200,
           replace = TRUE)
starwars_srswr2                                          
 
mean(starwars$height, na.rm = TRUE)                      
mean(starwars_srswr2$height, na.rm = TRUE)
```

### Systematic Sampling

Sample always the $5th$.

```{r}
#| echo: true
#| output: true


starwars_syst <- starwars %>%                            
  slice(seq(sample(1:5, 1),                              
            nrow(starwars),                              
            by = 5))                                   
starwars_syst                                           


```

### Stratified Sampling

```{r}
#| echo: true
#| output: true

table(starwars$sex)                                      
 
starwars_strat <- starwars %>%                          
  group_by(sex) %>%
  sample_frac(size = 0.3)
starwars_strat                                           
 
table(starwars_strat$sex)                               

```

### Clustered Sampling

```{r}
#| label: tbl-starwarsdata-clust
#| tbl-cap: The starwars dataset with clustered sampling

starwars_clust <- starwars %>%                          
  filter(homeworld %in% sample(unique(homeworld),
                               size = 10))
starwars_clust |> summary()

```


## Bootstrapping {.smaller}

:::{.r-stack}

:::{.fragment .fade-in-then-out}

```{r}
#| label: fig-smpl-btstrp
#| out-width: 75%
#| fig-cap: The idea of bootstrapping (Biggerj1, Marsupilami)
#| fig-pos: "H"

knitr::include_graphics(here("SamplingMethods","Illustration_bootstrap.png"))

```

:::

:::{.fragment .fade-in-then-out}

- **Definition:** Estimating sample statistic distribution by drawing new samples with replacement from observed data, providing insights into variability without strict population distribution assumptions.

- **Advantages:**
  - Non-parametric: Works without assuming a specific data distribution.
  - Confidence Intervals: Facilitates easy estimation of confidence intervals.
  - Robustness: Reliable for small sample sizes or unknown data distributions.

- **Disadvantages:**
  - Computationally Intensive: Resource-intensive for large datasets.
  - Results quality relies on the representativeness of the initial sample (garbage in - garbage out).
  - Cannot compensate for inadequate information in the original sample.
  - Not Always Optimal: Traditional methods may be better in cases meeting distribution assumptions.

:::

:::

::: {.content-visible when-profile="slides"}

### Bootstrapping in the classroom

:::{.r-stack}

:::{.fragment .fade-in-then-out}

:::{style="font-size: 80%;"}

![](chapter003/50_cent.jpg){fig-align="center" width=50%}

* imagine all $0.50â‚¬$ coins that are used today
* we are interested in the average minting year
* we can not just look at all the coins, we have to estimate the average minting year
* we assume the 50 coins to be a representative sample (is it though?)

:::

:::

:::{.fragment .fade-in-then-out}

```{r}
#| include: false

# coins_sample <- pennies_sample
set.seed(1253)
coins_sample <- data.frame(
  year = sample(seq(2002,2023),50, replace = TRUE)) %>% 
  rowid_to_column(var = "ID")

```


```{r}

plt_dist_coins <- coins_sample %>% 
  ggplot(
    aes(
      x = year
    )
  )+
  geom_histogram(
    binwidth = 2.5,
    color = "white"
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  scale_x_continuous(
    breaks = seq(1900,2500,2)
  )+
  labs(
    title = "Distribution of the minting year in the 50 coins"
  )+
  theme_bw(
    base_size = 20
  )

plt_dist_coins

```

:::

:::{.fragment .fade-in-then-out}

```{r}

plt_dist_coins+
  geom_vline(
    xintercept = mean(coins_sample$year), 
    color = "red",
    linewidth = 2
    )

```

:::

:::{.fragment .fade-in-then-out}

<!-- ![](chapter003/CoinResamplingPrint.svg){fig-align="center" width=30%} -->

:::{style="font-size: 80%;"}

* Population $N = ?$
* Population $\mu = ? \rightarrow$ we want to know that!
* sample mean $\bar{x} = `r mean(coins_sample$year)`$


* $\bar{x} = `r mean(coins_sample$year)`$ is the *point estimate*
* Strategy to be saver about the *mean minting year*? $\rightarrow$ MORE SAMPLES

:::

:::

:::{.fragment .fade-in-then-out}

![](chapter003/batman.jpg){.r-stretch}

:::

:::{.fragment .fade-in-then-out}

* What is a good sample size to resample (how often)?
* Draw a number from the hat.
* Put the number back into the hat. (This is called: *resampling with replacement*)
* Why do we put the number back into the hat?
* Now do it.

:::

:::

:::