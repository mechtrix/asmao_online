[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistical Methods and Optimization",
    "section": "",
    "text": "Preface\nThis is the script for the lecture “Advanced Statistical Methods and Optimization” at the DIT/Campus Cham. I do realize, that this body of knowledge has been repeated over and over, but have decided to do my own nonetheless so I can add my own flavor to the realms of statistics. This work is heavily inspired by (Wickham and Grolemund 2016). Please note that this material is copyrighted, you are not allowed to copy, at least ask for permission - you are likely to get it.\nContent is subject to change.\nTim Weber, Oct. 2024",
    "crumbs": [
      "Advanced Statistical Methods and Optimization"
    ]
  },
  {
    "objectID": "index.html#acronyms_HEADER_LOA",
    "href": "index.html#acronyms_HEADER_LOA",
    "title": "Advanced Statistical Methods and Optimization",
    "section": "List Of Acronyms",
    "text": "List Of Acronyms\n\nANOVA\n\nAnalysis of Variance\n\nCDF\n\nCumulative Density Function\n\nCI\n\nConfidence Interval\n\nCLT\n\nCentral Limit Theorem\n\nH0\n\nNull-Hypothesis\n\nHa\n\nalternative Hypothesis\n\nIQR\n\nInterquartile Range\n\nLLN\n\nLaw of Large Numbers\n\nPDF\n\nProbability Density Function\n\nPMF\n\nProbability Mass Function\n\nPoI\n\nParameter of Interest\n\nQC\n\nQuality Control\n\nSE\n\nStandard Error\n\nZ\n\nStandard Score / Z-Score\n\ncl\n\nconfidence level\n\ndof\n\ndegree of freedom\n\nwrt\n\nwith respect to",
    "crumbs": [
      "Advanced Statistical Methods and Optimization"
    ]
  },
  {
    "objectID": "index.html#symbol-abbreviations",
    "href": "index.html#symbol-abbreviations",
    "title": "Advanced Statistical Methods and Optimization",
    "section": "Symbol Abbreviations",
    "text": "Symbol Abbreviations\n\nGreek\n\n\n\n\\(\\alpha\\): significance level\n\n\n \n\n\n\\(\\beta\\): false negative risk, \\(\\beta\\)-risk\n\n\n\n\n\\(\\epsilon\\): residuals\n\n\n \n\n\n\\(\\mu_0\\), \\(\\mu\\): the true mean of a population\n\n\n\n\n\\(\\varphi(x)\\): probability density function\n\n\n \n\n\n\\(\\phi(x)\\): cumulative probability density function or cumulative distribution function\n\n\n\n\n\nLatin\n\n\n\n\\(C_g\\): potential Measurement System Capability Index\n\n\n \n\n\n\\(C_{gk}\\): Measurement Capability Index with systematic error\n\n\n\n\n\\(C_p\\): potential process capability\n\n\n \n\n\n\\(C_{pk}\\): actual process capability including centering\n\n\n\n\n\\(\\mathbb{E}[X]\\): expected value of random variable \\(X\\)\n\n\n \n\n\n\\(k\\): number of predictors in a model\n\n\n\n\n\\(n\\): number of data points/observations in the sample\n\n\n \n\n\n\\(N\\): number of datapoints/observations in the population\n\n\n\n\n\\(MSE\\): mean squared error\n\n\n \n\n\n\\(P\\): Probabilities\n\n\n\n\n\\(r\\): range of values\n\n\n \n\n\n\\(r^2\\): Coefficient of determination\n\n\n\n\n\\(r^2_{adjusted}\\): adjusted Coefficient of determination\n\n\n \n\n\n\\(sd\\): the standard deviation of a dataset\n\n\n\n\n\\(SSE\\): Sum of squared errors as calculated by\n\n\n \n\n\n\\(\\mathrm{Var}\\): Variance\n\n\n\n\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science. \"O’Reilly Media, Inc.\".",
    "crumbs": [
      "Advanced Statistical Methods and Optimization"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html",
    "href": "chapter000_BasicConcepts.html",
    "title": "1  Basic Concepts",
    "section": "",
    "text": "1.1 Probability\nStatistics is a fundamental field that plays a crucial role in various disciplines, from science and economics to social sciences and beyond. It’s the science of collecting, organizing, analyzing, interpreting, and presenting data. In this introductory overview, we’ll explore some key concepts and ideas that form the foundation of statistics:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#probability",
    "href": "chapter000_BasicConcepts.html#probability",
    "title": "1  Basic Concepts",
    "section": "",
    "text": "1.1.1 Overview\nProbability theory is a fundamental concept in the field of statistics, serving as the foundation upon which many statistical methods and models are built.\n\n\n1.1.2 What is Probability?\nProbability is a mathematical concept that quantifies the uncertainty or randomness of events. It provides a way to measure the likelihood of different outcomes occurring in a given situation. In essence, probability is a numerical representation of our uncertainty.\n\n\n1.1.3 Basic Probability Terminology\n\nExperiment: An experiment is any process or procedure that results in an outcome. For example, rolling a fair six-sided die is an experiment.\nOutcome: An outcome is a possible result of an experiment. When rolling a die, the outcomes are the numbers 1 through 6.\nSample Space (S): The sample space is the set of all possible outcomes of an experiment. For a fair six-sided die, the sample space is \\(\\{1, 2, 3, 4, 5, 6\\}\\).\nEvent (E): An event is a specific subset of the sample space. It represents a particular set of outcomes that we are interested in. For instance, “rolling an even number” is an event for a six-sided die, which includes outcomes \\(\\{2, 4, 6\\}\\).\n\n\n\n1.1.4 Probability Notation\nIn probability theory, we use notation to represent various concepts:\n\nP(E): Probability of event E occurring.\nP(A and B): Probability of both events A and B occurring.\nP(A or B): Probability of either event A or event B occurring.\nP(E’): Probability of the complement of event E, which is the probability of E not occurring.\n\n\n\n1.1.5 The Fundamental Principles of Probability\nThere are two fundamental principles of probability:\n\nThe Addition Rule: It states that the probability of either event A or event B occurring is given by the sum of their individual probabilities, provided that the events are mutually exclusive (i.e., they cannot both occur simultaneously).\n\n\\[\\begin{align}\nP(A \\; or \\; B) = P(A) + P(B)\n\\end{align}\\]\n\nThe Multiplication Rule: It states that the probability of both event A and event B occurring is the product of their individual probabilities, provided that the events are independent (i.e., the occurrence of one event does not affect the occurrence of the other).\n\n\\[\\begin{align}\nP(A \\; and\\;B) = P(A) * P(B)\n\\end{align}\\]\n\n\n1.1.6 Example: Rolling a Fair Six-Sided Die\nConsider rolling a fair six-sided die.\n\nSample Space (S): \\(\\{1, 2, 3, 4, 5, 6\\}\\) (Figure 1.1)\nEvent A: Rolling an even number = \\(\\{2, 4, 6\\}\\) (Figure 1.1)\nEvent B: Rolling a number greater than \\(3 = \\{4, 5, 6\\}\\) (Figure 1.1)\n\n\n\n\n\n\n\n\n\nFigure 1.1: This example’s sample space, as well as event A and event B.\n\n\n\n\n\n\n\n1.1.7 Probability in action - The Galton Board\nA Galton board, also known as a bean machine or a quincunx, is a mechanical device that demonstrates the principles of probability and the normal distribution. It was invented by Sir Francis Galton1 in the late 19th century. The Galton board consists of a vertical board with a series of pegs or nails arranged in triangular or hexagonal patterns.\nA Galton board, also known as a bean machine or a quincunx, is a mechanical device that demonstrates the principles of probability and the normal distribution. It was invented by Sir Francis Galton in the late 19th century. The Galton board consists of a vertical board with a series of pegs or nails arranged in triangular or hexagonal patterns.\n\nInitial Release: At the top of the Galton board, a ball or particle is released. This ball can take one of two paths at each peg, either to the left or to the right. The decision at each peg is determined by chance, such as the flip of a coin or the roll of a die. This represents a random event.\nMultiple Trials: As the ball progresses downward, it encounters several pegs, each of which randomly directs it either left or right. The ball continues to bounce off pegs until it reaches the bottom.\nAccumulation: Over multiple trials or runs of the Galton board, you will notice that the balls accumulate in a pattern at the bottom. This pattern forms a bell-shaped curve, which is the hallmark of a normal distribution.\nNormal Distribution: The accumulation of balls at the bottom resembles the shape of a normal distribution curve. This means that the majority of balls will tend to accumulate in the center, forming the peak of the curve, while fewer balls will accumulate at the extreme left and right sides.\n\nThe Galton board is a visual representation of the central limit theorem, a fundamental concept in probability theory. It demonstrates how random events, when repeated many times, tend to follow a normal distribution. This distribution is commonly observed in various natural phenomena and is essential in statistical analysis.\n\n\n\n\n\n\n\n\nFigure 1.2: A Galton board in action.\n\n\n\n\n\n\n1.1.7.1 Statistics and Probabbility\nThe Galton board is a nice example how statistics emerge from probability.\n\n1.1.7.1.1 Define the problem\n\nThe board has \\(n\\) rows of pegs (columns)\nEach ball has an equal probability of moving left or right (assuming no bias)\nThe number of rightward moves determines the final position in the bins\n\n\n\n1.1.7.1.2 Step 2: Binomial Probability Distribution\nEach ball independently moves right (\\(R\\)) or left (\\(L\\)) with a probability of \\(p=0.5\\).\nThe number of rightwards moves follows a binomial distribution.\n\\[\\begin{align}\nP(k) = \\binom{n}{k} p^k (1 - p)^{n - k}\n\\end{align}\\]\n\n\\(n\\)\n\ntotal number of columns (or pegs encountered)\n\n\\(k\\)\n\nnumber of rightward moves\n\n\\(\\binom{n}{k}\\)\n\nbiomial coefficient, given by \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)\n\n\nwith \\(p = 0.5\\) this simplifies to\n\\[\\begin{align}\nP(k) = \\binom{n}{k} ( \\frac{1}{2})^n\n\\end{align}\\]\n\n\n1.1.7.1.3 Step 3: Position Mapping\nThe final position of a ball in a bin corresponds to the number of rightwards moves \\(k\\). If the bins are indexed from \\(0\\) to \\(n\\) (where \\(k=0\\) means all left moves and \\(k=n\\) means all right moves) the probability of landing in bin \\(k\\) is:\n\\[\\begin{align}\nP(k) = \\frac{n!}{k!(n-k)!}(\\frac{1}{2})^n\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#section",
    "href": "chapter000_BasicConcepts.html#section",
    "title": "1  Basic Concepts",
    "section": "1.2 Central Limit Theorem (CLT)",
    "text": "1.2 Central Limit Theorem (CLT)\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: The central limit theorem in action.\n\n\n\n\n\n\n\nThe primary reason for the existence of the normal distribution in many real-world datasets is the CLT (Taboga 2017). The CLT states that when you take a large enough number of random samples from any population, the distribution of the sample means will tend to follow a normal distribution, even if the original population distribution is not normal. This means that the normal distribution emerges as a statistical consequence of aggregating random data points. This is shown in Figure 1.3.\nFrom \\(n=10000\\) uniformly distributed data points (the population) (\\(min=1, max = 100\\)) either \\(2,10,50\\) or \\(200\\) samples are taken randomly (the samples). For each of the samples the mean is calculated, resulting in \\(1000\\) mean values for each (\\(2,10,50\\) or \\(200\\)) sample size. In Figure 1.3 the results from this numerical study are shown. The larger the sample size, the closer the mean calculated \\(\\bar{x}\\)is to the population mean (\\(\\mu_0\\)). The effect is especially large on the standard deviation, resulting in a smaller standard deviation the larger the sample size is.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#section-1",
    "href": "chapter000_BasicConcepts.html#section-1",
    "title": "1  Basic Concepts",
    "section": "1.3 Law of Large Numbers (LLN)",
    "text": "1.3 Law of Large Numbers (LLN)\n\n\n\n\n\n\n\n\nFigure 1.4: The Law of Large Numbers in Action with die rolls as an example.\n\n\n\n\n\nThe CLT states that as the size of a random sample increases, the sample average converges to the population mean. This law, along with the CLT, explains why the normal distribution frequently arises. When you take many small, independent, and identically distributed measurements and compute their averages, these averages tend to cluster around the true population mean, forming a normal distribution Johnson (1994).\nThe LLN ar work is shown in Figure 1.4. A fair six-sided die is rolled 1000 times and the running average of the roll results after each roll is calculated. The resulting line plot shows how the running average approaches the expected value of \\(3.5\\), which is the average of all possible outcomes of the die. The line in the plot represents the running average It fluctuates at the beginning but gradually converges toward the expected value of \\(3.5\\). To emphasize this convergence, a dashed line indicating the theoretical expected value which is essentially the expected value applied to each roll. This visualization demonstrates the Law of Large Numbers, which states that as the number of trials or rolls increases, the sample mean (running average in this case) approaches the population mean (expected value) with greater accuracy, showing the predictability and stability of random processes over a large number of observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#population",
    "href": "chapter000_BasicConcepts.html#population",
    "title": "1  Basic Concepts",
    "section": "1.4 Population",
    "text": "1.4 Population\n\n\n\n\n\nAn example for a population.\n\n\n\n\nIn statistics, a population is the complete set of individuals, items, or data points that are the subject of a study. Understanding populations and how to work with them is fundamental in statistical analysis, as it forms the basis for making meaningful inferences and drawing conclusions about the broader group being studied. It is the complete collection of all elements that share a common characteristic or feature and is of interest to the researcher. The population can vary widely depending on the research question or problem at hand. A populations true mean is depicted with \\(\\mu_0\\) and the variance is depicted with \\(\\sigma_0^2\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#sample",
    "href": "chapter000_BasicConcepts.html#sample",
    "title": "1  Basic Concepts",
    "section": "1.5 Sample",
    "text": "1.5 Sample\n\n\n\n\n\nA sample drawn from the population.\n\n\n\n\nThe key principles behind a sample include its role as a manageable subset of data, which can be chosen randomly or purposefully. Ideally, it should be representative, reflecting the characteristics and diversity of the larger population. Statistical techniques are then applied to this sample to make inferences, estimate population parameters, or test hypotheses. The size of the sample matters, as a larger sample often leads to more precise estimates, but it should be determined based on research goals and available resources. Various sampling methods, such as random sampling, stratified sampling, or cluster sampling, can be employed depending on the research objectives and population characteristics. A samples true mean is depicted with \\(\\bar{x}\\) and the variance is depicted with \\(sd\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#descriptive-statistics",
    "href": "chapter000_BasicConcepts.html#descriptive-statistics",
    "title": "1  Basic Concepts",
    "section": "1.6 Descriptive Statistics",
    "text": "1.6 Descriptive Statistics\nDescriptive statistics are used to summarize and describe the main features of a data set. They provide a way to organize, present, and analyze data in a meaningful and concise manner. Descriptive statistics do not involve making inferences or drawing conclusions beyond the data that is being analyzed. Instead, they aim to provide a clear and accurate representation of the data set. Some common techniques and measures used in descriptive statistics include:\n\n1.6.1 Example Data: The drive shaft exercise\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: The drive shaft specification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.6: Difference between the population of ALL drive shafts and a sample of drive shafts.\n\n\n\n\n\n\n\n\n\n1.6.2 Measures of Central Tendency\nMeasures of central tendency are essential in statistics because they provide a single value that summarizes or represents the center point or typical value of a dataset. The main reasons for using these measures include:\n\nSimplification of Data: They condense large sets of data into a single representative value, making the data easier to understand and interpret.\nComparison Across Datasets: They allow for straightforward comparison between different groups or datasets by providing a common reference point.\nFoundation for Further Analysis: Many statistical techniques and models rely on an understanding of central tendency as a starting point, such as in regression analysis or hypothesis testing.\nDecision-Making: In fields such as economics, education, and public policy, central tendency helps inform decisions based on typical outcomes or behaviors (e.g., average income, median test scores).\nIdentification of Patterns: They help identify patterns and trends over time, especially in time-series data or longitudinal studies.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.7: Some drive shaft sample data in a 2D plot of sample index vs. variable value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.8: Some drive shaft sample data in a 2D plot of sample index vs. variable value\n\n\n\n\n\n\n\n\n1.6.2.1 Mean\n\n\n\n\n\n\n\n\n\n\nFigure 1.9: A graphical depiction of the mean\n\n\n\n\n\n\\[\\begin{align}\n\\text{discrete: } \\mu = \\mathbb{E}[X] &= \\sum_{i=1}^{n} x_i \\, p_i \\\\\n\\text{continous: }\\mu = \\mathbb{E}[X] &= \\int_{-\\infty}^{\\infty} x \\, f(x) \\, dx\n\\end{align}\\]\n\n\n\\[\\begin{align}\n\\text{population:} \\; \\mu &= \\frac{1}{N}\\sum_i^{N} x_i \\\\\n\\text{sample:} \\; \\bar{x} &= \\frac{1}{n}\\sum_i^{n} x_i\n\\end{align}\\]\n\n\n\n\n1.6.2.2 Median\n\n\n\n\n\n\n\n\nFigure 1.10: A graphical depiction of the median\n\n\n\n\n\n\\[\\begin{align}\n\\text{population:} \\; m &=\n\\begin{cases}\nx_{\\left(\\frac{N+1}{2}\\right)} & \\text{if } N \\text{ is odd} \\\\\n\\frac{1}{2} \\left( x_{\\left(\\frac{N}{2}\\right)} + x_{\\left(\\frac{N}{2} + 1\\right)} \\right) & \\text{if } N \\text{ is even}\n\\end{cases} \\\\\n\\text{sample:} \\; \\tilde{x} &=\n\\begin{cases}\nx_{\\left(\\frac{n+1}{2}\\right)} & \\text{if } n \\text{ is odd} \\\\\n\\frac{1}{2} \\left( x_{\\left(\\frac{n}{2}\\right)} + x_{\\left(\\frac{n}{2} + 1\\right)} \\right) & \\text{if } n \\text{ is even}\n\\end{cases}\n\\end{align}\\]\n\n\n\n1.6.3 Measures of Spread\n\n\n\n\n\n\n\n\nFigure 1.11: Spread, Dispersion, Variance … many names for measuring variability of data\n\n\n\n\n\nMeasures of spread (also called measures of dispersion or variability) are essential in statistics to provide information about the distribution of data — specifically, how much the data values differ from each other and from the central tendency.\n\nContextualizing Central Tendency: The mean or median alone does not give a complete picture of the data. Two datasets can have the same mean but very different spreads.\nUnderstanding Data Consistency: Measures of spread indicate how consistent or reliable the data are. A small spread suggests the values are closely clustered around the mean, while a large spread indicates greater variability and less predictability.\nIdentifying Outliers: Large measures of spread may indicate the presence of outliers — values that are significantly different from others in the dataset. This can be important in quality control, risk assessment, and anomaly detection.\nComparing Distributions: Spread allows for meaningful comparison between different datasets.\nInforming Statistical Models: Many statistical methods, such as regression, hypothesis testing, and confidence intervals, rely on measures of spread (like variance or standard deviation) to estimate error, assess significance, or make predictions.\n\n\n1.6.3.1 Range\n\n\n\n\n\n\n\n\nFigure 1.12: A graphical depiction of the range\n\n\n\n\n\n\\[\\begin{align}\n\\text{Range} = x_{\\text{max}} - x_{\\text{min}}\n\\end{align}\\]\nThere is no difference in computing the range for the population or the sample\n\n\n1.6.3.2 Variance\n\n\n\n\n\n\n\n\n\n\nFigure 1.13: A graphical depiction of the variance\n\n\n\n\n\nFor a random variable \\(X\\) with mean \\(\\mu=\\mathbb{E}\\!(X)\\):\n\\[\\begin{align}\n\\mathrm{Var}(X) &= \\mathbb{E}\\!\\big[(X - \\mu)^2\\big] \\label{var} \\\\\n\\mathrm{Var}(X) &= \\mathbb{E}[X^2] - \\big(\\mathbb{E}[X]\\big)^2\n\\end{align}\\]\n\n\nIt is the expected squared deviation from the mean.\n\nAlways non-negative.\nUnits: square of the units of \\(X\\)\n\n\n\n\n\n1.6.3.3 Standad Deviation\n\n\n\\[\\begin{align}\n\\sigma_0 &= \\sqrt{\\mathrm{Var}}\n\\end{align}\\]\n\n\n\\[\\begin{align}\n\\text{population: } \\sigma &= \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2}\\\\\n\\text{sample: } sd &= \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\nFigure 1.14: Why do we divide by n-1 for the sample variance?\n\n\n\n\n\n\n\n\n1.6.3.3.1 The Bessel’s correction\n\n\nThe variance calculated from a sample has one degree of freedom (dof) less, then the population variance.\n\n\n\nImagine you have 5 candies, and you want to give them to 5 friends — one candy to each. You decide how to give the first candy, then the second, third, and fourth. But when you get to the last candy, you have no choice — you have to give it to the last friend, so everyone gets one.\nThat’s kind of like degrees of freedom in statistics. It means how many things you’re free to choose before something has to be a certain way.\nSo if you’re working with 5 numbers, and they all have to add up to a certain total (like a mean), you can choose 4 of them freely, but the last one has to be whatever makes the total come out right. That’s why we say there are 4 degrees of freedom — 4 numbers you can choose any way you want.\n\n\n\n\n\\[\\{2,4,6\\}\\]\n\nMean: \\(\\bar{x} = \\frac{2+4+6}{3} = 4\\)\nDeviations: \\(-2,0,2\\)\nSquared Deviations: \\(4,0,4\\)\nSum of squared deviations: \\(8\\)\n\nwith Bessel’s correction:  \\(sd^2 = \\frac{8}{3-1} = 4\\)\nwithout Bessel’s correction:  \\(sd^2 = \\frac{8}{3} \\approx 2.67\\) (biased, underestimates variance)\n\n\n\nWhen computing the variance from a sample, we need to calculate \\(\\bar{x}\\), which uses up one degree of freedom and biases our estimate\n\n\n\n\n1.6.3.3.2 Bessel’s correction with increasing sample size\n\n\n\n\n\n\n\n\n\n\nFigure 1.15: The biased (n) variance is not as prcises as the unbiased (n-1) variance estimate. This effect decreases with increasing sample size.\n\n\n\n\n\n\n\n\n\n\n1.6.3.4 Percentiles, quantiles\n\n\n\nPercentiles: Divide data into \\(100\\) equal parts. The \\(p\\)th percentile is the value below which p% of the observations fall.\nQuantiles: Generalization of percentiles. The q-th quantile is the value below which a fraction q of the data falls. For example:\n\n\\(0.25\\) quantile: \\(25\\)th percentile - first quartile (Q1)\n\\(0.50\\) quantile: \\(50\\)th percentile - median\n\\(0.75\\) quantile: \\(75\\)th percentile - third quartile (Q3)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.16: A graphical depiction of various percentiles, quantiles\n\n\n\n\n\n\n\n\n\n\n1.6.4 Histogram\n\n\n\n\n\n\n\n\nFigure 1.17: An example for descriptive statistics (histogramm)\n\n\n\n\n\nAn example for descriptive statistics is shown in Figure 1.17 as a histogram. It shows data from a company that produces pharmaceutical syringes, taken from Ramalho (2021). During the production of those syringes, the so called barrel diameter is a critical parameter to the function of the syringe and therefore of special interest for the Quality Control.\nA histogram as shown in Figure 1.17 shows the data of 150 measurements during the QC. On the x-axis the barrel diameter is shown, while the count of each binned diameter is shown on the y-axis. The binning and of data is a crucial parameter for such a plot, because it already changes the appearance and width of the bars. Binning is a trade-off between visibility and readability.\n\n\n1.6.5 Density plot\n\n\n\n\n\n\n\n\nFigure 1.18: An example for a density plot for the syringe data (barrel diameter).\n\n\n\n\n\nDensity plots are another way of displaying the statistical distribution of an underlying dataset. The biggest strength of those plots is, that no binning is necessary in order to show the data. The limitation of this kind of plot is the interpretability. An example of a density plot for the syringe data is shown in Figure 1.18. On the x-axis the syringe barrel diameter is shown (as in a histogram). The y-axis in contrast does not display the count of a binned category, but rather the Probability Density Function (PDF) for the specific diameter. The grey area under the density curve depicts the probability of a syringe diameter to appear in the data. The complete area under the curve equals to \\(1\\) meaning that a certain diameter is sure to appear in the data.\n\n\n1.6.6 Boxplot\n\n\n\n\n\n\n\n\nFigure 1.19: A boxplot of the same syringe data combined with the according histogram.\n\n\n\n\n\nIt is very common to include and inspect measures of central tendency in the graphical depiction of data. A boxplot, also known as a box-and-whisker plot, is a very common way of doing this. A boxplot is a graphical representation of a dataset’s distribution. It displays the following key statistics:\n\nMedian (middle value).\nQuartiles (\\(25^{th}\\) and \\(75^{th}\\) percentiles), forming a box.\nMinimum and maximum values (whiskers).\nOutliers (data points significantly different from the rest).\n\nThe syringe data in boxplot form is shown in Figure 1.19 as an overlay of the histogram plot before. Boxplots are useful for quickly understanding the central tendency, spread, and presence of outliers in a dataset, making them a valuable tool in data analysis and visualization.\n\n\n1.6.7 Average, Standard deviation and Range\n\n\n\n\n\n\n\n\nFigure 1.20: A histogram of the syringe data with mean, standard deviation and range.\n\n\n\n\n\nVery popular measures of central tendency include the average (mean) and the standard deviation (variance) of a dataset. The computed mean from an actual dataset is depicted with \\(\\bar{x}\\) and calculated via \\(\\eqref{mean}\\).\n\\[\\begin{align}\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i \\label{mean}\n\\end{align}\\]\nWith [\\(n\\)]{index.qmd#n-gloss} being the number of datapoints and \\(x_i\\) being the datapoints. The mean is therefore the sum of all datapoints divided by the total number \\(n\\) of all datapoints. It is not to be confused with the true mean \\(\\mu_0\\) of a population.\nThe computed standard deviation from an actual dataset is depicted with [\\(sd\\)]{index.qmd#sd-gloss} and calculated via \\(\\eqref{sd}\\).\n\\[\\begin{align}\nsd = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\label{sd}\n\\end{align}\\]\nThe standard deviation can therefore be explained as the square root of the sum of all differences of each individual datapoints to the mean of a dataset divided by the number of datapoints. It is not to be confused with the true variance \\(\\sigma_0^2\\) of a population. The variance of a dataset can be calculated via \\(\\eqref{var2}\\).\n\\[\\begin{align}\n\\sigma = sd^2 \\label{var2}\n\\end{align}\\]\nThe range from an actual dataset is depicted with \\(r\\) and calculated via \\(\\eqref{range}\\).\n\\[\\begin{align}\nr = \\max(x_i) - \\min(x_i) \\label{range}\n\\end{align}\\]\nThe range can therefore be interpreted as the range from minimum to maximum in a dataset.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#sec-vis-grps",
    "href": "chapter000_BasicConcepts.html#sec-vis-grps",
    "title": "1  Basic Concepts",
    "section": "1.7 Visualizing Groups",
    "text": "1.7 Visualizing Groups\n\n1.7.1 Boxplots\n\n\n\n\n\n\n\n\nFigure 1.21: Boxplots of the syringe data with the samples as groups.\n\n\n\n\n\nThe methods described above are especially useful when it comes to visualizing groups in data. The data is discretized and the information density is increased. As with every discretization comes also a loss of information. It is therefore strongly advised to choose the right tool for the job.\nIf the underlying distribution of the data is unknown, a good start to visualize groups within data is usually a boxplot as shown in Figure 1.21. The syringe data from Ramalho (2021) contains six different groups, one for every sample drawn. Each sample consists of 25 observations in total. On the x-axis the diameter in mm is shown, the y-axis depicts the sample number. The boxplots are then drawn as described above (median, \\(25^{th}\\) and \\(75^{th}\\) percentile box, \\(5^{th}\\) and \\(95^{th}\\) whisker). The \\(25^{th}\\) and \\(75^{th}\\) percentile box is also known as the Interquartile Range (IQR).\n\n\n1.7.2 Mean and standard deviation plots\n\n\n\n\n\n\n\n\nFigure 1.22: Mean and standard deviation plots of the groups in the dataset.\n\n\n\n\n\nIf the data follows a normal distribution, showing the mean and standard deviation for each group is also very common. For the syringe dataset, this is shown in Figure 1.22. The plot follows the same logic as for the boxplots (x-axis-data, y-axis-data), but the data itself shows the mean with a \\(\\times\\)-symbol, as the length of the horizontal errorbars accords to \\(\\bar{x} \\pm sd(x)\\).\n\n\n1.7.3 Half-half plots\n\n\n\n\n\n\n\n\nFigure 1.23: Half-half plots that incooperate different types of plots\n\n\n\n\n\nBoxplots and mean-and-standard-deviation plots sometimes hide some details within the data, that may be of interest or simply important. Half-half plots, as shown in shown in Figure 1.23, incorporate different plot mechanisms. The left half shows a violin plot, which outlines the underlying distribution of the data using the PDF. This is very similar to a density plot. The right half shows the original data points and give the user a visible clue about the sample size in the data size. Note that the y-position of the points is jittered to counter overplotting. Details can be found in Tiedemann (2022).\n\n\n1.7.4 Ridgeline plots\n\n\n\n\n\n\n\n\nFigure 1.24: Ridgeline plots for distributions within groups.\n\n\n\n\n\nFigure 1.24 shows so called ridgeline plots as explained in Wilke (2022). They are in essence density plots that use the y-axis to differentiate between the groups. On the x-axis the density of the underlying dataset is shown. More info on the creation of these plots and graphics is available in Wickham (2016) as well as “The R Graph Gallery – Help and Inspiration for r Charts” (2022).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#the-drive-shaft-exercise",
    "href": "chapter000_BasicConcepts.html#the-drive-shaft-exercise",
    "title": "1  Basic Concepts",
    "section": "1.8 The drive shaft exercise",
    "text": "1.8 The drive shaft exercise\n\n1.8.1 Introduction\n\n\n\n\n\n\n\n\nFigure 1.25: The drive shaft specification.\n\n\n\n\n\nA drive shaft is a mechanical component used in various vehicles and machinery to transmit rotational power or torque from an engine or motor to the wheels or other driven components. It serves as a linkage between the power source and the driven part, allowing the transfer of energy to propel the vehicle or operate the machinery.\n\nMaterial Selection: Quality steel or aluminum alloys are chosen based on the specific application and requirements.\nCutting and Machining: The selected material is cut and machined to achieve the desired shape and size. Precision machining is crucial for balance and performance.\nWelding or Assembly: Multiple sections may be welded or assembled to achieve the required length. Proper welding techniques are used to maintain structural integrity.\nBalancing: Balancing is critical to minimize vibrations and ensure smooth operation. Counterweights are added or mass distribution is adjusted.\nSurface Treatment: Drive shafts are often coated or treated for corrosion resistance and durability. Common treatments include painting, plating, or applying protective coatings.\nQuality Control: Rigorous quality control measures are employed to meet specific standards and tolerances. This includes dimensional checks, material testing, and defect inspections.\nPackaging and Distribution: Once quality control is passed, drive shafts are packaged and prepared for distribution to manufacturers of vehicles or machinery.\n\nThe end diameter of a drive shaft is primarily determined by its torque capacity, length, and material selection. It needs to be designed to handle the maximum torque while maintaining structural integrity and flexibility as required by the specific application. For efficient load transfer, there are ball bearings mounted on the end diameter. Ball bearings at the end diameter of a drive shaft support its rotation, reducing friction. They handle axial and radial loads, need lubrication for longevity, and may include seals for protection. Proper alignment and maintenance are crucial for their performance and customization is possible to match specific requirements.\nThe end diameter of the drive shaft shall be \\(\\varnothing 12\\pm0.1mm\\) (see Figure 1.25). This example will haunt us the rest of this lecture.\n\n\n1.8.2 Visualizing all the Data\n\n\n\n\n\n\n\n\n\n\n\n(a) The drive shaft data shown in a histogram.\n\n\n\n\n\n\n\n\n\n\n\n(b) The drive shaft data shown in a density plot.\n\n\n\n\n\n\n\nFigure 1.26: The raw data of the measured drive shaft diameter.\n\n\n\nFirst, some descriptive statistics of \\(N=500\\) produced drive shafts are shown in Table 1.1 (\\(\\bar{x}(sd), median(IQR)\\)). This first table does not tell us an awful lot about the sample, apart from the classic statistical measures of central tendency and spread.\n\n\n\n\nTable 1.1: The summary table of the drive shaft data\n\n\n\n\n\n\n\n\n\nVariable\nN = 5001\n\n\n\n\ndiameter\n12.17 (0.51), 12.03 (0.58)\n\n\n\n1 Mean (SD), Median (IQR)\n\n\n\n\n\n\n\n\n\n\n\nIn Figure 1.26 the data and the distribution thereof is visualized using different modalities. The complete drive shaft data is shown as a histogram (Figure 1.26 (a)) and as a density plot (Figure 1.26 (b)). A single boxplot is plotted over the histogram data in Figure 1.26 (a), providing a link to Table 1.1 (median and IQR). One important conclusion may be draw from those plots already: There may be more than one dataset hidden inside the data. We will explore this possibility further.\n\n\n1.8.3 Visualizing groups within the data\n\n\n\n\n\n\n\n\n\n\n\n(a) The groups visualized as boxplots (including the specification)\n\n\n\n\n\n\n\n\n\n\n\n(b) The groups visualized as ridgeline plots\n\n\n\n\n\n\n\nFigure 1.27: The raw data of the measured drive shaft diameter.\n\n\n\nFortunately for us, the groups that may be hidden within the data are marked in the orginal dataset and denoted as group0x. Unfortunately for us, it is not known (purely from the data) how these groups come about. Because we did get the dataset from a colleague, we need to investigate the creation of the dataset even further. This is an important point, for without knowledge about the history of the data, it is impossible or at least unadvisable to make valid statements about the data. We will go on with a table of summary statistics, see Table 1.2. Surprisingly, there are five groups hidden within the data, something we would no be able to spot from the raw data alone.\n\n\n\n\nTable 1.2: The group summary table of the drive shaft data\n\n\n\n\n\n\n\n\n\nVariable\nN = 1001\n\n\n\n\ngroup01\n12.02 (0.11), 12.02 (0.16)\n\n\ngroup02\n12.36 (0.19), 12.34 (0.25)\n\n\ngroup03\n13.00 (0.10), 13.01 (0.13)\n\n\ngroup04\n11.49 (0.09), 11.49 (0.12)\n\n\ngroup05\n12.001 (0.026), 12.000 (0.030)\n\n\n\n1 Mean (SD), Median (IQR)\n\n\n\n\n\n\n\n\n\n\n\nAgain, the table is good to have, but not as engagingi for ourself and our co-workers to look at. In order to make the data more approachable, we will use some techniques shown in Section 1.7.\nFirst in Figure 1.27 (a) the raw data points are shown as points with overlayed boxplots. On the x-axis the groups are depicted, while the Parameter of Interest (PoI) (in this case the end diameter of the drive shaft) is shown on the y-axis. Because we are interested how the manufactured drive shafts behave with respect to (wrt) the specification limit, the nominal value as well as the uppper and the lower specification limit is also shown in the plot as horizontal lines.\nIn Figure 1.27 (b) the data is shown as ridgeline density plots. On the x-axis the diameter is depiected, while the y-axis shows two types of data. First, the groups \\(1\\ldots5\\) are shown. For the individual groups, the probability is depicted as a line, therefore indicating which values are most probable in the given group. Again, because we are interested how the manufactured drive shafts behave .w.r.t the specification limit, the nominal value as well as the uppper and the lower specification limit is also shown in the plot as vertical lines.\n\n\n\n\nJohnson, Norman Lloyd. 1994. Continuous Univariate Distributions. Wiley.\n\n\nRamalho, Joao. 2021. industRial: Data, Functions and Support Materials from the Book \"industRial Data Science\". https://CRAN.R-project.org/package=industRial.\n\n\nTaboga, Marco. 2017. Lectures on Probability Theory and Mathematical Statistics - 3rd Edition. Createspace Independent Publishing Platform.\n\n\n“The R Graph Gallery – Help and Inspiration for r Charts.” 2022. https://r-graph-gallery.com/.\n\n\nTiedemann, Frederik. 2022. Gghalves: Compose Half-Half Plots Using Your Favourite Geoms. https://CRAN.R-project.org/package=gghalves.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWilke, Claus O. 2022. Ggridges: Ridgeline Plots in ’Ggplot2’. https://CRAN.R-project.org/package=ggridges.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter000_BasicConcepts.html#footnotes",
    "href": "chapter000_BasicConcepts.html#footnotes",
    "title": "1  Basic Concepts",
    "section": "",
    "text": "Sir Francis Galton (1822-1911): Influential English scientist, notable for his contributions to statistics and genetics.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html",
    "href": "chapter001_Distributions.html",
    "title": "2  Statistical Distributions",
    "section": "",
    "text": "2.1 Why distributions?\nFigure 2.1: The model is perfect, but the real world is not.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#why-distributions",
    "href": "chapter001_Distributions.html#why-distributions",
    "title": "2  Statistical Distributions",
    "section": "",
    "text": "Distributions are models of the reality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#types-of-data",
    "href": "chapter001_Distributions.html#types-of-data",
    "title": "2  Statistical Distributions",
    "section": "2.2 Types of data",
    "text": "2.2 Types of data\n\n\n\n\n\n\n\n\n\nFigure 2.2: Data can be classified as different types.\n\n\n\n\n\n\n\nNominal Data:\n\nDescription: Nominal data represents categories with no inherent order or ranking.\nExamples: Colors, gender, or types of fruits.\nCharacteristics: Categories are distinct, but there is no meaningful numerical value associated.\n\nOrdinal Data:\n\nDescription: Ordinal data has categories with a meaningful order or ranking, but the intervals between them are not consistent or measurable.\nExamples: Educational levels (e.g., high school, bachelor’s, master’s), customer satisfaction ratings (e.g., low, medium, high).\nCharacteristics: The order is significant, but the differences between categories are not precisely quantifiable.\n\nDiscrete Data:\n\nDescription: Discrete data consists of separate, distinct values, often counted in whole numbers and with no intermediate values between them.\nExamples: Number of students in a class, number of cars in a parking lot.\nCharacteristics: The data points are distinct and separate; they do not have infinite possible values within a given range.\n\nContinuous Data:\n\nDescription: Continuous data can take any value within a given range and can be measured with precision.\nExamples: Height, weight, temperature.\nCharacteristics: Values can be any real number within a range, and there are theoretically infinite possible values within that range.\n\n\n\n2.2.1 Nominal Data\n\n\n\n\n\n\n\n\nFigure 2.3: Some example for nominal data.\n\n\n\n\n\nNominal data is a type of data that represents categories or labels without any specific order or ranking. These categories are distinct and non-numeric. For example, colors, types of fruits, or gender (male, female, other) are nominal data. Nominal data can be used for classification and grouping, but mathematical operations like addition or subtraction do not make sense in this context.\n\n\n2.2.2 Ordinal Data\n\n\n\n\n\n\n\n\nFigure 2.4: Some example for ordinal data.\n\n\n\n\n\nOrdinal data represents categories that have a specific order or ranking. While the categories themselves may not have a consistent numeric difference between them, they can be arranged in a meaningful sequence. A common example of ordinal data is survey responses with options like “strongly agree,” “agree,” “neutral,” “disagree,” and “strongly disagree.” These categories indicate a level of agreement, but the differences between them may not be uniform or measurable.\n\n\n2.2.3 Discrete Data\n\n\n\n\n\n\n\n\nFigure 2.5: Some example for discrete data.\n\n\n\n\n\nDiscrete data consists of distinct, separate values that can be counted and usually come in whole numbers. These values can be finite or infinite, but they are not continuous. Examples include the number of students in a class, the count of cars in a parking lot, or the quantity of books in a library. Discrete data is often used in counting and can be represented as integers.\nOne quote in the literature about discrete data, shows how difficult the classification of data types can become (J. Bibby (1980)): “… All actual sample spaces are discrete, and all observable random variables have discrete distributions. The continuous distribution is a mathematical construction, suitable for mathematical treatment, but not practically observable. …”\n\n\n2.2.4 Continous Data\n\n\n\n\n\n\n\n\nFigure 2.6: Some example for continous data.\n\n\n\n\n\nContinuous data encompasses a wide range of values within a given interval and can take on any real number. There are infinite possibilities between any two points in a continuous dataset, making it suitable for measurements with high precision. Examples of continuous data include temperature, height, weight, and time. It is important to note that continuous data can be measured with decimals or fractions and is not limited to whole numbers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#uniform-distribution",
    "href": "chapter001_Distributions.html#uniform-distribution",
    "title": "2  Statistical Distributions",
    "section": "2.3 Uniform Distribution",
    "text": "2.3 Uniform Distribution\n\n2.3.1 Dice\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: The probability for every face to turn up during a roll is uniformly distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.8: A plot of the probabilities that every face of a die turns up. They are all the same.\n\n\n\n\n\n\n\n\n\n2.3.2 Types of uniform distributions\n\nAll outcomes in the range are equally probable\nThe “range” can be:\n\na finite set of values (discrete)\na continuous interval (continuous)\n\n\n\\[\\begin{align}\nX \\sim \\mathrm{Uniform}(a,b) \\text{ continuous}\\\\\nX \\sim \\mathrm{Uniform}\\{a,a+1,\\ldots,b\\} \\text{ discrete}\n\\end{align}\\]\n\n\n2.3.3 Visual comparison of discrete and continuous\n\n\n\n\n\n\n\n\nFigure 2.9: A comparison of continnuous and discrete uniformly distributed data.\n\n\n\n\n\n\n\n2.3.4 Core properties\n\n\n\n\nTable 2.1: The core properties between the discrete and continuous disitrbution are very similar.\n\n\n\n\n\n\n\n\n\nCore Properties: Discrete vs. Continuous Uniform Distributions\n\n\nEqual probability everywhere, but for different types of data\n\n\nProperty\nDiscrete Uniform\nContinuous Uniform\n\n\n\n\nProbability\nEqual for each outcome.\nEqual density over the interval.\n\n\nSymmetry\nMean = midpoint of the range.\nMean = midpoint of the interval.\n\n\nIntuition\n‘Fair die’ with n sides.\n‘Fair spinner’ on a line segment.\n\n\nUse Cases\nCounting problems (e.g., dice, cards).\nMeasuring problems (e.g., time, space).\n\n\n\nNote: Both distributions share the ‘fairness’ property but apply to different data types.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#section",
    "href": "chapter001_Distributions.html#section",
    "title": "2  Statistical Distributions",
    "section": "2.4 Probability Mass Function (PMF)",
    "text": "2.4 Probability Mass Function (PMF)\n\n2.4.1 But why?\n\nPMFs provide a way to calculate and assign probabilities to each distinct outcome.\nEach outcome is assigned a probability that corresponds to its position in the total number of outcomes.\n\n\n\n2.4.2 Basics\n\n… assigns each outcome the same probability that always sum up to \\(1\\) (example: six-sided die)\n\n\n\\(X \\sim \\mathrm{Uniform}\\{a,b\\}\\) where \\(a\\) and \\(b\\) are integers and \\(a\\leq b\\)\n\\[\\begin{align}\nf(k) = P (X = k)\n\\end{align}\\]\n\n\n\n\\(k\\) meaning a specific value that \\(X\\) can take (e.g., \\(k = 0,1,2, ...\\))\n\\(f(k)\\) is the probability that \\(X\\) equals \\(k\\)\n\n\n\n\n2.4.3 Key Properties\n\n\\(0 \\leq f(k) \\leq 1\\) for all \\(k\\)\nThe of probabilities over all possible \\(k\\) must equal \\(1\\)\n\n\\[\\begin{align}\n\\sum_{all \\; k}f(k) = 1\n\\end{align}\\]\n\n\n2.4.4 6-sided die\n\n\\[\\begin{align}\nX &\\sim \\mathrm{Uniform}\\{a,a+1, ..., b\\} \\\\\nX &\\sim \\mathrm{Uniform}\\{1,2,3,4,5,6\\}\n\\end{align}\\]\n\n\nNumber of outcomes:\n\\[\\begin{align}\nN &= b-a+1\\\\\nN &= 6-1+1=6\n\\end{align}\\]\n\n\nEqual probability for each outcome: \\(f(k) = P (X = k)\\)\n\\[\\begin{align}\nf(a) = f(a+1)= ... = f(b) = c \\text{ (some constant)}\n\\end{align}\\]\n\n\nSum of all probabilities must equal 1\n\\[\\begin{align}\n\\sum_{k = a}^b f(k) = \\underbrace{c+c+ ... +}_{N\\text{times}} = N \\cdot c = 1\n\\end{align}\\]\n\nSolving for \\(c\\)\n\\[\\begin{align}\nc = \\frac{1}{N} = \\frac{1}{b-a+1}\n\\end{align}\\]\n\n\n\nFormal PMF:\n\\[\\begin{align}\nf(k) = \\begin{cases}\n\\frac{1}{b - a + 1} & \\text{if } k \\in \\{a, a+1, \\dots, b\\}, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\end{align}\\]\n\n\n\n2.4.5 Exercise\nA tool changer in production randomly selects one of four tools (labeled 1 to 4) with equal probability.\n\nDescribe the formal set in \\(X \\sim ...\\)\n\n\n\\(X \\sim \\mathrm{Uniform}\\{1,2,3,4\\}\\)\n\n\nFormally calculate the number of outcomes (\\(N\\))\n\n\n\\(N = 4-1+1 = 4\\)\n\n\n\nDescribe the PMF formally\n\n\n\\[\\begin{align}\nf(k) = \\begin{cases}\n\\frac{1}{4} & \\text{if } k \\in \\{1,2,3,4\\}, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\end{align}\\]\n\n\nWhat is the probability to select tool \\(2\\)\n\n\n\\(P(X = 2) = f(2) = \\frac{1}{4} = 0.25\\)\n\n\n\n2.4.6 Summary PMF\n\nThe PMF describes the probability distribution of a discrete random variable.\nThe probabilities associated with all hypothetical values must be non-negative and sum up to 1.\nA PMF is specific to discrete random variables, while a Probability Density Function (PDF) is associated with continuous random variables.\nUnlike a PDF, which requires integration over an interval, the PMF directly provides probabilities for individual values.\n“mass” is conserved (similar to how physical mass is conserved).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#binomial-distribution",
    "href": "chapter001_Distributions.html#binomial-distribution",
    "title": "2  Statistical Distributions",
    "section": "2.5 Binomial Distribution",
    "text": "2.5 Binomial Distribution\n\n2.5.1 Classroom example\n\n\nIf we roll a fair six-sided die 10 times, how many times do you expect to get a specific outcome?\n\n10 trials\n\\(P?\\) to roll a \\(6\\) (expected value)?\nrecord the number of times a specific outcome occurs\nrepeat for 2-4 students\n\n\n\n\n\\(P = 1/6 \\approx 16\\% \\rightarrow P_{10\\;times} = 10 * 1/6 = 1.6 times \\approx 2\\)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.10: Simulated die rolls (The die is rolle \\(10\\) times, the experiment is repeated \\(5\\) times)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: Is the outcome likely?\n\n\n\n\n\n\n\n\n\n2.5.2 Theory\n\n\n\n\n\n\n\n\nFigure 2.12: The binomial distribution\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. A Bernoulli trial, named after Swiss mathematician Jacob Bernoulli1, is a random experiment or trial with two possible outcomes: success and failure. These outcomes are typically labeled as \\(1\\) for success and \\(0\\) for failure. The key characteristics of a Bernoulli trial are:\n\nTwo Outcomes: There are only two possible outcomes in each trial, and they are mutually exclusive. For example, in a coin toss, the outcomes could be heads (success, represented as \\(1\\)) or tails (failure, represented as \\(0\\)).\nConstant Probability: The probability of success remains the same for each trial. This means that the likelihood of success and failure is consistent from one trial to the next.\nIndependence: Each trial is independent of others, meaning that the outcome of one trial does not influence the outcome of subsequent trials. For instance, the result of one coin toss doesn’t affect the result of the next coin toss.\n\nExamples of Bernoulli trials include:\n\nFlipping a coin (heads as success, tails as failure).\nRolling a die and checking if a specific number appears (the number as success, others as failure).\nTesting whether a manufactured product is defective or non-defective (defective as success, non-defective as failure).\n\nThe Bernoulli trial is the fundamental building block for many other probability distributions, including the binomial distribution, which models the number of successes in a fixed number of Bernoulli trials.\nThe PMF, also known as the discrete probability density function, is a fundamental concept in probability and statistics.\n\nDefinition: The PMF describes the probability distribution of a discrete random variable. It gives the probability that the random variable takes on a specific value. In other words, the PMF assigns probabilities to each possible outcome of the random variable.\nFormal Representation: For a discrete random variable X, the PMF is denoted as \\(P(X = x)\\), where \\(x\\) represents a specific value. Mathematically, the PMF is defined as: \\(P(X = x) = \\text{{probability that }} X \\text{{ takes the value }} x\\)\nProperties: The probabilities associated with all hypothetical values must be non-negative and sum up to 1. Thinking of probability as “mass” helps avoid mistakes, as the total probability for all possible outcomes is conserved (similar to how physical mass is conserved).\nComparison with PDF: A PMF is specific to discrete random variables, while a PDF is associated with continuous random variables. Unlike a PDF, which requires integration over an interval, the PMF directly provides probabilities for individual values.\nMode: The value of the random variable with the largest probability mass is called the mode.\nMeasure-Theoretic Formulation: The PMF can be seen as a special case of more general measure-theoretic constructions. It relates to the distribution of a random variable and the probability density function with respect to the counting measure.\n\nThe PMF for the binomial distribution is given in \\(\\eqref{PMFbinom}\\).\n\\[\\begin{align}\nP(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\label{PMFbinom}\n\\end{align}\\]\n\n\n2.5.3 The drive shaft exercise - Binomial Distribution\nIn the context of a drive shaft, you can think of it as a model for the number of defective drive shafts in a production batch. Each drive shaft is either good (success) or defective (failure).\nLet’s say you have a batch of 100 drive shafts, and the probability of any single drive shaft being defective is \\(0.05 (5\\%)\\). You want to find the probability of having a certain number of defective drive shafts in this batch.\n\n\n\n\n\n\n\n\nFigure 2.13: The binomial disitribution and the drive shaft exercise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#the-normal-distribution",
    "href": "chapter001_Distributions.html#the-normal-distribution",
    "title": "2  Statistical Distributions",
    "section": "2.6 The Normal Distribution",
    "text": "2.6 The Normal Distribution\n\n\n\n\n\n\n\n\nFigure 2.14: The standarized normal distribution\n\n\n\n\n\nThe normal distribution is a fundamental statistical concept that holds immense significance in the realms of engineering and production. It is often referred to as the Gaussian distribution or the bell curve, is a mathematical model that describes the distribution of data in various natural and human-made phenomena, see Johnson (1994). It forms a symmetrical curve when plotted, is centered around a mean (\\(\\mu_0\\)) and balanced on both sides (Figure 2.14). The spread or dispersion of the data points is characterized by \\(\\sigma_0^2\\). Those two parameters completely define the normal distribution. A remarkable property of the normal distribution is the empirical rule, which states that approximately \\(68\\%\\) of the data falls within one standard deviation from the mean, \\(95\\%\\) falls within two standard deviations, and \\(99.7\\%\\) falls within three standard deviations (Figure 2.14). The existence of the normal distribution in the real world is a result of the combination of several factors, including the principles of statistics and probability, the Central Limit Theorem (CLT), and the behavior of random processes in nature and society.\n\n2.6.1 Emergence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.1.1 The math behind\n\nAfter \\(n\\) rows a balls final position is the sum of \\(n\\) independent steps: \\(S = x_1 + x_2 + \\ldots + x_n\\) where \\(x_i = +1 \\text{( right) or}  -1 \\text{(left)}\\)\n\\(\\text{Number of paths to } k = \\binom{n}{(n + k)/2}\\)\n\n\n\n\n\n\n\n\n\nFigure 2.15: The number of ways a ball can take\n\n\n\n\n\n\n\n2.6.1.2 Approximation with a smooth curve\n\nFor large \\(n\\), the binomial distribution looks like a bell curve. It can be approximated using the Stirling approximation\n\n\n\n\n\n\n\n\n\nFigure 2.16: The normal distribution can be approximated using a continous curve\n\n\n\n\n\n\n\n2.6.1.3 Binomial Probability\n\nProbability of ending at position \\(k\\): \\(P(S=k)\\binom{n}{(n + k)/2}(\\frac{1}{2})^n\\)\nFor large \\(n\\) we can approximate the binomial coefficient: \\(\\binom{n}{m} \\approx \\frac{n^n}{m^m(n-m)^{n-m}}\\sqrt{\\frac{n}{2\\pi m (n-m)}}\\) where \\(m = (n+k)/2\\)\n\n\n\n2.6.1.4 Large \\(n\\)\n\nLet \\(k = x\\) (treat as continuous) and \\(n\\) be large\nAfter simplification the PMF becomes: \\(P(S=x) \\approx \\frac{1}{\\sqrt{2\\pi n}}e^{\\frac{-x^2}{2n}}\\)\nReplace \\(n\\) with \\(\\sigma^2\\) and allow for mean \\(\\mu\\): \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n\n\n2.6.1.5 The Normal PDF Equation\n\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\): Scaling factor to ensure Total Probability = 1\n\\(e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\): The “bell” shape - exponential decay based on the distance to the mean\n\n\nSymmetry: The term \\((x-\\mu)^2\\) ensures the curve is symmetric around \\(\\mu\\)\nPeak at mean: The exponent is zero when \\(x = \\mu\\), giving the maximum value\nThickness of Tails: \\(\\sigma\\) controls how spread out the curve is\n\n\n\n2.6.1.6 Parameter influence\n\n\n\n\n\n\n\n\nFigure 2.17: The influence of distributional parameters\n\n\n\n\n\n\n\n\n2.6.2 The Buffon needle problem\n\n\\(P\\) for crossing a line?\n\n\n\n\n\n\n\n\n\nFigure 2.18: An Illustration of the Buffon Needle Problem.\n\n\n\n\n\n\n2.6.2.1 Sample Size\n\n\n\n\n\n\n\n\nFigure 2.19: Counting needles leads to an esimtate of \\(\\pi\\).\n\n\n\n\n\n\n\n2.6.2.2 Connection to the Normal distribution\n\nWhy?\n\n\nGaussian integral\n\n\n\n2.6.2.3 \\(\\pi\\) in the Normal Distribution\n\\[\\begin{align}\nI = \\int_{-\\infty}^{\\infty}e^{-kx^2}\\,dx\\\\\nI^2 = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}e^{-k(x^2+y^2)}\\,dx\\,dy =  \\int_{0}^{2\\pi}\\int_{0}^{\\infty}e^{-k^2}r\\,dr\\,d\\theta = \\frac{\\pi}{k}\n\\end{align}\\]\n\n\\(e^{-k(x^2+y^2)}=e^{-kr^2}\\) is radially symmetric, which simplifies the integral\n\\(\\int_0^{2\\pi}\\,d\\theta = 2\\pi\\) introduces \\(\\pi\\) in the normalization constant of the normal distribution\n\n\n\n2.6.2.4 \\(\\pi\\) in the Buffon Needle Problem\n\nA needle of length \\(L\\) is dropped onto a plane with parallel lines spaced \\(D \\geq L\\) apart\nProbability \\(P\\) that the needle crosses a line is \\(\\pi\\approx \\frac{2l}{D}\\frac{\\text{Hits}}{N\\text{ total number}}\\)\nThe needles orientation \\(\\theta\\) is uniformly distributed in \\([0,\\pi/2]\\)\nThe crossing condition depends on \\(\\sin{\\theta}\\), and integrating over \\(\\theta\\) introduces \\(\\pi\\) through the average value of \\(\\theta\\) over its domain\n\n\n\n2.6.2.5 What they share\n\nBoth problems translate a probabilistic question into a geometric one, where \\(\\pi\\) emerges naturally from integrating over angles or symmetric regions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#z---standardization",
    "href": "chapter001_Distributions.html#z---standardization",
    "title": "2  Statistical Distributions",
    "section": "2.7 Z - Standardization",
    "text": "2.7 Z - Standardization\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: How can we compare this data?\n\n\n\n\n\n\n\nThe Standard Score / Z-Score (Z)-standardization, also known as Z or Z, is a common statistical technique used to transform data into a standard normal distribution with a mean of \\(0\\) and a standard deviation of \\(1\\) (Taboga 2017). This transformation is useful for comparing and analyzing data that have different scales and units \\(\\eqref{zscore}\\).\n\\[\\begin{align}\nZ = \\frac{x_i - \\bar{x}}{sd} \\label{zscore}\n\\end{align}\\]\nHow the Z can be applied is shown in Figure 2.20 and Figure 2.21. The data for group X and group Y may be measured in different units ( Figure 2.20). To answer the question, which of the values \\(x_i (i=1\\ldots5)\\) is more probable, the single data points are transformed to the respective z-score using \\(\\eqref{zscore}\\). In Figure 2.21, the Z for both groups are plotted against each other. The perfect correlation of the datapoints shows, that for every \\(x_i\\) the same probability applies. Thus, the datapoints are comparable.\n\n2.7.1 Properties of of Z-scores\n\nShape: The distribution’s shape remains normal\nRelative Positions: Values maintain their percentiles\nOutliers: Extreme values \\(|z|&gt;3\\) are easily flagged\n\n\n\n2.7.2 Comparison of standardized data\n\n\n\n\n\n\n\n\nFigure 2.21: Normalized data is easier to compare\n\n\n\n\n\n\n\n2.7.3 The Universal Yardstick\nThe standard normal distribution (\\(N(0,1)\\)) is the reference distribution (the perfect model).\n\n\\(\\bar{x} = 0,\\; sd = 1\\)\nEmpirical Rule (68-65-99.7)\n\n\\(68\\%\\) of data within \\(\\pm1sd\\)\n\\(95\\%\\) of data within \\(\\pm2sd\\)\n\\(99.7\\%\\) of data within \\(\\pm3sd\\)\n\n\n\n\n2.7.4 The drive shaft exercise - Z-Standardization\n\n\n\n\n\n\n\n\nFigure 2.22: The standardized data of the drive shaft data.\n\n\n\n\n\nIn Figure 2.22 the standardized drive shaft data is shown. The mean of the data (\\(\\bar{x}\\)) is now centered at \\(0\\) and the standard deviation is \\(1\\). For this case, the specification limits have also been transferred to the respective Z-score (even though they can not be interpreted as such anymore). For every \\(x_i\\) the probability to be within a normal distribution is now known. When comparing this to the transferred specification limits, it is clear to see that for group01 “most” of the data points are within the limits in contrast to group03 where none of the data points lies within the specification limits. When looking at group03 we see, that the nominal specification limit is -9.78 standard deviations away from the centered mean of the datapoints. The probability of a data point being located there is 6.8605273^{-23} which does not sound an awful lot. We will dwelve more into such investigation in another chapter, but this is a first step in the direction of inferential statistics.\n\n\n2.7.5 When Z-Scores Go Wrong\n\nNon-Normal Data:\n\n\nZ-Scores assume normality\n\n\nPopulation vs. Sample\n\n\nUse population parameters (\\(\\mu,\\sigma\\)) if known, otherwise use sample estimates (\\(\\bar{x},sd\\))\n\n\nOutliers:\n\n\nZ-scores are sensitive to outliers\n\n\n\n2.7.6 The Z-transform and the Galton Board\n\n\n\n\n\n\n\n\n\n\n2.7.6.1 Applying the Z-transform\n\\[Z = \\frac{X-\\mu}{\\sigma}\\]\n\n\\[Z = \\frac{X-\\frac{n}{2}}{\\frac{\\sqrt{n}}{2}}\\]\n\n\n\\[\\lim_{n\\to\\infty} P(a\\leq Z \\leq b)= \\int_a^b \\frac{1}{\\sqrt{2\\pi}}e^\\frac{-z^2}{2} \\,dz \\]\n\n\n\n2.7.6.2 Converting the bionmial Formula to a Normal Form\nStirling appoximation: \\(n!\\approx\\sqrt{2\\pi n} \\left( \\frac{n}{e} \\right)^n\\)\nAppprox: \\(\\binom{n}{k} \\approx \\frac{\\sqrt{2\\pi n} \\left( \\frac{n}{e} \\right)^n}{\\sqrt{2\\pi n} \\left( \\frac{k}{e} \\right)^k \\cdot \\sqrt{2\\pi(n-k)} \\left( \\frac{n-k}{e} \\right)^{n-k}}\\)\nsimplifies to: \\(\\binom{n}{k} = \\frac{1}{\\sqrt{2\\pi n p (1-p)}}e^{-\\frac{(k-np)^2}{2np(1-p)}}\\)\nsubstituting \\(p=0.5\\): \\(P(X = k) \\approx \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(k-\\mu)^2}{2\\sigma^2}}\\)\n\nWhich is the PDF\n\n\n\n\n2.7.7 The drive shaft exercise - Normal Distribution\n\n\n\n\n\n\n\n\nFigure 2.23: The drive shaft data with the respective normal distributions.\n\n\n\n\n\nIn Figure 2.23 the drive shaft data is shown for each group in a histogram. As an overlay, the respective normal distribution (with the groups \\(\\bar{x},sd\\)) is overlayed. If the data is normally distributed, is a different question.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#section-1",
    "href": "chapter001_Distributions.html#section-1",
    "title": "2  Statistical Distributions",
    "section": "2.8 PDF",
    "text": "2.8 PDF\n\n\n\n\n\n\n\n\nFigure 2.24: A visual represenstation of the PDF for the normal distribution.\n\n\n\n\n\n\\[\\begin{align}\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n\\end{align}\\]\nA PDF is a mathematical function that describes the likelihood of a continuous random variable taking on a particular value. Unlike discrete probability distributions, which assign probabilities to specific values of a discrete random variable, a PDF describes the relative likelihood of the variable falling within a particular range of values. The total area under the curve of a PDF over its entire range is equal to 1, indicating that the variable must take on some value within that range. In other words, the integral of the PDF over its entire domain equals 1. The probability of a continuous random variable falling within a specific interval is given by the integral of the PDF over that interval.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#section-2",
    "href": "chapter001_Distributions.html#section-2",
    "title": "2  Statistical Distributions",
    "section": "2.9 Cumulative Density Function (CDF)",
    "text": "2.9 Cumulative Density Function (CDF)\n\n\n\n\n\n\n\n\nFigure 2.25: A visual represenstation of the CDF for the normal distribution.\n\n\n\n\n\nA cumulative density function (CDF), also known as a cumulative distribution function, describes the probability that a random variable will take on a value less than or equal to a given point. It is the integral of the PDF from negative infinity to a certain value. The CDF provides a comprehensive view of the probability distribution of a random variable by showing how the probability accumulates as the value of the random variable increases. Unlike the PDF, which gives the probability density at a particular point, the CDF gives the cumulative probability up to that point.\n\\[\\begin{align}\nz &= \\frac{x-\\mu}{\\sigma} \\nonumber \\\\\n\\varphi(x) &= \\frac{1}{2\\pi}e^{\\frac{-z^2}{2}} \\\\\n\\phi(x)& = \\int \\frac{1}{2\\pi}e^{\\frac{-x^2}{2}} \\, dx \\\\\n\\lim_{x\\to\\infty} \\phi(x) &= 1 \\nonumber \\\\\n\\lim_{x\\to - \\infty} \\phi(x) &= 0 \\nonumber\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#likelihood-and-probability",
    "href": "chapter001_Distributions.html#likelihood-and-probability",
    "title": "2  Statistical Distributions",
    "section": "2.10 Likelihood and Probability",
    "text": "2.10 Likelihood and Probability\n\n\n\n\n\n\n\n\nFigure 2.26: The subtle difference between likelihood and probability.\n\n\n\n\n\n\nLikelihood\n\nrefers to the chance or plausibility of a particular event occurring given certain evidence or assumptions. It is often used in statistical inference, where it indicates how well a particular set of parameters (or hypotheses) explain the observed data. Likelihood is a measure of how compatible the observed data are with a specific hypothesis or model.\n\nProbability\n\nrepresents the measure of the likelihood that an event will occur. It is a quantification of uncertainty and ranges from \\(0\\) (indicating impossibility) to \\(1\\) (indicating certainty). Probability is commonly used to assess the chances of different outcomes in various scenarios.\n\n\nIn summary, while both likelihood and probability deal with the chance of events occurring, likelihood is often used in the context of comparing different hypotheses or models based on observed data, while probability is more broadly used to quantify the chances of events happening in general.\n\n2.10.1 Exercise\n\n\nExercise Data\n\n\nMachine\nSample 1\nSample 2\nSample 3\nSample 4\nSample 5\n\n\n\n\nMachine X\n44\n46\n45\n47\n43\n\n\nMachine Y\n52\n50\n53\n49\n51\n\n\nMachine Z\n40\n42\n39\n41\n43\n\n\n\n\n\n\n\nTable 2.2: Exercise Data\n\n\n\n\n\n\n\n\n\nmachine_x\nmachine_y\nmachine_z\n\n\n\n\n44\n52\n40\n\n\n46\n50\n42\n\n\n45\n53\n39\n\n\n47\n49\n41\n\n\n43\n51\n43\n\n\n\n\n\n\n\n\n\n\n\ncalculate \\(\\bar{x}\\) and \\(sd\\)\nCompute z-scores\nFlag anomalies\nRecommend actions\n\n\n\n\n\n\n\nTable 2.3: Results\n\n\n\n\n\n\n\n\n\nmachine\nmean_cycle_time\nsd_cycle_time\nvar_cycle_time\n\n\n\n\nmachine_x\n45\n1.581139\n2.5\n\n\nmachine_y\n51\n1.581139\n2.5\n\n\nmachine_z\n41\n1.581139\n2.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4: Results for z score\n\n\n\n\n\n\n\n\n\ncycle_time_s\nscaled_data\n\n\n\n\nmachine_x\n\n\n44\n-0.6324555\n\n\n46\n0.6324555\n\n\n45\n0.0000000\n\n\n47\n1.2649111\n\n\n43\n-1.2649111\n\n\nmachine_y\n\n\n52\n0.6324555\n\n\n50\n-0.6324555\n\n\n53\n1.2649111\n\n\n49\n-1.2649111\n\n\n51\n0.0000000\n\n\nmachine_z\n\n\n40\n-0.6324555\n\n\n42\n0.6324555\n\n\n39\n-1.2649111\n\n\n41\n0.0000000\n\n\n43\n1.2649111",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#chi2---distribution",
    "href": "chapter001_Distributions.html#chi2---distribution",
    "title": "2  Statistical Distributions",
    "section": "2.11 Chi2 - Distribution",
    "text": "2.11 Chi2 - Distribution\n\n\n\n\n\n\n\n\n\n\n\n\n(a) a normal distribution\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) the standard normal variable square (\\(dof = 1\\))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) the \\(\\chi^2\\) distributions with varying degrees of freedom\n\n\n\n\n\n\n\nFigure 2.27: What a \\(\\chi^2\\) distribution reprepresents and how it relates to a the normal distribution.\n\n\n\n\nThe \\(\\chi^2\\) distribution is a continuous probability distribution that is widely used in statistics (Taboga 2017). It is often used to test hypotheses about the independence of categorical variables.\n\\[\\begin{align}\n\\chi^2 = \\sum_{k = 1}^n \\frac{(O_k - E_k)^2}{E_k}\n\\end{align}\\]\nThe connection between the chi-squared distribution and sample variance holds significant importance in statistics.\n\nDistribution of Sample Variance: When calculating the sample variance from a dataset, it follows a chi-squared distribution. Specifically, for a random sample from a normally distributed population with mean \\(\\mu_0\\) and variance \\(\\sigma_0^2\\), the sample variance (adjusted for bias) divided by \\(\\sigma_0^2\\) follows a \\(\\chi^2\\) distribution with \\(n-1\\) degree of freedom (dof), where \\(n\\) is the sample size.\nHypothesis Testing: In statistical analysis, hypothesis testing is a common technique for making inferences about populations using sample data. The \\(\\chi^2\\) distribution plays a crucial role in hypothesis testing, especially when comparing variances between samples.\n\n\\(\\chi^2\\) Test for Variance: The \\(\\chi^2\\) distribution is used to test whether the variance of a sample matches a hypothesized variance. This is applicable in various scenarios, such as quality control, to assess the consistency of a manufacturing process.\n\nConfidence Intervals: When estimating population parameters like population variance, it’s essential to establish confidence intervals. The \\(\\chi^2\\) distribution aids in constructing these intervals, allowing researchers to quantify the uncertainty associated with their parameter estimates.\nModel Assessment: In regression analysis, the \\(\\chi^2\\) distribution is related to the F-statistic, which assesses the overall significance of a regression model. It helps determine whether the regression model is a good fit for the data.\n\nIn summary, the link between the chi-squared distribution and sample variance is fundamental in statistical analysis. It empowers statisticians and analysts to make informed decisions about population parameters based on sample data and evaluate the validity of statistical models. Understanding this relationship is essential for those working with data and conducting statistical investigations.\n\n2.11.1 The drive shaft exercise - Chi2 Distribution\n\n\n\n\n\n\n\n\nFigure 2.28: The \\(\\chi^2\\) disitribution of the drive shaft data.\n\n\n\n\n\nIn Figure 2.28 the squared standad deviation for every datapoint (from the stanardized data) is shown as a histogram for every group with an overlayed (and scaled) density plot. In the background of every group the theoretical \\(\\chi^2\\)-distribution with \\(dof = 1\\) is plotted to visually compare the empirical distribution of the datapoints to the theorectial.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#t---distribution",
    "href": "chapter001_Distributions.html#t---distribution",
    "title": "2  Statistical Distributions",
    "section": "2.12 t - Distribution",
    "text": "2.12 t - Distribution\n\n\n\n\n\n\n\n\nFigure 2.29: PDF of t-distribution with varying \\(dof\\)\n\n\n\n\n\nThe t-distribution, also known as the Student’s t-distribution (Student 1908), is a probability distribution that plays a significant role in statistics2. It is a symmetric distribution with a bell-shaped curve, similar to the normal distribution, but with heavier tails. The key significance of the t-distribution lies in its application to inferential statistics, particularly in hypothesis testing and confidence interval estimation.\n\nSmall Sample Sizes: When dealing with small sample sizes (typically less than 30), the t-distribution is used to make inferences about population parameters, such as the mean. This is crucial because the normal distribution assumptions are often violated with small samples.\nAccounting for Variability: The t-distribution accounts for the variability inherent in small samples. It provides wider confidence intervals and more conservative hypothesis tests compared to the normal distribution, making it more suitable for situations where sample size is limited.\nDegrees of Freedom: The shape of the t-distribution is determined by a parameter called dof. As the dof increases, the t-distribution approaches the normal distribution. When df is small, the tails of the t-distribution are fatter, allowing for greater uncertainty in estimates.\n\nStatisticians found that if they took samples of a constant size from a normal population, computed a statistic called a t-score for each sample, and put those into a relative frequency distribution, the distribution would be the same for samples of the same size drawn from any normal population. The shape of this sampling distribution of t’s varies somewhat as sample size varies, but for any \\(n\\), it is always the same. For example, for samples of \\(5\\), \\(90\\%\\) of the samples have t-scores between \\(-1.943\\) and \\(+1.943\\), while for samples of \\(15\\), \\(90\\%\\) have t-scores between \\(\\pm 1.761\\). The bigger the samples, the narrower the range of scores that covers any particular proportion of the samples \\(\\eqref{tscore}\\) (Note the similarity to \\(\\eqref{zscore}\\)). Since the t-score is computed for every \\(x_i\\) the resulting sampling distribution is called the t-disitribution.\n\\[\\begin{align}\nt_i = \\frac{x_i - \\mu_o}{sd/\\sqrt{n}} \\label{tscore}\n\\end{align}\\]\nIn Figure 2.29 it is shown, that with increasing dof (in this case sample size), the t-distribution approximates a normal distribution (gray area). Figure 2.29 also shows an example of the t-distribution in action. Of all possible samples with 9 \\(dof\\) \\(0.025\\;(2\\frac{1}{2}\\%)\\) of those samples would have t-scores greater than \\(2.262\\), and \\(.975\\;(97.5\\%)\\) would have t-scores less than \\(2.262\\). The advantage of the t-score and t-distribution is clearly visible. All these values can be computed from sampled data, the population can remain estimated \\(\\eqref{tscore}\\).\n\n2.12.1 The drive shaft exercise - t-Distribution\nThe t-score computation and the z-standardization look very familiar. While the z-score calculation needs some population parameters, the t-score calculation does not need such. It therefore allows us, to estimate population parameters based on a sample - a very frequent use case in statistics.\nSuppose we have some data (maybe the drive shaft exercise?) with which calculations can be done. First, the mean \\(\\bar{x}\\) and \\(sd\\) is calculated according to \\(\\eqref{mean}\\) and \\(\\eqref{sd}\\). After this, the confidence level (cl) (we will get to this later in more detail) is specified. A value of \\(95\\%\\) is a common choice of cl.\n\\[\\begin{align}\nci &= 0.95 \\quad \\text{(for a 95\\% confidence level)}\n\\end{align}\\]\nThen the Standard Error (SE) is calculated using \\(\\eqref{se}\\), which takes the \\(sd\\) and \\(n\\) of a sample into account (notice, how we did not use any population estimation?).\n\\[\\begin{align}\nSE &= \\frac{sd}{\\sqrt{n}} \\label{se}\n\\end{align}\\]\nIn the next step, the critical t-score is calculated using the cl as shown in \\(\\eqref{tscore}\\). qt in this case returns the value of the inverse CDF of the t-distribution given a certain random variable (or datapoint \\(x_i\\)) and \\(n-1\\) dof. Think of it as an automated look up in long statistical tables.\n\\[\\begin{align}\n% Step 5: Calculate the T-score\nt_{score} &= qt\\left(\\frac{1 - ci}{2}, df = n - 1\\right) \\label{tscore}\n\\end{align}\\]\nWith this, the margin of error can be calculated using the SE and the t-score as shown in \\(\\eqref{errormargin}\\).\n\\[\\begin{align}\n% Step 6: Calculate the margin of error\nmargin\\;of\\;error &= t_{score} \\times SE \\label{errormargin}\n\\end{align}\\]\nIn the last step the Confidence Interval is calculated for the lower and the upper bound with \\(\\eqref{cilobound}\\) and \\(\\eqref{cihibound}\\).\n\\[\\begin{align}\n% Step 7: Determine the confidence interval\nlo &= \\bar{x} - margin\\;of\\;error \\label{cilobound} \\\\\nhi &= \\bar{x} + margin\\;of\\;error \\label{cihibound}\n\\end{align}\\]\nIt all looks and feels very similar to using the normal disitrbution. Why this is the case, is shown in Figure 2.30. In ?fig-ds-t-1 the raw dataset is shown with the underlayed specification limits for the manufacturing of the drive shaft. For some groups the judgement if the drive shaft is wihtin specification is quite clear (group 1, group 2 and group 5). For the other groups, this can not be done so easily. For the drive shaft data, we of course now some population data, therefore the normal distribution can be compared to the t-distribution. This is done in ?fig-ds-t-2. On the x-axis the diameter is shown, the y-axis depicts the groups (as before). The distribution on top of the estimated parameters is the population (normal distribution), the distribution on the bottom follow a t-distribution. With \\(n&gt;30\\) (as for this dataset), the difference between distribution is very small, further showcasing the use of the t-distribution (also see Figure 2.29 for comparison).\n\n\n\n\n\n\n\n\nFigure 2.30: The drive shaft data with normal disitribution, t-distribution and confidence intervalls using the t-distribution",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#f---statistics",
    "href": "chapter001_Distributions.html#f---statistics",
    "title": "2  Statistical Distributions",
    "section": "2.13 F - Statistics",
    "text": "2.13 F - Statistics\nF-statistics, also known as the F-test or F-ratio, is a statistical measure used in Analysis of Variance (ANOVA) and regression analysis (Taboga 2017). It assesses the ratio of two variances, indicating the extent to which the variability between groups or models is greater than the variability within those groups or models. The F-statistic plays a crucial role in hypothesis testing and model comparison.\nSignificance of F-statistics: The significance of the F-statistic lies in its ability to help researchers determine whether the differences between group means or the goodness-of-fit of a regression model are statistically significant. In ANOVA, a high F-statistic suggests that at least one group mean differs significantly from the others, while in regression analysis, it indicates whether the regression model as a whole is a good fit for the data.\nApplications of F-statistics: 1. Analysis of Variance ANOVA: F-statistics are extensively used in ANOVA to compare means across two or more groups. It helps determine whether there are significant differences among the means of these groups. For example, an ANOVA might be used to compare the mean test scores of students taught using different teaching methods.\n\nRegression Analysis: F-statistics are used in regression analysis to assess the overall significance of a regression model. Specifically, in multiple linear regression, it helps determine whether the model, which includes multiple predictor variables, is better at explaining the variance in the response variable compared to a model with no predictors. It tests the null hypothesis that all coefficients of the model are equal to zero.\n\n\n\n\n\n\n\n\n\nFigure 2.31: F-distribution for \\(dof_1\\) on the horizontal and \\(dof_2\\) on the vertical axis\n\n\n\n\n\nThe dof in an F-distribution refer to the two sets of numbers that determine the shape and properties of the distribution (Figure 2.31).\nNumerator Degrees of Freedom (\\(dof_1\\)): The numerator degrees of freedom, often denoted as \\(dof_1\\), is associated with the variability between groups or models in statistical analyses (Figure 2.31 - horizontal axis). In the context of ANOVA, it represents the dof associated with the differences among group means. In regression analysis, it is related to the number of predictors or coefficients being tested simultaneously.\nDenominator Degrees of Freedom (\\(dof_2\\)): The denominator degrees of freedom, often denoted as \\(dof_2\\), is associated with the variability within groups or models (Figure 2.32 - vertical axis). In ANOVA, it represents the degrees of freedom associated with the variability within each group. In regression analysis, it is related to the error or residual degrees of freedom, indicating the remaining variability not explained by the model.\nThe F-distribution is used to compare two variances: one from the numerator and the other from the denominator. The F-statistic, calculated as the ratio of these variances, follows an F-distribution \\(\\eqref{fdist}\\).\n\\[\\begin{align}\nf(x; dof_1, dof_2) = \\frac{{\\Gamma\\left(\\frac{{dof_1 + dof_2}}{2}\\right)}}{{\\Gamma\\left(\\frac{{dof_1}}{2}\\right)\\Gamma\\left(\\frac{{dof_2}}{2}\\right)}} \\left(\\frac{{dof_1}}{{dof_2}}\\right)^{\\frac{{dof_1}}{2}} \\frac{{x^{\\frac{{dof_1}}{2} - 1}}}{{\\left(1 + \\frac{{dof_1}}{{dof_2}}x\\right)^{\\frac{{dof_1 + dof_2}}{2}}}} \\label{fdist} \\\\\nF_{m,n} = \\frac{\\chi^2_m/m}{\\chi^2_n/n}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nFigure 2.32: the maximum density as a function of \\(dof_1\\) and \\(dof_2\\) in a continous parameter space\n\n\n\n\n\n\nIn practical terms: A higher numerator degrees of freedom (\\(dof_1\\)) suggests that there are more groups or predictors being compared, which may result in larger F-statistic values. A higher denominator degrees of freedom (\\(dof_2\\)) implies that there is more data within each group or model, which may lead to smaller F-statistic values. The F-distribution is right-skewed and always positive. It has different shapes depending on the values of \\(dof_1\\) and \\(dof_2\\) (Figure 2.32). The exact shape is determined by these degrees of freedom and cannot be altered by changing sample sizes or data values (Figure 2.32). Researchers use F-distributions to conduct hypothesis tests, such as F-tests in ANOVA and F-tests in regression, to determine if there are significant differences between groups or if a regression model is statistically significant.\nIn summary, dof in the F-distribution are critical in hypothesis testing and model comparisons. They help quantify the variability between and within groups or models, allowing statisticians to assess the significance of observed differences and make informed statistical decisions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#interconnections",
    "href": "chapter001_Distributions.html#interconnections",
    "title": "2  Statistical Distributions",
    "section": "2.14 Interconnections",
    "text": "2.14 Interconnections\n\nNormal Distribution The Normal Distribution is characterized by its mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)), see Figure 2.33. It serves as the foundation for many statistical analyses.\nStandardized Normal Distribution The Standardized Normal Distribution, denoted as \\(Z \\sim N(0, 1)\\), is a special case of the normal distribution. It has a mean (\\(\\mu\\)) of \\(0\\) and a standard deviation (\\(\\sigma\\)) of \\(1\\). It is obtained by standardizing a normal distribution variable \\(X\\): \\(Z = \\frac{X - \\mu}{\\sigma}\\) (Figure 2.33).\nt Distribution The t Distribution is related to the normal distribution and depends on dof. As dof increases, the t-distribution approaches the standard normal distribution (Figure 2.33).\nChi-Square Distribution The Chi-Square Distribution is indirectly connected to the normal distribution through the concept of “sum of squared standard normals.” When standard normal random variables (\\(Z\\)) are squared and summed, the resulting distribution follows a chi-square distribution.\nF Distribution The F Distribution arises from the ratio of two independent chi-square distributed random variables. It is used for comparing variances between groups in statistical tests like ANOVA.\n\n\n\n\n\n\n\n\n\nFigure 2.33: The distributions are interconnected in several different ways.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#weibull---distribution",
    "href": "chapter001_Distributions.html#weibull---distribution",
    "title": "2  Statistical Distributions",
    "section": "2.15 Weibull - Distribution",
    "text": "2.15 Weibull - Distribution\n\n\n\n\n\n\n\n\nFigure 2.34: The weibull distribution and the influence of \\(\\beta\\) and \\(\\lambda\\)\n\n\n\n\n\nThe Weibull distribution is a probability distribution frequently used in statistics and reliability engineering to model the time until an event, particularly failures or lifetimes. It is named after Wallodi Weibull3, who developed it in the mid-20th century (Weibull 1951).\nThe Weibull distribution is characterized by two parameters:\nShape Parameter (\\(\\beta\\)): This parameter determines the shape of the distribution curve and can take on values greater than 0. Depending on the value of \\(\\beta\\), the Weibull distribution can exhibit different behaviors:\nIf \\(\\beta &lt; 1\\), the distribution has a decreasing failure rate, indicating that the probability of an event occurring decreases over time. This is often associated with “infant mortality” or early-life failures. If \\(\\beta = 1\\), the distribution follows an exponential distribution with a constant failure rate over time. If \\(\\beta &gt; 1\\), the distribution has an increasing failure rate, suggesting that the event becomes more likely as time progresses. This is often associated with “wear-out” failures.\nScale Parameter (\\(\\lambda\\)): This parameter represents a characteristic scale or location on the time axis. It influences the position of the distribution on the time axis. A larger \\(\\lambda\\) indicates that events are more likely to occur at later times.\nApplications: - Reliability Engineering: The Weibull distribution is extensively used in reliability engineering to assess the lifetime and failure characteristics of components and systems. Engineers can estimate the distribution parameters from data to predict product reliability, set warranty periods, and plan maintenance schedules.\n\nSurvival Analysis: In medical research and epidemiology, the Weibull distribution is employed to analyze survival data, such as time until the occurrence of a disease or death. It helps in modeling and understanding the progression of diseases and the effectiveness of treatments.\nEconomics and Finance: The Weibull distribution is used in finance to model the time between financial events, like market crashes or loan defaults. It can provide insights into risk assessment and portfolio management.\n\n\n2.15.1 The drive shaft exercise - Weibull distribution\nThe Weibull distribution can be applied to estimate the probability of a part to fail after a given time. Suppose there have been \\(n=100\\) drive shafts produced. In order to assure that the assembled drive shaft would last during their service time, they have been tested in a test-stand that mimics the mission profile4 of the product. This process is called qualification and a big part of any product development (Meyna 2023). The measured hours are shown in Figure 2.35 in a histogram of the data. On the x-axis the Time to failureis shown, while the y-axis shows the number of parts that failed within the time. They histogram plot is overlayed with an empirical density plot as a solid line, as well as the theoretical distribution as a dotted line (Luckily, we know the distribution parameters).\n\n\n\n\n\n\n\n\nFigure 2.35: The measured hours how long the drive shafts lasted in the test stand.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#poisson---distribution",
    "href": "chapter001_Distributions.html#poisson---distribution",
    "title": "2  Statistical Distributions",
    "section": "2.16 Poisson - Distribution",
    "text": "2.16 Poisson - Distribution\nThe Poisson distribution is a probability distribution commonly used in statistics to model the number of events that occur within a fixed interval of time or space, given a known average rate of occurrence. It is named after the French mathematician Siméon Denis Poisson5.\nThe Poisson distribution is an applicable probability model in such situations under specific conditions:\n1. Independence: Events should occur independently of each other within the specified interval of time or space. This means that the occurrence of one event should not affect the likelihood of another event happening.\n2. Constant Rate: The average rate (lambda, denoted as \\(\\lambda\\)) at which events occur should be constant over the entire interval. In other words, the probability of an event occurring should be the same at any point in the interval.\n3. Discreteness: The events being counted must be discrete in nature. This means that they should be countable and should not take on continuous values.\n4. Rare Events: The Poisson distribution is most appropriate when the events are rare, meaning that the probability of more than one event occurring in an infinitesimally small interval is negligible. This assumption helps ensure that the distribution models infrequent events.\n5. Fixed Interval: The interval of time or space in which events are counted should be fixed and well-defined. It should not vary or be open-ended.\n6. Memorylessness: The Poisson distribution assumes that the probability of an event occurring in the future is independent of past events. In other words, it does not take into account the history of events beyond the current interval.\n7. Count Data: The Poisson distribution is most suitable for count data, where you are interested in the number of events that occur in a given interval.\nIn the context of a Poisson distribution, the parameter lambda (\\(\\lambda\\)) represents the average rate of events occurring in a fixed interval of time or space. It is a crucial parameter that helps define the shape and characteristics of the Poisson distribution.\nAverage Rate: \\(\\lambda\\) is a positive real number that represents the average or expected number of events that occur in the specified interval. It tells you, on average, how many events you would expect to observe in that interval.\nRate of Occurrence: \\(\\lambda\\) quantifies the rate at which events happen. A higher value of \\(\\lambda\\) indicates a higher rate of occurrence, while a lower value of \\(\\lambda\\) indicates a lower rate.\nShape of the Distribution: The value of \\(\\lambda\\) determines the shape of the Poisson distribution. Specifically:\nWhen \\(\\lambda\\) is small, the distribution is skewed to the right and is more concentrated toward zero (Figure 2.36). When \\(\\lambda\\) is moderate, the distribution approaches a symmetric bell shape (Figure 2.36). When \\(\\lambda\\) is large, the distribution becomes increasingly similar to a normal distribution(Figure 2.36).\n\n\n\n\n\n\n\n\nFigure 2.36: The poisson distribution with different \\(\\lambda\\) values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Bibby, E. J. G. Pitman. 1980. “Some Basic Theory for Statistical Inference.” The Mathematical Gazette 64 (428): 138–38. https://doi.org/10.2307/3615104.\n\n\nJohnson, Norman Lloyd. 1994. Continuous Univariate Distributions. Wiley.\n\n\nMeyna, Arno. 2023. Sicherheit Und Zuverlässigkeit Technischer Systeme. Carl Hanser Verlag GmbH & Co. KG. https://doi.org/10.3139/9783446468085.fm.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1. https://doi.org/10.2307/2331554.\n\n\nTaboga, Marco. 2017. Lectures on Probability Theory and Mathematical Statistics - 3rd Edition. Createspace Independent Publishing Platform.\n\n\nWeibull, Waloddi. 1951. “A Statistical Distribution Function of Wide Applicability.” Journal of Applied Mechanics 18 (3): 293–97. https://doi.org/10.1115/1.4010337.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "chapter001_Distributions.html#footnotes",
    "href": "chapter001_Distributions.html#footnotes",
    "title": "2  Statistical Distributions",
    "section": "",
    "text": "Jacob Bernoulli (1654-1705): Notable Swiss mathematician, known for Bernoulli’s principle and significant contributions to calculus and probability theory.↩︎\nWilliam Sealy Gosset (June 13, 1876 - October 16, 1937) was a pioneering statistician known for developing the t-distribution, a key tool in modern statistical analysis.↩︎\nWaloddi Weibull (1887–1979) was a Swedish engineer and statistician known for his work on the Weibull distribution, which is widely used in reliability engineering and other fields.↩︎\nA mission profile for parts is a detailed plan specifying how specific components in a system should perform, considering factors like environment, performance, safety, and compliance.↩︎\nSiméon Denis Poisson (1781-1840) was a notable French mathematician, renowned for his work in probability theory and mathematical physics.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Distributions</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html",
    "href": "SamplingMethods.html",
    "title": "3  Sampling Methods",
    "section": "",
    "text": "3.1 Sample Size",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html#sample-size",
    "href": "SamplingMethods.html#sample-size",
    "title": "3  Sampling Methods",
    "section": "",
    "text": "3.1.1 Standard Error\n\n\n\n\n\n\n\n\n\n\n\n(a) maximum sample size \\(n = 200\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) sample size for \\(n = 5 \\ldots 50\\)\n\n\n\n\n\n\n\nFigure 3.1: The SE for varying sample sizes \\(n\\)\n\n\n\nStandard Error (SE) is a statistical measure that quantifies the variation or uncertainty in sample statistics, particularly the mean (average). It is a valuable tool in inferential statistics and provides an estimate of how much the sample mean is expected to vary from the true population mean.\n\\[\\begin{align}\nSE = \\frac{sd}{\\sqrt{n}}\n\\end{align}\\]\nA smaller SE indicates that the sample mean is likely very close to the population mean, while a larger standard error suggests greater variability and less precision in estimating the population mean. SE is crucial when constructing confidence intervals and performing hypothesis tests, as it helps in assessing the reliability of sample statistics as estimates of population parameters.\nVariance vs. Standard Deviation: The standard error formula is based on the standard deviation of the sample, not the variance. The standard deviation is the square root of the variance.\nScaling of Variability: The purpose of the standard error is to measure the variability or spread of sample means. The square root of the sample size reflects how that variability decreases as the sample size increases. When the sample size is larger, the sample mean is expected to be closer to the population mean, and the standard error becomes smaller to reflect this reduced variability.\nCentral Limit Theorem: The inclusion of \\(\\sqrt{n}\\) in the standard error formula is closely tied to the Central Limit Theorem, which states that the distribution of sample means approaches a normal distribution as the sample size increases. \\(\\sqrt{n}\\) helps in this context to ensure that the standard error appropriately reflects the distribution’s properties.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html#random-sampling",
    "href": "SamplingMethods.html#random-sampling",
    "title": "3  Sampling Methods",
    "section": "3.2 Random Sampling",
    "text": "3.2 Random Sampling\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: The idea of random sampling (Dan Kernler).\n\n\n\n\n\n\n\n\nDefinition: Selecting a sample from a population in a purely random manner, where every individual has an equal chance of being chosen.\nAdvantages:\n\nEliminates bias in selection.\nResults are often representative of the population.\n\nDisadvantages:\n\nPossibility of unequal representation of subgroups.\nTime-consuming and may not be practical for large populations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html#stratified-sampling",
    "href": "SamplingMethods.html#stratified-sampling",
    "title": "3  Sampling Methods",
    "section": "3.3 Stratified Sampling",
    "text": "3.3 Stratified Sampling\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: The idea of stratified sampling (Dan Kernler)\n\n\n\n\n\n\n\n\nDefinition: Dividing the population into subgroups or strata based on certain characteristics and then randomly sampling from each stratum.\nAdvantages:\n\nEnsures representation from all relevant subgroups.\nIncreased precision in estimating population parameters.\n\nDisadvantages:\n\nRequires accurate classification of the population into strata.\nComplexity in implementation and analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html#systematic-sampling",
    "href": "SamplingMethods.html#systematic-sampling",
    "title": "3  Sampling Methods",
    "section": "3.4 Systematic Sampling",
    "text": "3.4 Systematic Sampling\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: The idea of systematic sampling (Dan Kernler)\n\n\n\n\n\n\n\n\nDefinition: Choosing every kth individual from a list after selecting a random starting point.\nAdvantages:\n\nSimplicity in execution compared to random sampling.\nSuitable for large populations.\n\nDisadvantages:\n\nSusceptible to periodic patterns in the population.\nIf the periodicity aligns with the sampling interval, it can introduce bias.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html#cluster-sampling",
    "href": "SamplingMethods.html#cluster-sampling",
    "title": "3  Sampling Methods",
    "section": "3.5 Cluster Sampling",
    "text": "3.5 Cluster Sampling\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: The idea of clustered sampling (Dan Kernler).\n\n\n\n\n\n\n\n\nDefinition: Dividing the population into clusters, randomly selecting some clusters, and then including all individuals from the chosen clusters in the sample.\nAdvantages:\n\nCost-effective, especially for geographically dispersed populations.\nReduces logistical challenges compared to other methods.\n\nDisadvantages:\n\nIncreased variability within clusters compared to other methods.\nRequires accurate information on cluster characteristics.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html#example---classroom-sampling",
    "href": "SamplingMethods.html#example---classroom-sampling",
    "title": "3  Sampling Methods",
    "section": "3.6 Example - Classroom Sampling",
    "text": "3.6 Example - Classroom Sampling\n\n3.6.1 Descriptive Statistics (Population)\n\n\n\n\nTable 3.1: The Classroom data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 401\n\n\n\n\nAge\n30 (28, 34)\n\n\nGender\n\n\n\n\n    Female\n23 (58%)\n\n\n    Male\n17 (43%)\n\n\n\n1 Median (Q1, Q3); n (%)\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.1.1 Histogram\n\n\n\n\n\n\n\n\nFigure 3.6: The histograms of students\n\n\n\n\n\n\n\n3.6.1.2 Classroom Age\n\n\n\n\n\n\n\n\nFigure 3.7: Where students are seated (Age)\n\n\n\n\n\n\n\n3.6.1.3 Classroom Gender\n\n\n\n\n\n\n\n\nFigure 3.8: Where students are seated (Age)\n\n\n\n\n\n\n\n3.6.1.4 Population Values\n\\[\\mu_{Age} = 30.4\\] \\[sd_{Age} = 4.18\\]\nThese are the values we want to estimate using the introduced sampling strategies\n\n\n\n3.6.2 Simple Random Sampling\n\n\n\n\nTable 3.2: The means and standard deviations of the classroom data for n = 5,10,15,20\n\n\n\n\n\n\n\n\n\nn\nmean in years\nstandard deviation in years\n\n\n\n\n5\n29.80\n5.36\n\n\n10\n30.50\n3.37\n\n\n15\n30.93\n5.09\n\n\n20\n31.00\n4.00\n\n\npopulation\n30.40\n4.18\n\n\n\n\n\n\n\n\n\n\n\n3.6.2.1 \\(n = 5\\)\n\n\n\n\n\n\n\n\nFigure 3.9: Where students are seated (Age) and the 5 samples\n\n\n\n\n\n\n\n3.6.2.2 \\(n = 10\\)\n\n\n\n\n\n\n\n\nFigure 3.10: Where students are seated (Age) and the 10 samples\n\n\n\n\n\n\n\n3.6.2.3 \\(n = 15\\)\n\n\n\n\n\n\n\n\nFigure 3.11: Where students are seated (Age) and the 15 samples\n\n\n\n\n\n\n\n3.6.2.4 \\(n = 20\\)\n\n\n\n\n\n\n\n\nFigure 3.12: Where students are seated (Age) and the 20 samples\n\n\n\n\n\n\n\n3.6.2.5 Data\n\n\n\n\n\n\n\n\nFigure 3.13: Comparison of sample to the population\n\n\n\n\n\n\n\n3.6.2.6 Mean Comparison\n\n\n\n\n\n\n\n\nFigure 3.14: The difference in means at different sample sizes\n\n\n\n\n\n\n\n3.6.2.7 SD Comparison\n\n\n\n\n\n\n\n\nFigure 3.15: The difference in sd at different sample sizes\n\n\n\n\n\n\n\n\n3.6.3 Systematic Sampling\nSample always the \\(k\\)th.\n\n\n\n\nTable 3.3: The output of systematic sampling for every 8th, 5th, 4th, 2nd\n\n\n\n\n\n\n\n\n\nk\nmean in years\nstandard deviation in years\n\n\n\n\n8\n33.40\n3.58\n\n\n5\n30.62\n4.41\n\n\n3\n31.57\n4.07\n\n\n2\n30.95\n4.59\n\n\npopulation\n30.40\n4.18\n\n\n\n\n\n\n\n\n\n\n\n3.6.3.1 \\(k = 8\\)\n\n\n\n\n\n\n\n\nFigure 3.16: Where students are seated (Age) and every 8th sample\n\n\n\n\n\n\n\n3.6.3.2 \\(k = 5\\)\n\n\n\n\n\n\n\n\nFigure 3.17: Where students are seated (Age) and every 5th sample\n\n\n\n\n\n\n\n3.6.3.3 \\(k = 2\\)\n\n\n\n\n\n\n\n\nFigure 3.18: Where students are seated (Age) and every 2nd sample\n\n\n\n\n\n\n\n3.6.3.4 Data\n\n\n\n\n\n\n\n\nFigure 3.19: The data per subgroup\n\n\n\n\n\n\n\n3.6.3.5 Mean Comparison\n\n\n\n\n\n\n\n\nFigure 3.20: The difference in means at different sampling intervals\n\n\n\n\n\n\n\n3.6.3.6 SD Comparison\n\n\n\n\n\n\n\n\nFigure 3.21: The difference in sd at different sampling intervals\n\n\n\n\n\n\n\n\n3.6.4 Stratified Sampling\nChoose sample stratified to characteristic (Gender represented in population)\n\n3.6.4.1 \\(\\text{proportion} = 12\\% \\rightarrow n = 5\\)\n\n\n\n\n\n\n\n\nFigure 3.22: Where students are seated (Age), stratified according to gender (\\(12\\%\\))\n\n\n\n\n\n\n\n3.6.4.2 \\(\\text{proportion} = 25\\% \\rightarrow n = 10\\)\n\n\n\n\n\n\n\n\nFigure 3.23: Where students are seated (Age), stratified according to gender (\\(25\\%\\))\n\n\n\n\n\n\n\n3.6.4.3 \\(\\text{proportion} = 38\\% \\rightarrow n = 15\\)\n\n\n\n\n\n\n\n\nFigure 3.24: Where students are seated (Age), stratified according to gender (\\(38\\%\\))\n\n\n\n\n\n\n\n3.6.4.4 \\(\\text{proportion} = 50\\% \\rightarrow n = 20\\)\n\n\n\n\n\n\n\n\nFigure 3.25: Where students are seated (Age), stratified according to gender (\\(50\\%\\))\n\n\n\n\n\n\n\n3.6.4.5 Data\n\n\n\n\n\n\n\n\nFigure 3.26: The data of the stratified sampling\n\n\n\n\n\n\n\n3.6.4.6 Mean Comparsion\n\n\n\n\n\n\n\n\nFigure 3.27: The difference in means at different sample sizes\n\n\n\n\n\n\n\n3.6.4.7 SD Comparison\n\n\n\n\n\n\n\n\nFigure 3.28: The difference in sd at different sample sizes\n\n\n\n\n\n\n\n\n3.6.5 Clustered Sampling\nClusters are logical units which are sampled in order to save sampling resources.\nIn our case clusters are columns of students.\n\n3.6.5.1 One Cluster\n\n\n\n\n\n\n\n\nFigure 3.29: Where students are seated (Age), cluster one\n\n\n\n\n\n\n\n3.6.5.2 Two Clusters\n\n\n\n\n\n\n\n\nFigure 3.30: Where students are seated (Age), two clusters\n\n\n\n\n\n\n\n3.6.5.3 Three Clusters\n\n\n\n\n\n\n\n\nFigure 3.31: Where students are seated (Age), three clusters\n\n\n\n\n\n\n\n3.6.5.4 Four Clusters\n\n\n\n\n\n\n\n\nFigure 3.32: Where students are seated (Age), four clusters\n\n\n\n\n\n\n\n3.6.5.5 Data\n\n\n\n\n\n\n\n\nFigure 3.33: The data of the clustered sampling\n\n\n\n\n\n\n\n3.6.5.6 Mean Comparison\n\n\n\n\n\n\n\n\nFigure 3.34: The difference in means at different sample sizes\n\n\n\n\n\n\n\n3.6.5.7 SD Comparison\n\n\n\n\n\n\n\n\nFigure 3.35: The difference in sd at different sample sizes\n\n\n\n\n\n\n\n\n3.6.6 Overall Comparison of Sampling Strategies (Gender)\n\n\n3.6.7 Overall Comparison of Sampling Strategies (Mean)\n\n\n\n\n\n\n\n\nFigure 3.36: A graphical comparison of the absolute difference in means per sample strategy\n\n\n\n\n\n\n\n3.6.8 Overall Comparison of Sampling Strategies (SD)\n\n\n\n\n\n\n\n\nFigure 3.37: A graphical comparison of the absolute difference in means per sample strategy",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "SamplingMethods.html#bootstrapping",
    "href": "SamplingMethods.html#bootstrapping",
    "title": "3  Sampling Methods",
    "section": "3.7 Bootstrapping",
    "text": "3.7 Bootstrapping\n\n\n\n\n\n\n\n\n\n\nFigure 3.38: The idea of bootstrapping (Biggerj1, Marsupilami)\n\n\n\n\n\n\n\n\nDefinition: Estimating sample statistic distribution by drawing new samples with replacement from observed data, providing insights into variability without strict population distribution assumptions.\nAdvantages:\n\nNon-parametric: Works without assuming a specific data distribution.\nConfidence Intervals: Facilitates easy estimation of confidence intervals.\nRobustness: Reliable for small sample sizes or unknown data distributions.\n\nDisadvantages:\n\nComputationally Intensive: Resource-intensive for large datasets.\nResults quality relies on the representativeness of the initial sample (garbage in - garbage out).\nCannot compensate for inadequate information in the original sample.\nNot Always Optimal: Traditional methods may be better in cases meeting distribution assumptions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Methods</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html",
    "href": "chapter002_InferentialStatistics.html",
    "title": "4  Inferential Statistics",
    "section": "",
    "text": "4.1 Hypothesis Testing - Basics\nInferential statistics involves making predictions, generalizations, or inferences about a population based on a sample of data. These techniques are used when researchers want to draw conclusions beyond the specific data they have collected. Inferential statistics help answer questions about relationships, differences, and associations within a population.\nFigure 4.1: We are hypotheses.\nNull-Hypothesis (H0) This is the default or status quo assumption. It represents the belief that there is no significant change, effect, or difference in the production process. It is often denoted as a statement of equality (e.g., the mean production rate is equal to a certain value).\nalternative Hypothesis (Ha): This is the claim or statement we want to test. It represents the opposite of the null hypothesis, suggesting that there is a significant change, effect, or difference in the production process (e.g., the mean production rate is not equal to a certain value).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#hypothesis-testing---basics",
    "href": "chapter002_InferentialStatistics.html#hypothesis-testing---basics",
    "title": "4  Inferential Statistics",
    "section": "",
    "text": "4.1.1 The drive shaft exercise - Hypotheses\nDuring the Quality Control (QC) of the drive shaft \\(n=100\\) samples are taken and the diameter is measured with an accuracy of \\(\\pm 0.01mm\\). Is the true mean of all produced drive shafts within the specification?\nFor this we can formulate the hypotheses.\n\nH0\n\nThe drive shaft diameter is within the specification.\n\nHa:\n\nThe drive shaft diameter is not within the specification.\n\n\nIn the following we will explore, how to test for these hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#confidence-intervals",
    "href": "chapter002_InferentialStatistics.html#confidence-intervals",
    "title": "4  Inferential Statistics",
    "section": "4.2 Confidence Intervals",
    "text": "4.2 Confidence Intervals\nA Confidence Interval (CI) is a statistical concept used to estimate a range of values within which a population parameter, such as a population mean or proportion, is likely to fall. It provides a way to express the uncertainty or variability in our sample data when making inferences about the population. In other words, it quantifies the level of confidence we have in our estimate of a population parameter.\nConfidence intervals are typically expressed as a range with an associated confidence level. The confidence level, often denoted as \\(1-\\alpha\\), represents the probability that the calculated interval contains the true population parameter. Common confidence levels include \\(90\\%\\), \\(95\\%\\), and \\(99\\%\\).\nThere are different ways of calculating CI).\n\nFor the population mean \\(\\mu_0\\) when the population standard deviation \\(\\sigma_0^2\\) is known (\\(\\eqref{ci01}\\)).\n\n\\[\\begin{align}\nCI = \\bar{X} \\pm t \\frac{\\sigma_0}{\\sqrt{n}} \\label{ci01}\n\\end{align}\\]\n\n\\(\\bar{X}\\) is the sample mean.\n\\(Z\\) is the critical value from the standard normal distribution corresponding to the desired confidence level (e.g., \\(1.96\\) for a \\(95\\%\\) confidence interval).\n\\(\\sigma_0\\) is the populations standard deviation\n\\(n\\) is the sample size\n\n2.For the population mean \\(\\mu_0\\) when the population standard deviation \\(\\sigma_0^2\\) is Unknown (t-confidence interval), see \\(\\eqref{ci02}\\).\n\\[\\begin{align}\nCI = \\bar{X} \\pm t \\frac{sd}{\\sqrt{n}} \\label{ci02}\n\\end{align}\\]\n\n\\(\\bar{X}\\) is the sample mean.\n\\(t\\) is the critical value from the t-distribution with \\(n-1\\) degrees of freedom corresponding to the desired confidence level\n\\(sd\\) is the sample standard deviation\n\\(n\\) is the sample size\n\n\nFor a population proportion p, see \\(\\eqref{ci03}\\).\n\n\\[\\begin{align}\nCI = \\hat{p} \\pm Z \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\label{ci03}\n\\end{align}\\]\n\n\\(\\hat{p}\\) is the sample proportion\n\\(Z\\) is the critical value from the standard normal distribution corresponding to the desired confidence level\n\\(n\\) is the sample size\n\n\nThe method for calculating confidence intervals may vary depending on the estimated parameter. Estimating a population median or the differences between two population means, other statistical techniques may be used.\n\n\n4.2.1 The drive shaft exercise - Confidence Intervals\n\n\n\n\n\n\n\n\nFigure 4.2: The 95% CI for the drive shaft data.\n\n\n\n\n\nThe \\(95\\%\\) CI for the drive shaft data is shown in Figure 4.2. For comparison the histogram with an overlayed density curve is plotted. The highlighted area shows the minimum and maximum CI, the calculated mean is shown as a dashed line.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#significance-level",
    "href": "chapter002_InferentialStatistics.html#significance-level",
    "title": "4  Inferential Statistics",
    "section": "4.3 Significance Level",
    "text": "4.3 Significance Level\nThe significance level \\(\\alpha\\) is a critical component of hypothesis testing in statistics. It represents the maximum acceptable probability of making a Type I error, which is the error of rejecting a null hypothesis when it is actually true. In other words, \\(\\alpha\\) is the probability of concluding that there is an effect or relationship when there isn’t one. Commonly used significance levels include \\(0.05 (5\\%)\\), \\(0.01 (1\\%)\\), and \\(0.10 (10\\%)\\). The choice of \\(\\alpha\\) depends on the context of the study and the desired balance between making correct decisions and minimizing the risk of Type I errors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#false-negative---risk",
    "href": "chapter002_InferentialStatistics.html#false-negative---risk",
    "title": "4  Inferential Statistics",
    "section": "4.4 False negative - risk",
    "text": "4.4 False negative - risk\nThe risk for a false negative outcome is called \\(\\beta\\) - risk. Is is calculated using statistical power analysis. Statistical power is the probability of correctly rejecting a null hypothesis when it is false, which is essentially the complement of beta (\\(\\beta\\)).\n\\[\\begin{align}\n\\beta = 1 - \\text{Power}\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#power-analysis",
    "href": "chapter002_InferentialStatistics.html#power-analysis",
    "title": "4  Inferential Statistics",
    "section": "4.5 Power Analysis",
    "text": "4.5 Power Analysis\nStatistical power is calculated using software, statistical tables, or calculators specifically designed for this purpose. Generally speaking: The greater the statistical power, the greater is the evidence to accept or reject the \\(H_0\\) based on the study. Power analysis is also very useful in determining the sample size before the actualy experiments are conducted. Below is an example for a power calculation for a two-sample t-test.\n\\[\n\\text{Power} = 1 - \\beta = P\\left(\\frac{{|\\bar{X}_1 - \\bar{X}_2|}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}} &gt; Z_{\\frac{\\alpha}{2}} - \\frac{\\delta}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\\right)\n\\]\n\nEffect Size: This represents the magnitude of the effect you want to detect. Larger effects are easier to detect than smaller ones.\nSignificance Level (\\(\\alpha\\)): This is the predetermined level of significance that defines how confident you want to be in rejecting the null hypothesis (e.g., typically set at 0.05).\nSample Size (\\(n\\)): The number of observations or participants in your study. Increasing the sample size generally increases the power of the test.\nPower (\\(1 - \\beta\\)): This is the probability of correctly rejecting the null hypothesis when it is false. Higher power is desirable, as it minimizes the chances of a Type II error (failing to detect a true effect).\nType I Error (\\(\\alpha\\)): The probability of incorrectly rejecting the null hypothesis when it is true. This is typically set at \\(0.05\\) or \\(5\\%\\) in most studies.\nType II Error (\\(\\beta\\)): The probability of failing to reject the null hypothesis when it is false. Power is the complement of \\(\\beta\\) (\\(Power = 1 - \\beta\\)).\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: The coin toss with the respective probabilites (Champely 2020).\n\n\n\n\n\n\n\n\nH0:\n\nThe coin is fair and lands heads \\(50\\%\\) of the time.\n\nHa:\n\nThe coin is loaded and lands heads more than \\(50\\%\\) of the time.\n\n\n\n\n\npwr.p.test(h = ES.h(p1 = 0.75, p2 = 0.50),\n           sig.level = 0.05,\n           power = 0.80,\n           alternative = \"greater\")\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5235988\n              n = 22.55126\n      sig.level = 0.05\n          power = 0.8\n    alternative = greater\n\n\n\n\nThe sample size \\(n = 23\\), meaning \\(23\\) coin flips means that the statistical power is \\(80\\%\\) at a \\(\\alpha = 0.05\\) significance level (\\(\\beta = 1-power = 0.2 \\approx 20\\%\\)). But what if the sample size varies? This is the subject of Figure 4.4. On the x-axis the power is shown (or the \\(\\beta\\)-risk on the upper x-axis), whereas the sample size n is depicted on the y-axis. To increase the power by \\(10\\%\\) to be \\(90\\%\\) the sample sized must be increased by \\(11\\). A further power increase of \\(5\\%\\) would in turn mean an increase in sample size to be \\(n = 40\\). This highlights the non-linear nature of power calculations and why they are important for experimental planning.\n\n\n\n\n\n\n\n\nFigure 4.4: The power vs. the sample size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: The power vs. the sample size for different effect sizes\n\n\n\n\n\n\n\n\n4.5.1 A word on Effect Size\nCohen (Cohen 2013) describes effect size as “the degree to which the null hypothesis is false.” In the coin flipping example, this is the difference between \\(75\\%\\) and \\(50\\%\\). We could say the effect was 25% but recall we had to transform the absolute difference in proportions to another quantity using the ES.h function. This is a crucial part of doing power analysis correctly: An effect size must be provided on the expected scale. Doing otherwise will produce wrong sample size and power calculations.\nWhen in doubt, Conventional Effect Sizes can be used. These are pre-determined effect sizes for “small”, “medium”, and “large” effects, see Cohen (2013).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#p-value",
    "href": "chapter002_InferentialStatistics.html#p-value",
    "title": "4  Inferential Statistics",
    "section": "4.6 p-value",
    "text": "4.6 p-value\n\n\n\n\n\n\n\n\nFigure 4.6: Type I and Type II error in the context of inferential statistics.\n\n\n\n\n\nThe p-value is a statistical measure that quantifies the evidence against a null hypothesis. It represents the probability of obtaining test results as extreme or more extreme than the ones observed, assuming the null hypothesis is true. In hypothesis testing, a smaller p-value indicates stronger evidence against the null hypothesis. If the p-value is less than or equal to \\(\\alpha\\) (\\(p \\leq \\alpha\\)), you reject the null hypothesis. If the p-value is greater than \\(\\alpha\\) ( \\(p &gt; \\alpha\\) ), you fail to reject the null hypothesis. A common threshold for determining statistical significance is to reject the null hypothesis when \\(p\\leq\\alpha\\).\nThe p-value however does not give an assumption about the effect size, which can be quite insignificant (Nuzzo 2014). While the p-value therefore is the probability of accepting \\(H_a\\) as true, it is not a measure of magnitude or relative importance of an effect. Therefore the CI and the effect size should always be reported with a p-value. Some Researchers even claim that most of the research today is false (Ioannidis 2005). In practice, especially in the manufacturing industry, the p-value and its use is still popular. Before implementing any measures in a series production, those questions will be asked. The confident and reliable engineer asks them beforehand and is always his own greatest critique.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#statistical-errors",
    "href": "chapter002_InferentialStatistics.html#statistical-errors",
    "title": "4  Inferential Statistics",
    "section": "4.7 Statistical errors",
    "text": "4.7 Statistical errors\n\n\n\n\n\n\n\n\nFigure 4.7: The statistical Errors (Type I and Type II).\n\n\n\n\n\n\nType I Error (False Positive, see Figure 4.7):\n\nA Type I error occurs when a null hypothesis that is actually true is rejected. In other words, it’s a false alarm. It is concluded that there is a significant effect or difference when there is none. The probability of committing a Type I error is denoted by the significance level \\(\\alpha\\). Example: Imagine a drug trial where the null hypothesis is that the drug has no effect (it’s ineffective), but due to random chance, the data appears to show a significant effect, and you incorrectly conclude that the drug is effective (Type I error).\n\nType II Error (False Negative, see Figure 4.7):\n\nA Type II error occurs when a null hypothesis that is actually false is not rejected. It means failing to detect a significant effect or difference when one actually exists. The probability of committing a Type II error is denoted by the symbol \\(\\beta\\). Example: In a criminal trial, the null hypothesis might be that the defendant is innocent, but they are actually guilty. If the jury fails to find enough evidence to convict the guilty person, it is a Type II error.\nType I Error is falsely concluding, that there is an effect or difference when there is none (false positive). Type II Error failing to conclude that there is an effect or difference when there actually is one (false negative).\nThe relationship between Type I and Type II errors is often described as a trade-off. As the risk of Type I errors is reduced by lowering the significance level (\\(\\alpha\\)), the risk of Type II errors (\\(\\beta\\)) is typically increased (Figure 4.6). This trade-off is inherent in hypothesis testing, and the choice of significance level depends on the specific goals and context of the study. Researchers often aim to strike a balance between these two types of errors based on the consequences and costs associated with each. This balance is a critical aspect of the design and interpretation of statistical tests.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#parametric-and-non-parametric-tests",
    "href": "chapter002_InferentialStatistics.html#parametric-and-non-parametric-tests",
    "title": "4  Inferential Statistics",
    "section": "4.8 Parametric and Non-parametric Tests",
    "text": "4.8 Parametric and Non-parametric Tests\nParametric and non-parametric tests in statistics are methods used for analyzing data. The primary difference between them lies in the assumptions they make about the underlying data distribution:\n\nParametric Tests:\n\nThese tests assume that the data follows a specific probability distribution, often the normal distribution.\nParametric tests make assumptions about population parameters like means and variances.\nThey are more powerful when the data truly follows the assumed distribution.\nExamples of parametric tests include t-tests, ANOVA, regression analysis, and parametric correlation tests.\n\nNon-Parametric Tests:\n\nNon-parametric tests make minimal or no assumptions about the shape of the population distribution.\nThey are more robust and can be used when data deviates from a normal distribution or when dealing with ordinal or nominal data.\nNon-parametric tests are generally less powerful compared to parametric tests but can be more reliable in certain situations.\nExamples of non-parametric tests include the Mann-Whitney U test, Wilcoxon signed-rank test, Kruskal-Wallis test, and Spearman’s rank correlation.\n\n\nThe choice between parametric and non-parametric tests depends on the nature of the data and the assumptions. Parametric tests are appropriate when data follows the assumed distribution, while non-parametric tests are suitable when dealing with non-normally distributed data or ordinal data. Some examples for parametric and non-parametric tests are given in Table 4.1.\n\n\n\n\n\nTable 4.1: Some parametric and non-parametric statistical tests.\n\n\n\n\n\n\n\n\n\nParametric Tests\nNon-Parametric Tests\n\n\n\n\nOne-sample t-test\nWilcoxon signed rank test\n\n\nPaired t-test\nMann-Whitney U test\n\n\nTwo-sample t-test\nKruskal Wallis test\n\n\nOne-Way ANOVA\nWelch Test",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#paired-and-independent-tests",
    "href": "chapter002_InferentialStatistics.html#paired-and-independent-tests",
    "title": "4  Inferential Statistics",
    "section": "4.9 Paired and Independent Tests",
    "text": "4.9 Paired and Independent Tests\n\n\n\n\n\n\n\n\nFigure 4.8: The difference between paired and independent Tests.\n\n\n\n\n\n\nPaired Statistical Test:\n\n\nPaired tests are used when there is a natural pairing or connection between two sets of data points. This pairing is often due to repeated measurements on the same subjects or entities.\nThey are designed to assess the difference between two related samples, such as before and after measurements on the same group of individuals.\nThe key idea is to reduce variability by considering the differences within each pair, which can increase the test sensitivity.\n\n\nIndependent Statistical Test:\n\n\nIndependent tests, also known as unpaired or two-sample tests, are used when there is no inherent pairing between the two sets of data.\nThese tests are typically applied to compare two separate and unrelated groups or samples.\nThey assume that the data in each group is independent of the other, meaning that the value in one group doesn’t affect the value in the other group.\n\nAn example for a paired test is, if two groups of data are to be compared in two different points in time (see Figure 4.8).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#distribution-tests",
    "href": "chapter002_InferentialStatistics.html#distribution-tests",
    "title": "4  Inferential Statistics",
    "section": "4.10 Distribution Tests",
    "text": "4.10 Distribution Tests\nThe importance of testing for normality (or other distributions) lies in the fact that various statistical techniques, such as parametric tests (e.g., t-tests, ANOVA), are based on the assumption of for example normality. When data deviates significantly from a normal distribution, using these parametric methods can lead to incorrect conclusions and biased results. Therefore, it is essential to determine how a dataset is approximately distributed before applying such techniques.\nSeveral tests for normality are available, with the most common ones being the Kolmogorov-Smirnov test, the Shapiro-Wilk test, and the Anderson-Darling test. These tests provide a quantitative measure of how well the data conforms to a normal distribution.\nIn practice, it is important to interpret the results of these tests cautiously. Sometimes, a minor departure from normality may not affect the validity of parametric tests, especially when the sample size is large. In such cases, using non-parametric methods may be an alternative. However, in cases where normality assumptions are crucial, transformations of the data or choosing appropriate non-parametric tests may be necessary to ensure the reliability of statistical analyses.\nTests for normality do not free you from the burden of thinking for yourself.\n\n4.10.1 Quantile-Quantile plots\nQuantile-Quantile plots are a graphical tool used in statistics to assess whether a dataset follows a particular theoretical distribution, typically the normal distribution. They provide a visual comparison between the observed quantiles1 of the data and the quantiles expected from the chosen theoretical distribution.\nA neutral explanation of how QQ plots work:\n\n4.10.1.1 Sample data\nIn Table 4.2 \\(n=10\\) datapoints are shown as a sample dataset.\n\n\n\n\nTable 4.2: 10 randomly sampled datapoints for the creation of the QQ-plot\n\n\n\n\n\n\n\n\n\nx\nsmpl_no\n\n\n\n\n-0.56047565\n1\n\n\n-0.23017749\n2\n\n\n1.55870831\n3\n\n\n0.07050839\n4\n\n\n0.12928774\n5\n\n\n1.71506499\n6\n\n\n0.46091621\n7\n\n\n-1.26506123\n8\n\n\n-0.68685285\n9\n\n\n-0.44566197\n10\n\n\n\n\n\n\n\n\n\n\n\n\n4.10.1.2 Data Sorting\nTo create a QQ plot, the data must be sorted in ascending order.\n\n\n\n\nTable 4.3: The sorted data points.\n\n\n\n\n\n\n\n\n\nx\nsmpl_no\n\n\n\n\n-1.26506123\n8\n\n\n-0.68685285\n9\n\n\n-0.56047565\n1\n\n\n-0.44566197\n10\n\n\n-0.23017749\n2\n\n\n0.07050839\n4\n\n\n0.12928774\n5\n\n\n0.46091621\n7\n\n\n1.55870831\n3\n\n\n1.71506499\n6\n\n\n\n\n\n\n\n\n\n\n\n\n4.10.1.3 Theoretical Quantiles\nTheoretical quantiles are calculated based on the chosen distribution (e.g., the normal distribution). These quantiles represent the expected values if the data perfectly follows that distribution.\n\n\n\n\nTable 4.4: The calculated theoretical quantiles\n\n\n\n\n\n\n\n\n\nx\nsmpl_no\nx_norm\nx_thrtcl\n\n\n\n\n-1.26506123\n8\n-1.404601888\n0.08006985\n\n\n-0.68685285\n9\n-0.798376211\n0.21232610\n\n\n-0.56047565\n1\n-0.665875352\n0.25274539\n\n\n-0.44566197\n10\n-0.545498338\n0.29270541\n\n\n-0.23017749\n2\n-0.319572479\n0.37464622\n\n\n0.07050839\n4\n-0.004316756\n0.49827787\n\n\n0.12928774\n5\n0.057310762\n0.52285118\n\n\n0.46091621\n7\n0.405008410\n0.65726434\n\n\n1.55870831\n3\n1.555994430\n0.94014529\n\n\n1.71506499\n6\n1.719927421\n0.95727718\n\n\n\n\n\n\n\n\n\n\n\n\n4.10.1.4 Plotting Points\n\n\n\n\n\n\n\n\nFigure 4.9: The QQ points as calculated before.\n\n\n\n\n\nFor each data point, a point is plotted in the QQ plot. The x-coordinate of the point corresponds to the theoretical quantile, and the y-coordinate corresponds to the observed quantile from the data, see Figure 4.9.\n\n\n4.10.1.5 Perfect Normal Distribution\n\n\n\n\n\n\n\n\nFigure 4.10: A perfect normal distribution would be indicated if all points would fall on this straight line.\n\n\n\n\n\nIn the case of a perfect normal distribution, all the points would fall along a straight line at a 45-degree angle. If the data deviates from normality, the points may deviate from this line in specific ways, see Figure 4.10.\n\n\n4.10.1.6 Interpretation\n\n\n\n\n\n\n\n\nFigure 4.11: The QQ line as plotted using the theoretical and sample quantiles.\n\n\n\n\n\nDeviations from the straight line suggest departures from the assumed distribution. For example, if points curve upward, it indicates that the data has heavier tails than a normal distribution. If points curve downward, it suggests lighter tails. S-shaped curves or other patterns can reveal additional information about the data’s distribution. In Figure 4.11 the QQ-points are shown together with the respective QQ-line and a line of perfectly normal distributed points. Some deviations can be seen, but it is hard to judge, if the data is normally distributed or not.\n\n\n4.10.1.7 Confidence Interval\n\n\n\n\n\n\n\n\nFigure 4.12: The QQ plot with confidence bands.\n\n\n\n\n\nBecause it is hard to judge from Figure 4.11 if the points are normally distributed, it makes sense to get limits for normally disitrbuted points. This is shown in Figure 4.12. The gray area depicts the (\\(95\\%\\)) confidence bands for a normal distribution. All the points fall into the area, as well as the line. This shows, that the points are likely to be normally distributed.\n\n\n4.10.1.8 The drive shaft exercise\n\n\n\n\n\n\n\n\nFigure 4.13: The QQ plots for each drive shaft group shown in subplots.\n\n\n\n\n\nThe QQ plot method is extended to the drive shaft exercise in Figure 4.13. In each subplot the plot for the respective group is shown together with the QQ-points, the QQ-line and the respective confidence bands. The scaling for each plot is different to enhance visibility of every subplot. A line for the perfect normal distribution is also shown in solid linestyle. From group \\(1 \\ldots 4\\) all points fall into the QQ confidence bands. Group05 differs however. The points from visible categories, which is a strong indicator, that the measurement system may be to inaccurate.\n\n\n\n4.10.2 Quantitative Methods\n\n\n\n\n\n\n\n\nFigure 4.14: A visualisation of the KS test using the 10 datapoints from before\n\n\n\n\n\nThe Kolmogorov-Smirnov test for normality, often referred to as the KS test, is a statistical test used to assess whether a dataset follows a normal distribution. It evaluates how closely the cumulative distribution function of the dataset matches the expected CDF of a normal distribution.\n\nNull Hypothesis (H0): The null hypothesis in the KS test states that the sample data follows a normal distribution.\nAlternative Hypothesis (Ha): The alternative hypothesis suggests that the sample data significantly deviates from a normal distribution.\nTest Statistic (D): The KS test calculates a test statistic, denoted as D which measures the maximum vertical difference between the empirical CDF of the data and the theoretical CDF of a normal distribution. It quantifies how far the observed data diverges from the expected normal distribution. A visualization of the KS-test is shown in Figure 4.14. The red line denotes a perfect normal distribution, whereas the step function shows the empirical CDF of the data itself.\nCritical Value: To assess the significance of D, a critical value is determined based on the sample size and the chosen significance level (\\(\\alpha\\)). If D exceeds the critical value, it indicates that the dataset deviates significantly from a normal distribution.\nDecision: If D is greater than the critical value, the null hypothesis is rejected, and it is concluded that the data is not normally distributed. If D is less than or equal to the critical value, there is not enough evidence to reject the null hypothesis, suggesting that the data may follow a normal distribution.\n\nIt is important to note that the KS test is sensitive to departures from normality in both tails of the distribution. There are other normality tests, like the Shapiro-Wilk test and Anderson-Darling test, which may be more suitable in certain situations. Researchers typically choose the most appropriate test based on the characteristics of their data and the assumptions they want to test.\n\n\n4.10.3 Expanding to non-normal disitributions\n\n\n\n\n\n\n\n\n\n\n\n(a) the QQ-plot for the weibull distribution using the drive shaft failure time data\n\n\n\n\n\n\n\n\n\n\n\n(b) a detrended QQ-plot\n\n\n\n\n\n\n\nFigure 4.15: The QQ-plot can easily be extended to non-normal distributions.\n\n\n\nThe QQ-plot can easily be extended to non-normal disitributions as well. This is shown in Figure 4.15. In Figure 4.15 (a) a classic QQ-plot for Figure 2.35 is shown. The same rules as before still apply, they are only extended to the weibull distribution. In Figure 4.15 (b) a detrended QQ-plot is shown in order to account for visual bias. It is of course known, that the data follows a weibull disitribution with a shape parameter \\(\\beta=2\\) and a scale parameter \\(\\lambda = 500\\), but such distributional parameters can also be estimated (Delignette-Muller and Dutang 2015).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#test-1-variable",
    "href": "chapter002_InferentialStatistics.html#test-1-variable",
    "title": "4  Inferential Statistics",
    "section": "4.11 Test 1 Variable",
    "text": "4.11 Test 1 Variable\n\n\n\n\n\n\n\n\nFigure 4.16: Statistical tests for one variable.\n\n\n\n\n\n\n4.11.1 One Proportion Test\n\n\n\n\nTable 4.5: The raw data for the proportion test.\n\n\n\n\n\n\n\n\n\nCategory\nCount\nTotal\nplt_lbl\n\n\n\n\nA\n35\n100\n35 counts 100 trials\n\n\nB\n20\n100\n20 counts 100 trials\n\n\n\n\n\n\n\n\n\n\nThe one proportion test is used on categorical data with a binary outcome, such as success or failure. Its prerequisite is having a known or hypothesized population proportion that the sample proportion shall be compared to. This test helps determine if the sample proportion significantly differs from the population proportion, making it valuable for studies involving proportions and percentages.\n\n\n\n\nTable 4.6: The test results for the proportion test.\n\n\n\n\n\n\n\n\n\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nalternative\n\n\n\n\n0.350\n0.200\n4.915\n0.027\n1.000\n0.018\n0.282\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n4.11.2 Chi2 goodness of fit test\n\n\n\n\n\n\nTable 4.7: The raw data for the gof \\(\\chi^2\\) test.\n\n\n\n\n\n\n\n\n\ngroup\ncount_n_observed\n\n\n\n\ngroup01\n100.000\n\n\ngroup02\n100.000\n\n\ngroup03\n100.000\n\n\ngroup04\n100.000\n\n\ngroup05\n100.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.8: The test results for the gof \\(\\chi^2\\) test.\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\n\n\n\n\n0.000\n1.000\n4.000\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(\\chi^2\\) goodness of Fit Test (gof) is applied on categorical data with expected frequencies. It is suitable for analyzing nominal or ordinal data. This test assesses whether there is a significant difference between the observed and expected frequencies in your dataset, making it useful for determining if the data fits an expected distribution.\n\n\n4.11.3 One-sample t-test\nThe one-sample t-test is designed for continuous data when you have a known or hypothesized population mean that you want to compare your sample mean to. It relies on the assumption of normal distribution, making it applicable when assessing whether a sample’s mean differs significantly from a specified population mean.\nThe test can be applied in various settings. One is, to test if measured data comes from a population with a certain mean (for exampe a test against a specification). To show the application, the drive shaft data is employed. In Table 4.9 the per group summarised data of the dirve shaft data is shown.\n\n\n\n\n\n\nTable 4.9: The raw data for the one sample t-test.\n\n\n\n\n\n\n\n\n\ngroup\nmean_diameter\nsd_diameter\n\n\n\n\ngroup01\n12.015\n0.111\n\n\ngroup02\n12.364\n0.189\n\n\ngroup03\n13.002\n0.102\n\n\ngroup04\n11.486\n0.094\n\n\ngroup05\n12.001\n0.026\n\n\n\n\n\n\n\n\n\n\n\nOne important prerequisite for the One sample t-test normally distributed data. For this, graphical and numerical methods have been introduced in previous chapters. First, a classic QQ-plot is created for every group (see Figure 4.17). From a first glance, the data appears to be normally distributed.\n\n\n\n\n\n\n\n\n\nFigure 4.17: The qq-plot for the drive shaft data\n\n\n\n\n\n\nA more quantitative approach to tests for normality is shown in Table 4.10. Here, each group is tested with the KS-test for normality. H0 is accepted (the data is normal distributed) because the computed p-value is larger than the significance level (\\(\\alpha  = 0.05\\)).\n\n\n\n\n\nTable 4.10: The results for the one KS normality test for each group.\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\ngroup\nstatistic\np.value\nmethod\nalternative\n\n\n\n\ngroup01\n0.048\n0.975\nAsymptoticone-sampleKolmogorov-Smirnovtest\ntwo-sided\n\n\ngroup02\n0.067\n0.754\nAsymptoticone-sampleKolmogorov-Smirnovtest\ntwo-sided\n\n\ngroup03\n0.075\n0.633\nAsymptoticone-sampleKolmogorov-Smirnovtest\ntwo-sided\n\n\ngroup04\n0.060\n0.862\nAsymptoticone-sampleKolmogorov-Smirnovtest\ntwo-sided\n\n\ngroup05\n0.127\n0.081\nAsymptoticone-sampleKolmogorov-Smirnovtest\ntwo-sided\n\n\n\n\n\n\n\n\n\n\n\nThere is sufficient evidence to assume normal distributed data within each group. The next step is, to test if the data comes from a certain population mean (\\(\\mu_0\\)). In this case, the population mean is the specification of the drive shaft at a diameter \\(=12mm\\).\n\n\n\n\n\nTable 4.11: The results for the one sample t-test (against mean = 12mm).\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\ngroup01\n12.015\n1.391\n0.167\n99.000\n11.993\n12.038\nOneSamplet-test\ntwo.sided\n\n\ngroup02\n12.364\n19.274\n0.000\n99.000\n12.326\n12.401\nOneSamplet-test\ntwo.sided\n\n\ngroup03\n13.002\n97.769\n0.000\n99.000\n12.982\n13.022\nOneSamplet-test\ntwo.sided\n\n\ngroup04\n11.486\n−54.441\n0.000\n99.000\n11.468\n11.505\nOneSamplet-test\ntwo.sided\n\n\ngroup05\n12.001\n0.418\n0.677\n99.000\n11.996\n12.006\nOneSamplet-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.11.4 One sample Wilcoxon test\nFor situations where your data may not follow a normal distribution or when dealing with ordinal data, the one-sample Wilcoxon test is a non-parametric alternative to the t-test. It is used to evaluate whether a sample’s median significantly differs from a specified population median.\nThe wear and tear of drive shafts can occur due to various factors related to the vehicle’s operation and maintenance. Some common causes include:\n\nNormal Usage: Over time, the drive shaft undergoes stress and strain during regular driving. This can lead to gradual wear on components, especially if the vehicle is frequently used.\nMisalignment: Improper alignment of the drive shaft can result in uneven distribution of forces, causing accelerated wear. This misalignment may stem from issues with the suspension system or other related components.\nLack of Lubrication: Inadequate lubrication of the drive shaft joints and bearings can lead to increased friction, accelerating wear. Regular maintenance, including proper lubrication, is essential to mitigate this factor.\nContamination: Exposure to dirt, debris, and water can contribute to the degradation of drive shaft components. Contaminants can infiltrate joints and bearings, causing abrasive damage over time.\nVibration and Imbalance: Excessive vibration or imbalance in the drive shaft can lead to increased stress on its components. This may result from issues with the balance of the rotating parts or damage to the shaft itself.\nExtreme Operating Conditions: Harsh driving conditions, such as off-road terrain or constant heavy loads, can accelerate wear on the drive shaft. The components may be subjected to higher levels of stress than they were designed for, leading to premature wear and tear.\n\nThe wear and tear because o the reasons above can be rated on a scale with discrete values from \\(1 \\ldots 5\\) with \\(2\\) being the reference value. It is therefore interesting, if the wear and tear rating of \\(n=100\\) drive shafts per group differs significantly from the reference value \\(2\\). Because we are dealing with discrete data, the one sample t-test can not be used.\n\n\n\n\n\n\n\n\n\n\nFigure 4.18: The wear and tear rating data histograms.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.12: The results for the one sample Wilcoxon test for every group against the reference value.\n\n\n\n\n  \n  \n\n\n\ngroup\nstatistic\np.value\nalternative\n\n\n\n\ngroup01\n3,208.500\n0.000\ngreater\n\n\ngroup02\n5,050.000\n0.000\ngreater\n\n\ngroup03\n0.000\n1.000\ngreater\n\n\ngroup04\n3,203.500\n0.000\ngreater\n\n\ngroup05\n3,003.000\n0.000\ngreater\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.13: The results for the one sample t-test compared to the results of a one sample Wilcoxon test.\n\n\n\n\n  \n  \n\n\n\ngroup\nt_tidy_p.value\nwilcox_tidy_p.value\n\n\n\n\ngroup01\n0.167\n0.182\n\n\ngroup02\n0.000\n0.000\n\n\ngroup03\n0.000\n0.000\n\n\ngroup04\n0.000\n0.000\n\n\ngroup05\n0.677\n0.803",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#test-2-variable-qualitative-or-quantitative",
    "href": "chapter002_InferentialStatistics.html#test-2-variable-qualitative-or-quantitative",
    "title": "4  Inferential Statistics",
    "section": "4.12 Test 2 Variable (Qualitative or Quantitative)",
    "text": "4.12 Test 2 Variable (Qualitative or Quantitative)\n\n\n\n\n\n\n\n\nFigure 4.19: Statistical tests for two variables.\n\n\n\n\n\n\n4.12.1 Cochrane’s Q-test\nCochran’s Q test is employed when you have categorical data with three or more related groups, often collected over time or with repeated measurements. It assesses if there is a significant difference in proportions between the related groups.\n\n\n4.12.2 Chi2 test of independence\nThis test is appropriate when you have two categorical variables, and you want to determine if there is an association between them. It is useful for assessing whether the two variables are dependent or independent of each other.\nIn the context of the drive shaft production the example assumes a dataset with categorical variables like “Defects” (Yes/No) and “Operator” (Operator A/B).\n\n4.12.2.1 Contingency tables\nA contingency table, also known as a cross-tabulation or crosstab, is a statistical table that displays the frequency distribution of variables. It organizes data into rows and columns to show the frequency or relationship between two or more categorical variables. Each cell in the table represents the count or frequency of occurrences that fall into a specific combination of categories for the variables being analyzed. It is commonly used in statistics to examine the association between categorical variables and to understand patterns within data sets.\n\n\n\n\nTable 4.14: The contingency table for this example.\n\n\n\n\n  \n  \n\n\n\nDefects\nOperator A\nOperator B\n\n\n\n\nNo\n2\n3\n\n\nYes\n3\n2\n\n\n\n\n\n\n\n\n\n\n\n\n4.12.2.2 test results\nWith \\(p\\approx1&gt;0.05\\) the \\(p\\)-value is greater than the significance level of \\(\\alpha = 0.05\\). The \\(H_0\\) is therefore proven, there is no difference between the operators. The test results are depicted below-\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_table\nX-squared = 0, df = 1, p-value = 1\n\n\n\n\n\n4.12.3 Correlation\n\n\n\n\n\n\n\n\nFigure 4.20: Correlation between two variables and the quantification thereof.\n\n\n\n\n\nCorrelation refers to a statistical measure that describes the relationship between two variables. It indicates the extent to which changes in one variable are associated with changes in another.\nCorrelation is measured on a scale from -1 to 1:\n\nA correlation of 1 implies a perfect positive relationship, where an increase in one variable corresponds to a proportional increase in the other.\nA correlation of -1 implies a perfect negative relationship, where an increase in one variable corresponds to a proportional decrease in the other.\nA correlation close to 0 suggests a weak or no relationship between the variables.\n\nCorrelation doesn’t imply causation; it only indicates that two variables change together but doesn’t determine if one causes the change in the other.\n\n4.12.3.1 Pearson Corrrelation\n\n\n\nThe pearson correlation coefficient is a normalized version of the covariance.\n\n\n\n\\[\\begin{align}\nR = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_x \\sigma_y}\n\\end{align}\\]\n\nCovariance is sensitive to scale (\\(mm\\) vs. \\(cm\\))\nPearson correlation removes units, allowing for meaningful comparisons across datasets\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.21: The QQ-plot of both variables. There is strong evidence that they are normally distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.22: Correlation between rpm of lathe machine and the diameter of the drive shaft.\n\n\n\n\n\n\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  drive_shaft_rpm_dia$rpm and drive_shaft_rpm_dia$diameter\nt = 67.895, df = 498, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9406732 0.9578924\nsample estimates:\n cor \n0.95 \n\n\n\n\nWhen you have two continuous variables and want to measure the strength and direction of their linear relationship, Pearson correlation is the go-to choice (Pearson 1895). It assumes normally distributed data and is particularly valuable for exploring linear associations between variables and is calculated via \\(\\eqref{pearcorr}\\).\n\\[\\begin{align}\nR = \\frac{\\sum_{i = 1}^{n}(x_i - \\bar{x}) \\times (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\times \\sum_{i=1}^{n}(y_i-\\bar{y})^2} \\label{pearcorr}\n\\end{align}\\]\nThe Pearson Correlation Coeffcient works best with normal disitributed data. The normal distribution of the data is verified in Figure 4.21.\n\n\n4.12.3.2 Spearman Correlation\nSpearman (Spearman 1904) correlation is a non-parametric alternative to Pearson correlation. It is used when the data is not normally distributed or when the relationship between variables is monotonic but not necessarily linear.\n\\[\\begin{align}\n\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)} \\label{spearcorr}\n\\end{align}\\]\nIn Figure 4.23 the example data for a drive shaft production is shown. The Production_Time and the Defects seem to form a relationship, but the data does not appear to be normally distributed. This can also be seen in the QQ-plots of both variables in Figure 4.24.\n\n\nThe spearman correlation coefficient (\\(\\rho\\)) is based on the pearson correlation, but applied to ranked data\n\n\n\n\n\n\n\n\n\n\nFigure 4.23: The relationship between the production time and the number of defects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.24: The QQ-plots of both variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.12.3.3 Correlation - methodogical limits\nWhile correlation analysis and summary statistics are certainly useful, one must always consider the raw data. The data taken from Davies, Locke, and D’Agostino McGowan (2022) showcases this. The summary statistics in Table 4.15 are practically the same, one would not suspect different underlying data. When the raw data is plotted though (Figure 4.25), it can be seen that the data appears to be highly non linear, forming different shapes as well as different categories etc.\nAlways check the raw data.\n\n\n\n\n\n\nTable 4.15: The datasauRus data and the respective summary statistics.\n\n\n\n\n  \n  \n\n\n\ndataset\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\n\naway\n54.266\n47.835\n16.770\n26.940\n−0.064\n\n\nbullseye\n54.269\n47.831\n16.769\n26.936\n−0.069\n\n\ncircle\n54.267\n47.838\n16.760\n26.930\n−0.068\n\n\ndino\n54.263\n47.832\n16.765\n26.935\n−0.064\n\n\ndots\n54.260\n47.840\n16.768\n26.930\n−0.060\n\n\nh_lines\n54.261\n47.830\n16.766\n26.940\n−0.062\n\n\nhigh_lines\n54.269\n47.835\n16.767\n26.940\n−0.069\n\n\nslant_down\n54.268\n47.836\n16.767\n26.936\n−0.069\n\n\nslant_up\n54.266\n47.831\n16.769\n26.939\n−0.069\n\n\nstar\n54.267\n47.840\n16.769\n26.930\n−0.063\n\n\nv_lines\n54.270\n47.837\n16.770\n26.938\n−0.069\n\n\nwide_lines\n54.267\n47.832\n16.770\n26.938\n−0.067\n\n\nx_shape\n54.260\n47.840\n16.770\n26.930\n−0.066\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.25: The raw data from the datasauRus packages shows, that summary statistics may be misleading.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#test-2-variables-2-groups",
    "href": "chapter002_InferentialStatistics.html#test-2-variables-2-groups",
    "title": "4  Inferential Statistics",
    "section": "4.13 Test 2 Variables (2 Groups)",
    "text": "4.13 Test 2 Variables (2 Groups)\n\n\n\n\n\n\n\n\nFigure 4.26: Statistical tests for two variable.\n\n\n\n\n\n\n4.13.1 Test for equal variance (homoscedasticity)\n\n\n\n\n\n\n\n\nFigure 4.27: The variances (\\(sd^2\\)) for the drive shaft data.\n\n\n\n\n\nTests for equal variances, also known as tests for homoscedasticity, are used to determine if the variances of two or more groups or samples are equal. Equal variances are an assumption in various statistical tests, such as the t-test and analysis of variance (ANOVA). When the variances are not equal, it can affect the validity of these tests. Two common tests for equal variances are:\nCertainly, here are bullet points outlining the null hypothesis, prerequisites, and decisions for each of the three tests:\n\n4.13.1.1 F-Test (Hahs-Vaughn and Lomax 2013)\n\n\n\nNull Hypothesis: The variances of the different groups or samples are equal.\nPrerequisites:\n\nIndependence\nNormality\nNumber of groups \\(= 2\\)\n\n\n\n\nDecisions:\n\n\\(p&gt; \\alpha \\rightarrow\\) fail to reject H0\n\\(p&lt; \\alpha \\rightarrow\\) reject H0\n\n\n\n\n\n\n\n    F test to compare two variances\n\ndata:  ds_wide$group01 and ds_wide$group03\nF = 1.1817, num df = 99, denom df = 99, p-value = 0.4076\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.7951211 1.7563357\nsample estimates:\nratio of variances \n          1.181736 \n\n\n\n\n4.13.1.2 Bartlett Test (Bartlett 1937)\n\n\n\nNull Hypothesis: The variances of the different groups or samples are equal.\nPrerequisites:\n\nIndependence\nNormality\nNumber of groups \\(&gt; 2\\)\n\n\n\n\nDecisions:\n\n\\(p&gt; \\alpha \\rightarrow\\) fail to reject H0\n\\(p&lt; \\alpha \\rightarrow\\) reject H0\n\n\n\n\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  diameter by group\nBartlett's K-squared = 275.61, df = 4, p-value &lt; 2.2e-16\n\n\n\n\n4.13.1.3 Levene Test (Olkin June)\n\n\n\nNull Hypothesis: The variances of the different groups or samples are equal.\nPrerequisites:\n\nIndependence\nNumber of groups \\(&gt; 2\\)\n\n\n\n\nDecisions:\n\n\\(p&gt; \\alpha \\rightarrow\\) fail to reject H0\n\\(p&lt; \\alpha \\rightarrow\\) reject H0\n\n\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   4  38.893 &lt; 2.2e-16 ***\n      495                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n4.13.2 t-test for independent samples\nThe independent samples t-test is applied when you have continuous data from two independent groups. It evaluates whether there is a significant difference in means between these groups, assuming a normal distribution of the data.\n\n\n\nNull Hypothesis: The means of the two samples are equal.\nPrerequisites:\n\nIndependence\nNormal Distribution\nNumber of groups \\(=2\\)\nequal Variances of the groups\n\n\n\n\nFirst, the variances are compared in order to check if they are equal using the F-Test (as described in Section 4.13.1.1).\n\n\n\n    F test to compare two variances\n\ndata:  group01 %&gt;% pull(\"diameter\") and group03 %&gt;% pull(\"diameter\")\nF = 1.1817, num df = 99, denom df = 99, p-value = 0.4076\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.7951211 1.7563357\nsample estimates:\nratio of variances \n          1.181736 \n\n\nWith \\(p&gt;\\alpha = 0.05\\) the \\(H_0\\) is accepted, the variances are equal.\n\n\nThe next step is to check the data for normality using the KS-test (as described in Section 4.10.2).\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  group01 %&gt;% pull(\"diameter\")\nD = 0.048142, p-value = 0.9746\nalternative hypothesis: two-sided\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  group03 %&gt;% pull(\"diameter\")\nD = 0.074644, p-value = 0.6332\nalternative hypothesis: two-sided\n\n\nWith \\(p&gt;\\alpha = 0.05\\) the \\(H_0\\) is accepted, the data seems to be normally distributed.\n\n\n\n\n\n\n\n\n\n\nFigure 4.28: The data within the two groups for comparing the sample means using the t-test for independent samples.\n\n\n\n\n\n\n\nThe formal test is then carried out. With \\(p&lt;\\alpha=0.05\\) \\(H_0\\) is rejected, the data comes from populations with different means.\n\n\n\n    Two Sample t-test\n\ndata:  group01 %&gt;% pull(diameter) and group03 %&gt;% pull(diameter)\nt = -65.167, df = 198, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.0164554 -0.9567446\nsample estimates:\nmean of x mean of y \n  12.0155   13.0021 \n\n\n\n\n\n\n4.13.3 Welch t-test for independent samples\nSimilar to the independent samples t-test, the Welch t-test is used for continuous data with two independent groups (WELCH 1947). However, it is employed when there are unequal variances between the groups, relaxing the assumption of equal variances in the standard t-test.\n\n\n\nNull Hypothesis: The means of the two samples are equal.\nPrerequisites:\n\nIndependence\nNormal Distribution\nNumber of groups \\(=2\\)\n\n\n\n\nFirst, the variances are compared in order to check if they are equal using the F-Test (as described in Section 4.13.1.1).\n\n\n\n    F test to compare two variances\n\ndata:  group01 %&gt;% pull(\"diameter\") and group02 %&gt;% pull(\"diameter\")\nF = 0.34904, num df = 99, denom df = 99, p-value = 3.223e-07\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2348504 0.5187589\nsample estimates:\nratio of variances \n         0.3490426 \n\n\nWith \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) is rejected and \\(H_a\\) is accepted. The variances are different.\n\n\nUsing the KS-test (see Section 4.10.2) the data is checked for normality.\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  group01 %&gt;% pull(\"diameter\")\nD = 0.048142, p-value = 0.9746\nalternative hypothesis: two-sided\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  group02 %&gt;% pull(\"diameter\")\nD = 0.067403, p-value = 0.7539\nalternative hypothesis: two-sided\n\n\nWith \\(p&gt;\\alpha = 0.05\\) \\(H_0\\) is accepted, the data seems to be normally distributed.\n\n\n\n\n\n\n\n\n\n\nFigure 4.29: The data within the two groups for comparing the sample means using the Welch-test for independent samples.\n\n\n\n\n\n\n\nThen, the formal test is carried out.\n\n\n\n    Welch Two Sample t-test\n\ndata:  group01 %&gt;% pull(diameter) and group02 %&gt;% pull(diameter)\nt = -15.887, df = 160.61, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3912592 -0.3047408\nsample estimates:\nmean of x mean of y \n  12.0155   12.3635 \n\n\nWith \\(p&lt;\\alpha = 0.05\\) we reject \\(H_0\\), the data seems to be coming from different population means, even though the variances are overlapping (and different).\n\n\n\n\n4.13.4 Mann-Whitney U test\nFor non-normally distributed data or small sample sizes, the Mann-Whitney U test serves as a non-parametric alternative to the independent samples t-test (Mann and Whitney 1947). It assesses whether there is a significant difference in medians between two independent groups.\n\n\n\nNull Hypothesis: The medians of the two samples are equal.\nPrerequisites:\n\nIndependence\nno specific distribution (non-parametric)\nNumber of groups \\(=2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.30: The data within the two groups for comparing the sample medians using the Mann-Whitney-U Test.\n\n\n\n\n\n\n\nThis time a graphical method to check for normality is employed (QQ-plot, see Section 4.10.1). From the Figure 4.31 it is pretty clear, that the data is not normally distributed. Furthermore, the variances seem to be unequal as well.\n\n\n\n\n\n\n\n\nFigure 4.31: The data within the two groups for comparing the sample medians using the Mann-Whitney-U Test.\n\n\n\n\n\n\n\nThen, the formal test is carried out. With \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) is rejected, the true location shift is not equal to \\(0\\).\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  diameter by group\nW = 7396, p-value = 4.642e-09\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n4.13.5 t-test for paired samples\nThe paired samples t-test is suitable when you have continuous data from two related groups or repeated measures. It helps determine if there is a significant difference in means between the related groups, assuming normally distributed data.\n\n\n\nNull Hypothesis: True mean difference is not equal to 0.\nPrerequisites:\n\nPaired Data\nNormal Distribution\nequal variances\nNumber of groups \\(=2\\)\n\n\n\n\nUsing the F-Test, the variances are compared.\n\n\n\n    F test to compare two variances\n\ndata:  diameter by timepoint\nF = 1, num df = 9, denom df = 9, p-value = 1\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2483859 4.0259942\nsample estimates:\nratio of variances \n                 1 \n\n\nWith \\(p&gt;\\alpha = 0.05\\) \\(H_0\\) is accepted, the variances are equal.\n\n\nUsing a QQ-plot the data is checked for normality.\n\n\n\n\n\n\n\n\n\nWithout a formal test, the data is assumed to be normally distributed.\n\n\n\n\n\n\n\n\n\n\nFigure 4.32: A boxplot of the data, showing the connections between the datapoints.\n\n\n\n\n\n\n\nThe formal test is then carried out.\n\n\n# A tibble: 1 × 8\n  .y.      group1 group2    n1    n2 statistic    df           p\n* &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 diameter t0     t1        10    10     -13.4     9 0.000000296\n\n\nWith \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) is rejected, the treatment changed the properties of the product.\n\n\n\n\n4.13.6 Wilcoxon signed rank test\nFor non-normally distributed data or situations involving paired samples, the Wilcoxon signed rank test is a non-parametric alternative to the paired samples t-test. It evaluates whether there is a significant difference in medians between the related groups.\n\n\n\nNull Hypothesis: True mean difference is not equal to 0.\nPrerequisites:\n\nPaired Data\nNumber of groups \\(=2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 7\n  .y.      group1 group2    n1    n2 statistic       p\n* &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 diameter t0     t1        20    20        25 0.00169",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#test-2-variables-2-groups-1",
    "href": "chapter002_InferentialStatistics.html#test-2-variables-2-groups-1",
    "title": "4  Inferential Statistics",
    "section": "4.14 Test 2 Variables (> 2 Groups)",
    "text": "4.14 Test 2 Variables (&gt; 2 Groups)\n\n\n\n\n\n\n\n\nFigure 4.33: Statistical tests for one variable.\n\n\n\n\n\n\n4.14.1 Analysis of Variance (ANOVA) - Basic Idea\nANOVA’s ability to compare multiple groups or factors makes it widely applicable across diverse fields for analyzing variance and understanding relationships within data. In the context of engineering sciences the application of ANOVA include:\n\nExperimental Design and Analysis: Engineers often conduct experiments to optimize processes, test materials, or evaluate designs. ANOVA aids in analyzing these experiments by assessing the effects of various factors (like temperature, pressure, or material composition) on the performance of systems or products. It helps identify significant factors and their interactions to improve engineering processes.\nProduct Testing and Reliability: Engineers use ANOVA to compare the performance of products manufactured under different conditions or using different materials. This analysis helps ensure product reliability by identifying which factors significantly impact product quality, durability, or functionality.\nProcess Control and Improvement: ANOVA plays a crucial role in quality control and process improvement within engineering. It helps identify variations in manufacturing processes, such as assessing the impact of machine settings or production methods on product quality. By understanding these variations, engineers can make informed decisions to optimize processes and minimize defects.\nSupply Chain and Logistics: In engineering logistics and supply chain management, ANOVA aids in analyzing the performance of different suppliers or transportation methods. It helps assess variations in delivery times, costs, or product quality across various suppliers or logistical approaches.\nSimulation and Modeling: In computational engineering, ANOVA is used to analyze the outputs of simulations or models. It helps understand the significance of different input variables on the output, enabling engineers to refine models and simulations for more accurate predictions.\n\n\n\n\n\n\n\n\n\nFigure 4.34: The basic idea of an ANOVA.\n\n\n\n\n\nAcross such fields ANOVA is often used to:\nComparing Means: ANOVA is employed when comparing means between three or more groups. It assesses whether there are statistically significant differences among the means of these groups. For instance, in an experiment testing the effect of different fertilizers on plant growth, ANOVA can determine if there’s a significant difference in growth rates among the groups treated with various fertilizers.\nModeling Dependencies: ANOVA can be extended to model dependencies among variables in more complex designs. For instance, in factorial ANOVA, it’s used to study the interaction effects among multiple independent variables on a dependent variable. This allows researchers to understand how different factors might interact to influence an outcome.\nMeasurement System Analysis (MSA): ANOVA is integral in MSA to evaluate the variation contributed by different components of a measurement system. In assessing the reliability and consistency of measurement instruments or processes, ANOVA helps in dissecting the total variance into components attributed to equipment variation, operator variability, and measurement error.\nAs with statistical tests before, the applicability of the ANOVA depends on various factors.\n\n4.14.1.1 Sum of squared error (SSE)\nThe sum of squared errors is a statistical measure used to assess the goodness of fit of a model to its data. It is calculated by squaring the differences between the observed values and the values predicted by the model for each data point, then summing up these squared differences. The SSE indicates the total variability or dispersion of the observed data points around the fitted regression line or model. Lower SSE values generally indicate a better fit of the model to the data.\n\\[\\begin{align}\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\label{sse}\n\\end{align}\\]\n\n\n\n\n\n\n\n\nFigure 4.35: A graphical depiction of the SSE.\n\n\n\n\n\n\n\n4.14.1.2 Mean squared error (MSE)\nThe mean squared error is a measure used to assess the average squared difference between the predicted and actual values in a dataset. It is frequently employed in regression analysis to evaluate the accuracy of a predictive model. The MSE is calculated by taking the average of the squared differences between predicted values and observed values. A lower MSE indicates that the model’s predictions are closer to the actual values, reflecting better accuracy.\n\n\\[\\begin{align}\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\label{mse}\n\\end{align}\\]\n\n\n\n\n4.14.2 One-way ANOVA\nThe one-way analysis of variance (ANOVA) is used for continuous data with three or more independent groups. It assesses whether there are significant differences in means among these groups, assuming a normal distribution.\n\n\n\nNull Hypothesis: True mean difference is equal to 0.\nPrerequisites:\n\nequal variances\nNumber of groups \\(&gt;2\\)\nOne response, one predictor variable\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.36: The basic idea of a One-way ANOVA.\n\n\n\n\n\n\n\nThe most important prerequisite for a One-way ANOVA are equal variances. Because there are more than two groups, the Bartlett test (as introduced in Section 4.13.1.2) is chosen (data is normally distributed).\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  diameter by group\nBartlett's K-squared = 275.61, df = 4, p-value &lt; 2.2e-16\n\n\n\n\nBecause \\(p&lt;\\alpha = 0.05\\) the variances are different.\n\n\n\n\n\n\n\n\n\n\nFigure 4.37: The groups with equal variance are highlighted.\n\n\n\n\n\n\n\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  diameter by group\nBartlett's K-squared = 2.7239, df = 2, p-value = 0.2562\n\n\n\n\nWith \\(p&gt;\\alpha=0.05\\) \\(H_0\\) is accepted, the variances of group01, group02 and group03 are equal.\n\n\nOf course, many software package provide an automated way of performing a One-way ANOVA, but the first will be explained in detail. The general model for a One-way ANOVA is shown in \\(\\eqref{onewayanova}\\).\n\\[\\begin{align}\nY \\sim X + \\epsilon \\label{onewayanova}\n\\end{align}\\]\n\n\\(H_0\\): All population means are equal.\n\\(H_a\\): Not all population means are equal.\n\nFor a One-way ANOVA the predictor variable \\(X\\) is the mean (\\(\\bar{x}\\)) of all datapoints \\(x_i\\).\n\n\nFirst the SSE and the MSE is calculated for the complete model (\\(H_a\\) is true), see Table 4.16. The complete model means, that every mean, for every group is calculated and the \\(SSE\\) according to \\(\\eqref{sse}\\) is calculated.\n\n\n\n\n\n\n\n\nFigure 4.38: Computation of error for the complete model (mean per group as model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.39: Computation of error for the reduced model (overall mean as model)\n\n\n\n\n\n\n\n\n\n\n\nTable 4.16: The SSE and MSE for the complete model.\n\n\n\n\n  \n  \n\n\n\nsse\ndf\nn\np\nmse\n\n\n\n\n3.150\n297.000\n300.000\n3.000\n0.011\n\n\n\n\n\n\n\n\n\n\n\n\nThen, the SSE and the MSE is calculated for the reduced model (\\(H_0\\) is true). In the reduced model, the mean is not calculated per group, the overall mean is calculated (results in Table 4.17).\n\n\n\n\nTable 4.17: The SSE and MSE from the reduced model.\n\n\n\n\n  \n  \n\n\n\nsse\ndf\nn\np\nmse\n\n\n\n\n121.506\n299.000\n300.000\n1.000\n0.406\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(SSE\\), \\(df\\) and \\(MSE\\) explained by the complete model are calculated:\n\\[\\begin{align}\nSSE_{explained} &= SSE_{reduced}-SSE_{complete} = 118.36 \\\\\ndf_{explained} &= df_{reduced} - df_{complete} = 2 \\\\\nMSE_{explained} &= \\frac{SSE_{explained}}{df_{explained}} = 59.18\n\\end{align}\\]\n\n\nThe ratio of the variance (MSE) as explained by the complete model to the reduced model is then calculated. The probability of this statistic is afterwards calculated (if \\(H_0\\) is true).\n\n\n[1] 2.762026e-236\n\n\nThe probability of a F-statistic with \\(pf = 5579.207\\) is \\(0\\).\n\n\nA crosscheck with a automated solution (aov-function) yields the results shown in Table 4.18.\n\n\n\n\nTable 4.18: The ANOVA results from the aov function.\n\n\n\n\n  \n  \n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ngroup\n2.000\n118.356\n59.178\n5,579.207\n0.000\n\n\nResiduals\n297.000\n3.150\n0.011\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\nSome sanity checks are of course required to ensure the validity of the results. First, the variance of the residuals must be equal along the groups (see Figure 4.40).\n\n\n\n\n\n\n\n\nFigure 4.40: The variances of the residuals.\n\n\n\n\n\n\n\nAlso, the residuals from the model must be normally distributed (see Figure 4.41).\n\n\n\n\n\n\n\n\nFigure 4.41: The distribution of the residuals.\n\n\n\n\n\n\n\nThe model seems to be valid (equal variances of residuals, normal distributed residuals).\nWith \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) can be rejected, the means come from different populations.\n\n\n\n\n4.14.3 Welch ANOVA\nWelch ANOVA: Similar to one-way ANOVA, the Welch ANOVA is employed when there are unequal variances between the groups being compared. It relaxes the assumption of equal variances, making it suitable for situations where variance heterogeneity exists.\n\n\n\nNull Hypothesis: True mean difference is not equal to 0.\nPrerequisites:\n\nNumber of groups \\(&gt;2\\)\nOne response, one predictor variable\n\n\n\n\nThe Welch ANOVA drops the prerequisite of equal variances in groups. Because there are more than two groups, the Bartlett test (as introduced in Section 4.13.1.2) is chosen (data is normally distributed).\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  diameter by group\nBartlett's K-squared = 275.61, df = 4, p-value &lt; 2.2e-16\n\n\nWith \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) can be rejected, the variances are not equal.\n\n\nThe ANOVA table for the Welch ANOVA is shown in Table 4.19.\n\n\n\n\nTable 4.19: The ANOVA results from the ANOVA Welch Test (not assuming equal variances).\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\nnum.df\nden.df\nstatistic\np.value\nmethod\n\n\n\n\n4.000\n215.085\n3,158.109\n0.000\nOne-way analysis ofmeans (not assuming equalvariances)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.14.4 Kruskal Wallis\n\nKruskal-Wallis Test: When dealing with non-normally distributed data, the Kruskal-Wallis test is a non-parametric alternative to one-way ANOVA. It is used to evaluate whether there are significant differences in medians among three or more independent groups.\nIn this example the drive strength is measured using three-point bending. Three different methods are employed to increase the strength of the drive shaft.\n\n\n\n\n\n\n\n\n\nFigure 4.42: The mechanical Background for a three-point bending test\n\n\n\n\n\n\nMethod A: baseline material\nMethod B: different geometry\nMethod C: different material\n\n\n\nIn Figure 4.43 the raw drive shaft strength data for Method A, B and C is shown. At first glance, the data does not appear to be normally distributed.\n\n\n\n\n\n\n\n\nFigure 4.43: The raw data from the drive shaft strength testing.\n\n\n\n\n\n\n\nIn Figure 4.44 the visual test for normal distribution is performed. The data does not appear to be normally distributed.\n\n\n\n\n\n\n\n\nFigure 4.44: The qq-plot for the drive shaft strength testing data.\n\n\n\n\n\n\n\nThe Kruskal-Wallis test is then carried out. With \\(p&lt; \\alpha = 0.05\\) it is shown, that the groups come from populations with different means. The next step is to find which of the groups are different using a post-hoc analysis.\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  strength by group\nKruskal-Wallis chi-squared = 107.65, df = 2, p-value &lt; 2.2e-16\n\n\n\n\nThe Kruskal-Wallis Test (as the ANOVA) can only tell you, if there is a signifcant difference between the groups, not what groups are different. Post-hoc tests are able to determine such, but must be used with a correction for multiple testing (see (Tamhane 1977))\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  kw_shaft_data$strength and kw_shaft_data$group \n\n         Method_A Method_B\nMethod_B &lt; 2e-16  -       \nMethod_C 6.8e-14  2.0e-10 \n\nP value adjustment method: bonferroni \n\n\nBecause \\(p&lt;\\alpha = 0.05\\) it can be concluded, that all means are different from each other.\n\n\n\n\n4.14.5 repeated measures ANOVA\n\n\nRepeated Measures ANOVA: The repeated measures ANOVA is applicable when you have continuous data with multiple measurements within the same subjects or units over time. It is used to assess whether there are significant differences in means over the repeated measurements, under the assumptions of sphericity and normal distribution.\nIn this example, the diameter of \\(n = 20\\) drive shafts is measured after three different steps.\n\nBefore Machining\nAfter Machining\nAfter Inspection\n\n\n\n\n\n\n\n\n\nFigure 4.45: The raw data for the repeated measures ANOVA.\n\n\n\n\n\n\n\nFirst, outliers are identified. There is no strict rule to identify outliers, in this case a classical measure is applied according to \\(\\eqref{outlierrule}\\)\n\\[\\begin{align}\n\\text{outlier} &=\n\\begin{cases}\nx_i & &gt;Q3 + 1.5 \\cdot IQR \\\\\nx_i & &lt;Q1 - 1.5 \\cdot IQR\n\\end{cases}\n\\label{outlierrule}\n\\end{align}\\]\n\n\n# A tibble: 1 × 5\n  timepoint        Subject_ID diameter is.outlier is.extreme\n  &lt;chr&gt;            &lt;fct&gt;         &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 After_Inspection 15             12.9 TRUE       FALSE     \n\n\n\n\nA check for normality is done employing the Shapiro-Wilk test (Shapiro and Wilk 1965).\n\n\n\n  \n  \n\n\n\ntimepoint\nvariable\nstatistic\np\n\n\n\n\nAfter_Inspection\ndiameter\n0.968\n0.727\n\n\nAfter_Machining\ndiameter\n0.954\n0.456\n\n\nBefore_Machining\ndiameter\n0.968\n0.741\n\n\n\n\n\n\n\n\n\nThe next step is to check the dataset for sphericity, meaning to compare the variance of the groups among each other in order to determine the equality thereof. For this the Mauchly Test for sphericity is employed (Mauchly 1940).\n\n\n     Effect     W     p p&lt;.05\n1 timepoint 0.927 0.524      \n\n\nWith \\(p&gt;\\alpha = 0.05\\) \\(H_0\\) is accepted, the variances are equal. Otherwise sphericity corrections must be applied (Greenhouse and Geisser 1959).\n\n\nThe next step is to perform the repeated measures ANOVA, which yields the following results.\n\n\n\n  \n  \n\n\n\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\ntimepoint\n2.000\n36.000\n18.081\n0.000\n*\n0.444\n\n\n\n\n\n\n\nWith \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) is rejected, the different timepoints yield different diameters. Which groups are different is then determined using a post-hoc test, including a correction for the significance level (Bonferroni 1936).\n\n\nIn this case, the assumptions for a t-test are met, the pairwise t-test can be used.\n\n\n\n  \n  \n\n\n\ngroup1\ngroup2\nn1\nn2\nstatistic\ndf\np\np.adj\nsignif\n\n\n\n\nAfter_Inspection\nAfter_Machining\n19\n19\n0.342\n18\n0.736\n1.000\nns\n\n\nAfter_Inspection\nBefore_Machining\n19\n19\n−4.803\n18\n0.000\n0.000\n***\n\n\nAfter_Machining\nBefore_Machining\n19\n19\n−6.283\n18\n0.000\n0.000\n****\n\n\n\n\n\n\n\nwith \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) is rejected for the comparison Before_Machining - After_Machining and After_Inspection - Before_Machining. It can therefore be concluded that the machining has a significant influence on the diameter, whereas the inspection has none.\n\n\n\n\n4.14.6 Friedman test\n\n\nThe Friedman test is a non-parametric alternative to repeated measures ANOVA (Friedman 1937). It is utilized when dealing with non-normally distributed data and multiple measurements within the same subjects. This test helps determine if there are significant differences in medians over the repeated measurements.\nThe same data as for the repeated measures ANOVA will be used.\n\n\n\n  \n  \n\n\n\n.y.\nn\nstatistic\ndf\np\nmethod\n\n\n\n\ndiameter\n20.000\n16.900\n2.000\n0.000\nFriedman test\n\n\n\n\n\n\n\nWith \\(p&lt;\\alpha = 0.05\\) \\(H_0\\) is rejected, the timepoints play a vital role for the drive shaft parameter.\n\n\n\n\n\n\nBartlett, Maurice Stevenson. 1937. “Properties of Sufficiency and Statistical Tests.” Proceedings of the Royal Society of London. Series A - Mathematical and Physical Sciences 160 (901): 268–82. https://doi.org/10.1098/rspa.1937.0109.\n\n\nBonferroni, C. E. 1936. Teoria Statistica Delle Classi e Calcolo Delle Probabilità. Pubblicazioni Del r. Istituto Superiore Di Scienze Economiche e Commerciali Di Firenze. Seeber. https://books.google.de/books?id=3CY-HQAACAAJ.\n\n\nChampely, Stephane. 2020. Pwr: Basic Functions for Power Analysis. https://CRAN.R-project.org/package=pwr.\n\n\nCohen, Jacob. 2013. Statistical Power Analysis for the Behavioral Sciences. Routledge. https://doi.org/10.4324/9780203771587.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDelignette-Muller, Marie Laure, and Christophe Dutang. 2015. “fitdistrplus: An R Package for Fitting Distributions.” Journal of Statistical Software 64 (4): 1–34. https://doi.org/10.18637/jss.v064.i04.\n\n\nFriedman, Milton. 1937. “The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance.” Journal of the American Statistical Association 32 (December): 675–701. https://doi.org/10.1080/01621459.1937.10503522.\n\n\nGreenhouse, Samuel W., and Seymour Geisser. 1959. “On Methods in the Analysis of Profile Data.” Psychometrika 24 (June): 95–112. https://doi.org/10.1007/bf02289823.\n\n\nHahs-Vaughn, Debbie L., and Richard G. Lomax. 2013. An Introduction to Statistical Concepts. Routledge. https://doi.org/10.4324/9780203137819.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nMann, H. B., and D. R. Whitney. 1947. “On a Test of Whether One of Two Random Variables Is Stochastically Larger Than the Other.” The Annals of Mathematical Statistics. https://doi.org/10.1214/aoms/1177730491.\n\n\nMauchly, John W. 1940. “Significance Test for Sphericity of a Normal n-Variate Distribution.” The Annals of Mathematical Statistics 11 (2): 204–9. http://www.jstor.org/stable/2235878.\n\n\nNuzzo, Regina. 2014. “Scientific Method: Statistical Errors.” Nature 506 (7487): 150–52. https://doi.org/10.1038/506150a.\n\n\nOlkin, Ingram. June. Contributions to Probability and Statistics. Stanford Univ Pr.\n\n\nPearson, Karl. 1895. “Note on Regression and Inheritance in the Case of Two Parents.” Proceedings of the Royal Society of London Series I 58: 240–42.\n\n\nShapiro, S. S., and M. B. Wilk. 1965. “An Analysis of Variance Test for Normality (Complete Samples).” Biometrika 52 (December): 591–611. https://doi.org/10.1093/biomet/52.3-4.591.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” The American Journal of Psychology. https://doi.org/10.2307/1412159.\n\n\nTamhane, Ajit C. 1977. “Multiple Comparisons in Model i One-Way Anova with Unequal Variances.” Communications in Statistics - Theory and Methods 6 (January): 15–32. https://doi.org/10.1080/03610927708827466.\n\n\nWELCH, B. L. 1947. “The Generalization of \"STUDENT’s\" Problem When Several Different Population Variances Are Involved.” Biometrika. https://doi.org/10.1093/biomet/34.1-2.28.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter002_InferentialStatistics.html#footnotes",
    "href": "chapter002_InferentialStatistics.html#footnotes",
    "title": "4  Inferential Statistics",
    "section": "",
    "text": "A quantile is a statistical concept used to divide a dataset into equal-sized subsets or intervals.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chapter003_Regression.html",
    "href": "chapter003_Regression.html",
    "title": "5  Regression Analysis",
    "section": "",
    "text": "5.1 Linear Regression\nRegression analysis is a statistical method used to examine the relationship between one dependent variable and one or more independent variables. It aims to understand how the dependent variable changes when one or more independent variables change.\nThe core idea is to create a mathematical model that represents this relationship. The model is typically in the form of an equation that predicts the value of the dependent variable based on the values of the independent variables.\nThere are different types of regression analysis, such as linear regression (when the relationship between variables is linear) and nonlinear regression (when the relationship is not linear). The process involves finding the best-fitting line or curve that minimizes the differences between the predicted values from the model and the actual observed values.\n\\[\\begin{align}\ny = \\beta_0 + \\beta_1 \\cdot X \\label{linreg}\n\\end{align}\\]\nFigure 5.1: The basic idea behind linear regression.\nThe basic idea behind linear regression is, to find the line of the form \\(Y = \\beta_0 + \\beta_1 \\cdot X\\) that best fits the datapoints. In order to determine the best fit, a criterion to optimize for is needed. This is where residuals come into play.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapter003_Regression.html#linear-regression",
    "href": "chapter003_Regression.html#linear-regression",
    "title": "5  Regression Analysis",
    "section": "",
    "text": "5.1.1 Residuals\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: The calculation of residuals.\n\n\n\n\n\n\n\nThe computation of the residuals is based on \\(\\eqref{rss}\\) to the residual sum of squares.\n\\[\\begin{align}\nRSS = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_1 x_i+\\beta_0))^2 \\label{rss}\n\\end{align}\\]\n\n\n\n\n5.1.2 Gradient Descent (Ruder 2016)\n\n\n\n\n\n\n\n\nFigure 5.3: An example for the gradient descent algorithm\n\n\n\n\n\nIn linear regression, gradient descent is an iterative optimization process used to minimize the difference between predicted and actual values. It starts with initial coefficients and calculates the gradient of the cost function, representing the error. The coefficients are then updated in the opposite direction of the gradient, with the magnitude of the update controlled by a learning rate. This process is repeated until convergence, gradually refining the coefficients to improve the accuracy of the linear regression model.\n\n\n5.1.3 Model Evaluation and Interpretation\n\n\n\n\n\n\n\n\n\n\nFigure 5.4: The linear regression between rounds per minute (rpm) of the lathing machine and the diameter of the drive shaft.\n\n\n\n\n\n\n\nThe coefficient of determination (\\(r^2\\)), is a statistical measure that assesses the proportion of the variance in the dependent variable that is explained by the independent variable(s) in a regression model. It ranges from \\(0\\) to \\(1\\), where \\(0\\) indicates that the model does not explain any variability, and \\(1\\) indicates that the model explains all the variability. In other words, \\(r^2\\) provides insight into the goodness of fit of a regression model, indicating how well the model’s predictions match the observed data.\n\\[\\begin{align}\nr^2 = 1- \\frac{RSS}{SSE} \\label{r2}\n\\end{align}\\]\n\n\nThe adjusted coefficient of determination, is a modification of the regular \\(r^2\\) in regression analysis. While \\(r^2\\) assesses the proportion of variance explained by the independent variables, the \\(r^2_{adjusted}\\) takes into account the number of predictors (\\(k\\)) in the model, addressing potential issues with overfitting according to \\(\\eqref{r2adj}\\).\nThe \\(r^2_{adjusted}\\) incorporates a penalty for adding unnecessary predictors that do not significantly contribute to explaining the variance in the dependent variable. This adjustment helps prevent an inflated \\(r^2\\) when including more predictors, even if they don’t improve the model significantly.\n\n\n\n\n\n\n\n\nFigure 5.5: The influence of k (number of predictors) on \\(r^2\\) and \\(r^2_{adjusted}\\).\n\n\n\n\n\n\\[\\begin{align}\nr^2_{adjusted} = 1 - (1-r^2)\\frac{n-1}{n-k-1} \\label{r2adj}\n\\end{align}\\]\n\n\n\n\n\nCall:\nlm(formula = diameter ~ rpm, data = drive_shaft_rpm_dia)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89501 -0.19690 -0.01096  0.21917  1.00742 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.5000000  0.1406190   17.78   &lt;2e-16 ***\nrpm         0.0095000  0.0001399   67.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3126 on 498 degrees of freedom\nMultiple R-squared:  0.9025,    Adjusted R-squared:  0.9023 \nF-statistic:  4610 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nIn linear regression modeling, the absence of a visible pattern in the residuals is desirable because it indicates that the model adequately captures the underlying relationship between the independent and dependent variables. Residuals are the differences between the observed and predicted values, and their randomness or lack of discernible pattern suggests that the model is effectively explaining the variance in the data. A visible pattern in residuals could indicate that the model fails to account for certain patterns or trends, suggesting potential shortcomings or misspecifications in the regression model. Detecting and addressing such patterns in residuals is crucial for ensuring the validity and reliability of the linear regression analysis.\n\n\n\n\n\n\n\n\nFigure 5.6: There should not be a visible pattern in the residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: The resiuals should be normally distributed.\n\n\n\n\n\nIn linear regression, the assumption of normally distributed residuals is essential for accurate statistical inference, parameter estimation using ordinary least squares, and constructing reliable confidence intervals. Normal residuals indicate that the model appropriately captures data variability and helps identify issues like heteroscedasticity. While departures from normality may not always invalidate results, adherence to this assumption enhances the model’s robustness and reliability. If consistently violated, alternative modeling approaches or transformations may be considered.\n\n\n\n\n5.1.4 Hypostesis testing in linear regression\n\nNull Hypothesis (H0): \\(\\beta_1 = 0\\)\nAlternative Hypothesis (Ha): \\(\\beta_1 \\neq 0\\)\n\n\n\n\nTable 5.1: The significance of model parameters.\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.500\n0.141\n17.779\n0.000\n\n\nrpm\n0.010\n0.000\n67.895\n0.000\n\n\n\n\n\n\n\n\n\n\nIn linear regression, t testing of coefficients assesses whether individual regression coefficients significantly differ from zero, providing insights into the significance of each predictor’s contribution to the model.\n\n\n\n\nTable 5.2: The significance of the model.\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nstatistic\np.value\ndf\ndf.residual\nnobs\n\n\n\n\n0.902\n0.902\n4,609.692\n0.000\n1.000\n498.000\n500.000\n\n\n\n\n\n\n\n\n\n\nIn linear regression, the F-test assesses the overall significance of the regression model by comparing the fit of the model with predictors to a model without predictors, helping determine if the regression equation explains a significant proportion of the variance in the dependent variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapter003_Regression.html#multiple-linear-regression",
    "href": "chapter003_Regression.html#multiple-linear-regression",
    "title": "5  Regression Analysis",
    "section": "5.2 Multiple linear regression",
    "text": "5.2 Multiple linear regression\n\n\n\n\n\n\nTable 5.3: The data in a tabular overview including test for normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall\nN = 5001\nA\nN = 1651\nB\nN = 1811\nC\nN = 1541\np-value2\n\n\n\n\nrpm\n999 (932, 1,068)\n993 (923, 1,061)\n995 (927, 1,074)\n1,012 (946, 1,068)\n0.5\n\n\ndiameter\n11.95 (11.30, 12.66)\n11.90 (11.24, 12.51)\n11.98 (11.30, 12.67)\n12.01 (11.41, 12.77)\n0.4\n\n\nfeed\n40.01 (39.34, 40.67)\n39.98 (39.34, 40.63)\n39.91 (39.34, 40.65)\n40.05 (39.37, 40.78)\n0.7\n\n\n\n1 Median (Q1, Q3)\n\n\n2 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\n\n\n\nA short exploratory data analysis of the data for the multiple linear regression is given in Table 5.3.\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: The graphical test for normal distribution (QQ-plot)\n\n\n\n\n\nFigure 5.8 shows the graphical test for normal distribution for the multiple linear regression.\n\n\n\n\n\n\n\n\n\n\nFigure 5.9: The distribution of the output and input parameters.\n\n\n\n\n\nIn Figure 5.9 the distribution of the input data is shown in a histogram.\n\n\n\\[\\begin{align}\nY \\sim rpm + feed+ site \\label{mlmmodel}\n\\end{align}\\]\n\n\n\n\nTable 5.4: The output of the multiple linear regression modelling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nrpm\n0.00\n0.00, 0.01\n&lt;0.001\n\n\nfeed\n0.44\n0.29, 0.58\n&lt;0.001\n\n\nsite\n\n\n\n\n\n\n\n\n    A\n0.00\n—\n\n\n\n\n    B\n0.09\n-0.02, 0.20\n0.11\n\n\n    C\n0.08\n-0.03, 0.20\n0.15\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\\(\\eqref{mlmmodel}\\) shows the general model for the multiple linear regression model. In this example, also the production site (site A, site B and site C) is included to test, if different production sites lead to differently produced drive shafts. The results of the multiple regression are shown in Table 5.4. Whilst the continuous variables appear to be significant (\\(p&lt;\\alpha = 0.05\\)), the production site does not play a significant rolefor the drive shaft diameter.\n\n\n\n\n\n\n\n\n\n\nFigure 5.10: The model of the mulitple linear regression\n\n\n\n\n\nIn Figure 5.10 the model is shown to ease the interpretation. With increasing rpm or feed also the drive shaft diameter increases.\n\n\n\n\n\n\n\n\n\n\nFigure 5.11: The check for pattern in the residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.12: The check for normal distribution in the residuals.\n\n\n\n\n\nIn Figure 5.12 the normal distribution of the residuals is confirmed, the model appears to be valid.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapter003_Regression.html#logistic-regression",
    "href": "chapter003_Regression.html#logistic-regression",
    "title": "5  Regression Analysis",
    "section": "5.3 Logistic Regression",
    "text": "5.3 Logistic Regression\n\n\n\n\n\n\n\n\n\n\nFigure 5.13: The basic idea of logisitic regression.\n\n\n\n\n\nLogistic regression is a statistical method designed for binary classification problems (Figure 5.13). It models the probability that an observation belongs to a particular class using the sigmoid (logistic) function \\(\\eqref{sigmoid}\\). The key steps include:\n\nProbability Modeling:\n\nModel predicts the probability of an instance belonging to a specific class.\n\nLinear Combination:\n\nCombines linearly weighted input features, representing the log-odds of the positive class.\n\nSigmoid Function:\n\nTransforms the linear combination to ensure output is between 0 and 1.\n\nDecision Boundary:\n\nThreshold probability (usually \\(0.5\\)) determines class assignment.\n\nMaximum Likelihood Estimation:\n\nParameters are estimated using maximum likelihood to maximize the likelihood of observed outcomes.\n\nOdds Ratio:\n\nQuantifies the impact of each predictor on the odds of the positive class.\n\n\nLogistic regression is widely used for binary classification tasks in different domains, providing an interpretable way to model the relationship between predictors and a binary outcome.\n\n\n\\[\\begin{align}\np = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x)}} \\label{sigmoid}\n\\end{align}\\]\n\n\nThe ordinary linear regression equation is shown in \\(\\eqref{linreg}\\).\nIf for \\(y\\) the probabilities \\(P\\) are used they may be \\(&gt;1\\) or \\(&lt;0\\) which is not possible for \\(P\\).\n\n\nTo overcome this issue, the odds of \\(P = \\frac{P}{1-P}\\) are taken.\n\\[\\begin{align}\n\\frac{P}{1-P} &= \\beta_0 + \\beta_1x \\label{logreg01} \\\\\n\\frac{P}{1-P} &\\in {0 \\ldots + \\infty} \\nonumber\n\\end{align}\\]\nRestricted variables are not easy to model why \\(\\eqref{logreg01}\\) is expanded to \\(\\eqref{logreg02}\\).\n\\[\\begin{align}\n\\log\\left( \\frac{P}{1-P}\\right) &= \\beta_0 + \\beta_1x \\label{logreg02}\n\\end{align}\\]\nWhich then in turn gives \\(\\eqref{sigmoid}\\).\n\n\n\n\n5.3.1 \\(\\beta_0 = 1\\) and \\(\\beta_1 = 1\\)\n\n\n\n\n\n\n\n\nFigure 5.14: The influence of different paramters for the sigmoid function\n\n\n\n\n\nIn order to better understand the influencing factors a small parametric study on \\(\\beta_0\\) and \\(\\beta_1\\) is given. Figure 5.14 the sigmoid function \\(p = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x)}}\\) with \\(\\beta_0=1\\) and \\(\\beta_1 = 1\\) is shown as a reference. Please note that the linear regression (\\(\\beta_0 + \\beta_1x\\)) expands the usual sigmoid function which is given by\n\\[f(x) = \\frac{1}{1+e^{-x}}\\]\nto model it in the intercept and gradient kind of logic.\n\n\n\n5.3.2 \\(\\beta_0 = 1\\) and \\(\\beta_1 = 0 \\ldots 5\\)\n\n\n\n\n\n\n\n\nFigure 5.15: The influence of different paramters for the sigmoid function\n\n\n\n\n\nIn the first case of the parametric study the gradient parameter is studied by varying it between \\(0\\ldots5\\) with \\(step_{size}=1\\). From Figure 5.15 it can be seen, that the linear regression gradient parameters varies the characteristic S-like shape of the sigmoid function. The higher \\(\\beta_1\\) is, the more pronounced the S-shape becomes. The reference shape for \\(\\beta_0 = 1\\) and \\(\\beta_1 = 1\\) is shown in light gray in the figure. An interesting effect is visible for a gradient of \\(\\beta_1 = 0\\): The function becomes a constant which only depends on the intercept (in this case \\(\\beta_0=1\\)).\n\n\n\n5.3.3 \\(\\beta_0 = 1\\) and \\(\\beta_1 = -5 \\ldots 0\\)\n\n\n\n\n\n\n\n\nFigure 5.16: The influence of different paramters for the sigmoid function\n\n\n\n\n\nWhen the parameter study is expanded to negative values of \\(\\beta_1\\) (\\(\\beta_1 = -5 \\ldots 0\\)) the curve is mirrored and reverses its direction (see Figure 5.16), which is also highlighted by the reference shape for \\(\\beta_0 = 1\\) and \\(\\beta_1 = 1\\) in light gray. The general interpretation for the influence of this parameter is reversed by stays the same: the larger the deviation from \\(0\\) is for \\(\\beta_1\\), the more pronounced the S-like shape becomes.\n\n\n\n5.3.4 \\(\\beta_0 = 0\\ldots 5\\) and \\(\\beta_1 = 1\\)\n\n\n\n\n\n\n\n\nFigure 5.17: The influence of different paramters for the sigmoid function\n\n\n\n\n\nThe second step is to vary the intercept (\\(\\beta_1\\)) of the linear regression function that is “hidden” within the sigmoid function. The reference function for \\(\\beta_0 = 1\\) and \\(\\beta_1 = 1\\) is again shown in light gray in the background in Figure 5.17. It can clearly be seen, that the intercept in a sigmoid-function setting can be used as a kind of offset. Whilst the curve is exactly \\(0.5\\) at \\(\\beta_0 = 0\\), this intersection can be adapted by modeling the intercept. For \\(\\beta_0 &gt; 0\\) the intersection point becomes \\(&gt;0.5\\).\n\n\n\n5.3.5 \\(\\beta_0 = -5 \\ldots 0\\) and \\(\\beta_1 = 1\\)\n\n\n\n\n\n\n\n\nFigure 5.18: The influence of different paramters for the sigmoid function\n\n\n\n\n\nThe reference function for \\(\\beta_0 = 1\\) and \\(\\beta_1 = 1\\) is again shown in light gray in the background in Figure 5.18. For an intercept \\(&lt;0\\) the intersection point with the xaxis then offsets the curve in the other direction compared with Figure 5.17. For \\(\\beta_0 &lt; 0\\) the intersection point becomes \\(&lt;0.5\\). In both cases the S-shape like characteristic of the sigmoid function is retained.\n\n\n5.3.6 Maximum Likelihood Estimation (MLE)\n\n\nMaximum Likelihood Estimation (MLE) is a statistical method used for estimating the parameters of a model (Starmer 2022). In this approach, the parameter values are chosen to maximize the likelihood function, which represents the probability of observing the given data under the assumed statistical model. The idea is to find the parameter values that make the observed data most probable.\nIn contrast to the cost function for linear regression \\(\\eqref{mse}\\), \\(\\hat{y_i}\\) in logistic regression is a non-linear function \\(\\eqref{yhatlog}\\).\n\\[\\begin{align}\n\\hat{y} = \\frac{1}{1+e^{-z}} \\label{yhatlog}\n\\end{align}\\]\nWhich is why the Maximum Likelihood Estimator is used.\nUsing the MLE basically means, to try different models (with different model parameters) that maximize the likelihood of the parameters being true. Because it is easier to look for minima (gradient descent), a loss function is formulated that can be used as a loss function.\n\n\n\n\n\n\n\n\n\n\nFigure 5.19: The principle of MLE.\n\n\n\n\n\n\\[\\begin{align}\n-\\log L(\\theta) = -\\sum_{i=1}^{n} y \\log(\\sigma(\\theta^Tx^i)) + (1-y)\\log(1-\\sigma(\\theta^Tx^i)) \\label{logresloss}\n\\end{align}\\]\n\n\n\n\n5.3.7 Modeling Production Data\n\n\n\n\n\n\n\n\n\n\nFigure 5.20: The data for the logistic regression data.\n\n\n\n\n\nIn Figure 5.20 the data for the production data. The drive shafts have been rated between PASSand FAIL and the lathing machine feed has been recorded. The question is now, at which feed the drive shafts start to FAIL.\n\n\n\n\n\n\nTable 5.5: The overview of the logistic regression data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 5001\n\n\n\n\nfeed\n19.89 (18.55, 21.40)\n\n\npass_1_fail_0\n\n\n\n\n    0\n256 (51%)\n\n\n    1\n244 (49%)\n\n\n\n1 Median (Q1, Q3); n (%)\n\n\n\n\n\n\n\n\n\n\n\nTable 5.5 shows an overview of the logistic regression data. PASS and FAIL are fairly similar distributed.\n\n\n\n\n\n\nTable 5.6: The modeling of the logisitic regression data.\n\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)\n95% CI\np-value\n\n\n\n\nfeed\n0.46\n0.35, 0.57\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\n\n\nThe model coefficients are shown in Table 5.6. Translated in equation \\(\\eqref{logregor}\\) and \\(\\eqref{logrege}\\) we can see, what has been computed.\n\\[\\begin{align}\n\\log(\\frac{P}{1-P}) &= -9.17 + 0.46x \\label{logregor} \\\\\n\\frac{P}{1-P} &= e^{-9.17 + 0.46x} \\label{logrege}\n\\end{align}\\]\nTherefore the models explains what the odds \\(\\frac{P}{1-P}\\) are for a drive shaft to be FAIL or PASS for a given feed.\n\n\n\n\n\n\n\n\n\n\nFigure 5.21: The probability (odds) for a drive shaft being PASS or FAIL for a given feed\n\n\n\n\n\nFigure 5.21 shows the probability for a drive shaft PASS or FAIL for a given feed as well as the confidence interval of the odds ratio for any given feed. For example the probability for PASS at a feed of 20 is \\(49 \\%\\) with a confidence interval of \\(44\\%\\) to \\(54\\%\\).\n\n\n\n5.3.7.0.1 residuals\n\n\n\n\n\n\n\n\nFigure 5.22: Are the residuals of the model normally distributed?\n\n\n\n\n\n\n\n5.3.7.1 Mc Fadden \\(R^2\\)\nMcFadden’s \\(R^2\\) is a measure used to evaluate the goodness of fit for logistic regression models and is calculated using \\(\\eqref{mcfadden}\\).\n\\[\\begin{align}\nR^2 = 1- \\frac{\\log(L_{model})}{\\log(L_{null})} \\label{mcfadden} = 0.1198876\n\\end{align}\\]\nIt compares the model to the null-model. It is much smaller then the coefficient of determination with values ranging between \\(0.2 \\ldots 0.4\\) already indicating a good model fit in practice.\n\n\n5.3.7.2 Confusion Matrix\n\n\n\n\n\n\n\n\n\n\nFigure 5.23: A confusion matrix\n\n\n\n\n\n\nA confusion matrix is a table used to evaluate the performance of a classification algorithm. It provides a detailed breakdown of the actual versus predicted classifications, enabling the calculation of various performance metrics. The matrix is particularly useful for binary and multiclass classification problems.\nOn the x-axis usually the ground truth is depicted whereas on the y-axis the predictions of the algorithm are shown. From this several performance metrics can be calculated.\n\n\nTrue Positive (TP): The number of positive instances correctly classified as positive.\nFalse Positive (FP): The number of negative instances incorrectly classified as positive (also known as Type I error).\nTrue Negative (TN): The number of negative instances correctly classified as negative.\nFalse Negative (FN): The number of positive instances incorrectly classified as negative (also known as Type II error).\n\n\n\n\n5.3.7.2.1 Accuracy\n\\[\\frac{TP + TN}{TP+FP+TN+FN}\\]\n\n\nDefinition\n\nThe ratio of correctly predicted instances (both true positives and true negatives) to the total instances.\n\n\n\n\nInterpretation\n\nAccuracy measures the overall correctness of the model. It indicates the proportion of total predictions that were correct. While accuracy is useful, it can be misleading in cases of imbalanced datasets where one class is more frequent than the other.\n\n\n\n\n\n\n5.3.7.2.2 Precision\n\\[\\frac{TP}{TP+FP}\\]\n\n\nDefinition\n\nThe ratio of true positive instances to the total instances predicted as positive.\n\n\n\n\nInterpretation\n\nPrecision, also known as positive predictive value, measures the accuracy of positive predictions. It is the proportion of correctly identified positive instances out of all instances predicted as positive. High precision indicates a low false positive rate.\n\n\n\n\n\n\n5.3.7.2.3 Recall\n\\[\\frac{TP}{TP+FN}\\]\n\n\nDefinition\n\nThe ratio of true positive instances to the total actual positive instances.\n\n\n\n\nInterpretation\n\nRecall measures the model’s ability to correctly identify all positive instances. It is the proportion of correctly identified positive instances out of all actual positive instances. High recall indicates a low false negative rate.\n\n\n\n\n\n\n5.3.7.2.4 Specificity\n\\[\\frac{TN}{TN+FP}\\]\n\n\nDefinition\n\nThe ratio of true negative instances to the total actual negative instances.\n\n\n\n\nInterpretation\n\nSpecificity measures the model’s ability to correctly identify negative instances. It is the proportion of correctly identified negative instances out of all actual negative instances. High specificity indicates a low false positive rate.\n\n\n\n\n\n\n5.3.7.2.5 F1 Score\n\\[2\\times\\frac{Precision\\times Recall}{Precision + Recall}\\]\n\n\nDefinition\n\nThe harmonic mean of precision and recall.\n\n\n\n\nInterpretation\n\nThe F1 Score combines precision and recall into a single metric. It provides a balance between the two, particularly useful when you need to take both false positives and false negatives into account. The F1 score is especially helpful when the class distribution is uneven or when you seek a balance between precision and recall.\n\n\n\n\n\n\n5.3.7.2.6 Summary on metrics\n\nAccuracy is best for overall performance but can be misleading for imbalanced datasets.\nPrecision is crucial when the cost of false positives is high.\nRecall is important when the cost of false negatives is high.\nSpecificity complements recall, providing insight into the true negative rate.\nF1 Score offers a balanced measure, useful when both precision and recall are important.\n\n\n\n\n5.3.7.3 Confusion Matrix in practice\n\n\n\n\n\n\n\n\nFigure 5.24: Confusion matrices at different probability thresholds\n\n\n\n\n\nFigure 5.24 shows three different confusion matrices at different probability threshold for the logistic regression model and the respective True Positive, False Positive, True Negative and False Negative rates. On the x-axis the reference is depicted and the true classes, being \\(0\\) for FAIL and \\(1\\) for PASS parts. The y-axis shows the prediction of the respective model with the classes again being \\(0\\) for FAIL and \\(1\\) for PASS. The probability threshold \\(P = 0.3 \\ldots 0.7\\) is the classification threshold of the model. The logistic regression model computes a Probability based on the Predictor variable (feed). This threshold then classifies the product as pass or fail\n\n\n5.3.7.4 Accuracy, correct classification rate, proportion of correct predictions\n\n\n\n\n\n\n\n\n\n\n\n5.3.7.5 Precision\n\n\n\n\n\n\n\n\n\n\n\n5.3.7.6 Recall, True positive rate, sensitivity, hit rate, detection rate\n\n\n\n\n\n\n\n\n\n\n\n5.3.7.7 Specificity, true negative rate, selectivity, true negative fraction, 1 - false positive rate\n\n\n\n\n\n\n\n\n\n\n\n5.3.7.8 F1 Score, harmonic mean of precision and recall\n\n\n\n\n\n\n\n\n\n\n\n5.3.7.9 Receiver Operator Curve (ROC)\n\n\n\n\n\n\n\n\n\n\n\n5.3.7.10 METRICSSS!!!!!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapter004_ProductionStatistics.html",
    "href": "chapter004_ProductionStatistics.html",
    "title": "6  Production Statistics",
    "section": "",
    "text": "6.1 Introduction to Production Statistics\nWhat Production Statistics tries to quanitfy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Production Statistics</span>"
    ]
  },
  {
    "objectID": "chapter004_ProductionStatistics.html#introduction-to-production-statistics",
    "href": "chapter004_ProductionStatistics.html#introduction-to-production-statistics",
    "title": "6  Production Statistics",
    "section": "",
    "text": "Output and Yield statistics refer to the measurement of both the quantity and quality of products or services produced during a specific period. This includes tracking metrics such as the number of units produced, yield rates, and defect rates, as well as assessing production cycle times.\nResource Utilization statistics involve the monitoring and analysis of how efficiently resources such as labor, machinery, materials, and energy are used in production processes. Key metrics in this category include machine uptime, downtime, and overall resource efficiency.\nQuality Control statistics play a vital role in evaluating the quality of products or services by tracking defects, errors, and variations in the production process. These statistics encompass defect rates, reject rates, and variation analysis to ensure products meet specified quality standards.\nCost Analysis through production statistics involves assessing the cost-effectiveness of production processes. This includes analyzing production costs, overhead expenses, and calculating the cost per unit produced. Such data aids in making informed decisions related to cost reduction and budgeting.\nInventory and Stock statistics pertain to the management of inventory levels and turnover rates. These statistics also encompass lead times and tracking stockouts, which are crucial for efficient inventory management and ensuring product availability.\nProduction Planning statistics are essential for optimizing production processes. Metrics include capacity utilization, order fulfillment rates, and production lead times. This data assists in scheduling and ensuring the efficient use of resources.\nDowntime and Maintenance statistics track equipment breakdowns, maintenance schedules, and production interruptions. Monitoring such data is vital for minimizing production downtime and ensuring equipment operates efficiently.\nEmployee Productivity statistics evaluate workforce performance and efficiency. Metrics such as output per worker and labor efficiency are used to assess employee contributions and identify areas for improvement, including training needs.\nSupply Chain Performance statistics extend beyond production to evaluate the entire supply chain, including suppliers, logistics, and distribution. Metrics like lead times, order fulfillment rates, and supplier performance data help ensure the efficiency of the supply chain.\nEnvironmental and Sustainability Metrics encompass resource consumption, waste generation, and environmental impact. They are used to assess an organization’s environmental footprint and implement sustainable practices.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Production Statistics</span>"
    ]
  },
  {
    "objectID": "chapter004_ProductionStatistics.html#control-charts-for-variables",
    "href": "chapter004_ProductionStatistics.html#control-charts-for-variables",
    "title": "6  Production Statistics",
    "section": "6.2 Control Charts for Variables",
    "text": "6.2 Control Charts for Variables\n\n6.2.1 The production\n\n\n\n\n\n\n\n\nFigure 6.1: The drive shaft production over time\n\n\n\n\n\nIn Figure 6.1 the drive shaft production and the behaviour of the mission critical parameter diameter is shown over time.\n\n\n6.2.2 Run Chart\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: A run chart with control and warning limits without subgroups.\n\n\n\n\n\n\n\n\\[\\begin{align}\nUCL &= \\bar{x} + 2.58\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=1 \\\\\nLCL &= \\bar{x} - 2.58\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=1 \\\\\nUWL &= \\bar{x} + 1.96\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=1 \\\\\nLWL &= \\bar{x} - 1.96\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=1\n\\end{align}\\]\n\nIn Shewhart (Shewhart and Deming 1986) charts for statistical process control, control limits such as the Upper Control Limit (UCL), Lower Control Limit (LCL), Upper Warning Limit (UWL), and Lower Warning Limit (LWL) play a crucial role. These limits establish boundaries for normal process variability. By incorporating confidence intervals, such as \\(97\\%\\) or \\(99\\%\\), into these limits, a statistical framework is added, providing a nuanced understanding of process variability. A \\(97\\%\\) confidence interval implies that \\(97\\%\\) of data points should fall within the calculated range, while a \\(99\\%\\) interval accommodates \\(99\\%\\). This approach enhances the sensitivity of Shewhart charts, aiding in the timely detection of significant process shifts. The choice of confidence level depends on the desired balance between false alarms and the risk of missing genuine deviations from the norm.\n\n\n\n6.2.3 X-bar chart\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: A X-bar chart with control and warning limits based on subgroups of \\(n=5\\)\n\n\n\n\n\n\n\n\\[\\begin{align}\nUCL &= \\bar{x} + 2.58\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=5 \\\\\nLCL &= \\bar{x} - 2.58\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=5 \\\\\nUWL &= \\bar{x} + 1.96\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=5 \\\\\nLWL &= \\bar{x} - 1.96\\frac{sd(x)}{\\sqrt{n}} \\;\\text{with}\\;GroupSize=5\n\\end{align}\\]\n\nAn X-bar chart is a statistical tool for quality control, used to monitor process stability over time. It involves collecting data, calculating subgroup means, determining control limits, and plotting the data on a chart. By monitoring points relative to the control limits, it helps identify shifts in the process mean, allowing corrective action for consistent quality.\nIt is effective in quality control because it focuses on detecting changes in the process mean. By setting statistical control limits, it distinguishes between common and special causes of variation. When data points fall outside these limits, it signals the presence of external factors, prompting corrective action. The chart’s visual representation of data points over time facilitates early issue detection, supporting a proactive approach to maintaining process stability and continuous improvement in quality control.\n\n\n\n6.2.4 S-Chart\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: The s chart with control and warning limits.\n\n\n\n\n\n\n\n\\[\\begin{align}\nUCL &= \\sigma * \\sqrt{\\frac{\\chi^2_{1-\\beta=0.995;n-1}}{n-1}} \\;\\text{with}\\;n=5 \\\\\nLCL &= \\sigma * \\sqrt{\\frac{\\chi^2_{1-\\beta=0.005;n-1}}{n-1}} \\;\\text{with}\\;n=5 \\\\\nUWL &= \\sigma * \\sqrt{\\frac{\\chi^2_{1-\\beta=0.975;n-1}}{n-1}} \\;\\text{with}\\;n=5 \\\\\nLWL &= \\sigma * \\sqrt{\\frac{\\chi^2_{1-\\beta=0.025;n-1}}{n-1}} \\;\\text{with}\\;n=5\n\\end{align}\\]\n\nAn S chart, or standard deviation chart, is a type of control chart used in statistical process control. It is designed to monitor the variability or dispersion of a process over time. The S chart displays the sample standard deviation of a process by plotting it against time or the sequence of samples. Similar to other control charts, it typically includes a central line representing the average standard deviation and upper and lower control limits. The S chart is useful for detecting shifts or trends in the variability of a process, allowing for timely adjustments or interventions if needed.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Production Statistics</span>"
    ]
  },
  {
    "objectID": "chapter004_ProductionStatistics.html#control-charts-for-attributes",
    "href": "chapter004_ProductionStatistics.html#control-charts-for-attributes",
    "title": "6  Production Statistics",
    "section": "6.3 Control Charts for Attributes",
    "text": "6.3 Control Charts for Attributes\n\n6.3.1 NP Chart\n\n\n\n\n\n\n\n\n\n\nFigure 6.5: A N-Chart with control limits.\n\n\n\n\n\n\n\n\\[\\begin{align}\nCL = n\\bar{p} \\pm 3\\sqrt{n\\bar{p}(1-\\bar{p})}\n\\end{align}\\]\n\nAn NP chart, also known as a Number of Defects Per Unit chart, is a statistical tool used in quality control to monitor the number of defects or errors in a process over time. It is commonly employed in manufacturing and other industries to assess the stability and performance of a production process. The chart typically displays the number of defects observed in a sample of units or products, allowing for the identification of trends, patterns, or variations in the defect rates. This information aids in quality improvement efforts by enabling organizations to take corrective actions and maintain consistent product or service quality.\n\n\n\n6.3.2 P Chart\n\n\n\n\n\n\n\n\n\n\nFigure 6.6: A P-Chart with control limits.\n\n\n\n\n\n\n\n\\[\\begin{align}\nCL = \\bar{p} \\pm 3\\sqrt{\\frac{\\bar{p}(1-\\bar{p})}{n}}\n\\end{align}\\]\n\nThe P chart is designed to track the proportion of nonconforming items or defects within a sample or subgroup over consecutive periods. The chart typically consists of a horizontal axis representing time periods and a vertical axis representing the proportion of nonconforming items. It helps identify variations and trends in the process, allowing for timely corrective actions when necessary.\nP charts are commonly used in industries where the output is binary, such as the presence or absence of a specific attribute, and provide a visual representation of the process’s performance, aiding in quality improvement efforts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Production Statistics</span>"
    ]
  },
  {
    "objectID": "chapter004_ProductionStatistics.html#process-capability-and-six-sigma",
    "href": "chapter004_ProductionStatistics.html#process-capability-and-six-sigma",
    "title": "6  Production Statistics",
    "section": "6.4 Process Capability and Six Sigma",
    "text": "6.4 Process Capability and Six Sigma\n\n6.4.1 How good is good enough?\n\n\n\n\n\n\n\n\nFigure 6.7: What are the joint probabilities?\n\n\n\n\n\nA success rate of \\(95\\%\\) per step (Figure 6.7) sounds at first glance like a successful process. After all, having a \\(95\\%\\) chance of winning the lottery would be awesome. Yet, the question is: What are the joint probabilities when we connect five steps sequentially? From previous chapters we know that the joint probability can be calculated in \\(\\eqref{jntprob}\\).\n\\[\\begin{align}\nP_{ges} = P_1 * P_2 * P_3 * P_4 * P_5 = 0.95^{(n=5)}=0.774 \\approx 77.4\\% \\label{jntprob}\n\\end{align}\\]\n\n\n\n\n\n\n\n\nFigure 6.8: Probabilities for success in sequence.\n\n\n\n\n\nThe joint probability for n-steps in sequence can therefore be estimated using \\(\\eqref{jntprob}\\) and visually represented in Figure 6.8. On the x-axis the number of steps is depicted whereas on the y-axis the joint probability is shown for the respective step index. As also calculated in \\(\\eqref{jntprob}\\) after \\(n=5\\) steps the joint probability for a good part drops to around \\(77\\%\\), which is not acceptable. Figure 6.8 shows that not even \\(98\\%\\) probability for a good part for a single step results in an acceptable joint probability (\\(P = 0.98^{n=5} = 0.904\\)). A staggering probability of \\(99.7\\%\\) for a single step is necessary to still reach a probability for a good part of \\(98\\%\\), and this is only true for \\(n = 5\\)steps. For an acceptable parts per milltion (ppm) rate the acceptable single step probability is \\(99.975\\%\\) as shown in Figure 6.8.\n\n\n\n\n\n\n\n\nFigure 6.9: The origin of the term Six Sigma (\\(6\\sigma\\))\n\n\n\n\n\nWhat that means in a tolerance-specification setting is shown in Figure 6.9. In order to ensure a \\(99.975\\%\\) for a continuous variable, the process variation (here measured as process standard deviation) must fit at least 6 times into the actual tolerance/specification window of the Critical to Quality (CTQ) measure. Additionally, this is only true if the process is centered. The term \\(6\\sigma\\) carries this inherent property for a \\(0ppm\\) production, which is favoured by many, but achieved by few.\n\n\n6.4.2 The Six Sigma Project Model (DMAIC)\n\n\n\n\n\n\n\n\nFigure 6.10: DMAIC Process\n\n\n\n\n\nThe Six Sigma Project model consists of five phases in total: (D)efine, (M)easure, (A)nalyse, (I)mprove, (C)ontrol. In essence these project phases are the application of the scientific method, but in a systematic and industry friendly way.\nThe Define Phase involves setting the project’s goals and objectives, identifying key stakeholders, developing a high-level process map, and defining customer requirements and critical-to-quality (CTQ) characteristics. Additionally, the project scope is established, and a project charter is developed to guide the overall initiative.\nIn the Measure Phase, key process metrics are identified, and relevant data is collected to assess the current state of the process. This phase includes analyzing process capability, creating detailed process maps, performing baseline measurements, and identifying potential data sources to ensure comprehensive data collection.\nDuring the Analyze Phase, potential root causes of process variation are identified through data analysis using statistical tools. Hypotheses for root causes are developed and verified through further data analysis. Root causes are then prioritized based on their impact and feasibility, and findings are validated with stakeholders to ensure accuracy and relevance.\nThe Improve Phase focuses on generating and evaluating potential solutions for process improvement. Implementing these improvements involves developing an implementation plan, conducting pilot tests if applicable, and optimizing the process based on feedback. Control measures are implemented to sustain the improvements achieved.\nFinally, the Control Phase involves developing control plans to monitor process performance continuously. This includes establishing process controls and standard operating procedures, implementing mistake-proofing measures, and defining key performance indicators (KPIs). Additionally, training programs for process stakeholders are developed, and a system for ongoing monitoring and feedback is established to ensure the process remains effective over time.\n\n\n\n6.4.3 Process Capability - idea\n\n\n\n\n\n\n\n\nFigure 6.11: The idea of process capabilities\n\n\n\n\n\nProcess capability refers to the ability of a process to consistently produce outputs that meet predetermined specifications or requirements. It is a measure of how well a process performs relative to its specifications. The general idea behind process capability is to assess the inherent variability of a process and determine whether it is capable of producing products or services within the desired quality limits.\n\nSpecification Limits: These are the predetermined limits or requirements for a particular process output, defining the range within which the product or service should fall to meet customer expectations.\nProcess Variation: This refers to the natural variability inherent in the process. Sources of variation can include factors such as machine performance, material properties, human factors, and environmental conditions.\nProcess Capability Indices: These are statistical measures used to quantify the relationship between process variation and specification limits. Common indices include \\(C_p\\), \\(C_{pk}\\), \\(P_p\\), and \\(P_{pk}\\), which provide insights into whether a process is capable of meeting specifications and how well it is centered within the specification limits.\nAssessment and Improvement: Once process capability is assessed, steps can be taken to improve it if necessary. This may involve reducing process variation, adjusting process parameters, implementing quality control measures, or redesigning the process altogether.\n\nOverall, the goal of analyzing process capability is to ensure that processes are capable of consistently delivering products or services that meet customer requirements, minimize defects, and optimize quality and efficiency.\n\n\n6.4.4 High Accuracy - Low Precision\n\n\n\n\n\n\n\n\nFigure 6.12: The spreaded - High Accuracy, Low Precision\n\n\n\n\n\nIn this scenario, the process consistently produces results that are very close to the target or desired value (high accuracy). However, the variation among individual measurements is large, meaning they are not tightly clustered around the target value (low precision). For example, if a machine consistently produces parts with dimensions close to the desired specifications but with significant variation in each part’s dimensions, it exhibits high accuracy but low precision.\n\n\n6.4.5 Low Accuracy - Low Precision\n\n\n\n\n\n\n\n\nFigure 6.13: The worst - Low Accuracy, Low Precision\n\n\n\n\n\nHere, the process consistently produces results that are far from the target or desired value (low accuracy). Additionally, the variation among individual measurements is large, indicating low precision. An example could be a manufacturing process that consistently produces parts with dimensions that are both far from the desired specifications and vary significantly from one part to another.\n\n\n6.4.6 Low Accuracy - High Precision\n\n\n\n\n\n\n\n\nFigure 6.14: The missing the mark - Low Accuracy, High Precision\n\n\n\n\n\nThis scenario involves a process that consistently produces results that are tightly clustered around a single point, but that point is far from the target or desired value (low accuracy). For instance, if a weighing scale consistently displays a weight that is slightly off from the true weight but shows very little variation between repeated measurements, it demonstrates low accuracy but high precision.\n\n\n6.4.7 High Accuracy - High Precision\n\n\n\n\n\n\n\n\nFigure 6.15: The desired - High Accuracy, High Precision\n\n\n\n\n\nThis is the ideal scenario where the process consistently produces results that are both very close to the target or desired value (high accuracy) and tightly clustered around that value (high precision). For example, a manufacturing process that consistently produces parts with dimensions very close to the desired specifications and with minimal variation between individual parts exhibits both high accuracy and high precision.\n\n\n\n6.4.8 Computing Process Capabilities\n\n\n\n\n\n\n\n\n\n\nFigure 6.16: The idea to calculate the \\(C_{pk}\\)\n\n\n\n\n\n\n\n\\[\\begin{align}\nC_{p} &= \\frac{USL-LSL}{6*sd} \\label{CpCalc} \\\\\nC_{pk} &= \\frac{\\min(USL-\\bar{x},\\bar{x}-LSL)}{3*sd} \\label{CpkCalc}\n\\end{align}\\]\n\n\n\\(C_p\\) compares the spread of the process variation to the width of the specification limits \\(\\eqref{CpCalc}\\). A \\(C_p\\) value greater than \\(1\\) indicates that the process spread fits within the specification limits, suggesting that the process has the potential to meet specifications. However, \\(C_p\\) does not take into account the process mean, so it does not provide information about process centering. For a more comprehensive assessment of process capability, both \\(C_p\\) and \\(C_{pk}\\) are often used together.\nThe \\(C_{pk}\\) value indicates the capability of the process relative to the specified limits \\(\\eqref{CpkCalc}\\). A \\(C_{pk}\\) value greater than 1 indicates that the process spread (6 standard deviations) fits within the specification limits. A value less than 1 indicates that the process spread exceeds the specification limits, indicating potential issues with meeting specifications. A higher \\(C_{pk}\\) value indicates better process capability.\n\n\n6.4.9 Process Capabilities and ppm\n\n\n\n\n\n\n\n\nFigure 6.17: The failed parts per million vs. the \\(C_{pk}\\)\n\n\n\n\n\nProcess capability and parts per million (PPM) are closely related metrics used to assess the performance of manufacturing processes. They provide a statistical measure of how well a process can produce output within specified limits. PPM is a measure of the number of defective parts per million produced by the process. The connection between process capability indices and PPM can be understood through statistical distributions, primarily the normal distribution, and the concept of defects or non-conformance.\nThe connection between process capability indices and PPM can be established through the Z-score (Z-standardization), which translates process capability into the probability of defects.\n\nUsing \\(C_p\\): Assuming the process is centered and follows a normal distribution: \\(Z = 3C_p\\). The corresponding PPM can be found from standard normal distribution tables. For example, if \\(C_p = 1\\), then \\(Z = 3\\), and the area under the normal curve beyond \\(3\\) standard deviations on either side is approximately \\(0.0027\\), or \\(2700PPM\\).\nUsing \\(C_{pk}\\) \\(C_{pk}\\) directly relates to the Z-score: \\(Z = 3C_{pk}\\). The PPM can be calculated using the cumulative distribution function for the normal distribution. For example, if \\(C_{pk} = 1.33\\), then \\(Z = 3 \\times 1.33 = 3.99\\). Using standard normal distribution tables, the area beyond \\(Z = 3.99\\) is approximately \\(0.000066\\), or \\(66ppm\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Production Statistics</span>"
    ]
  },
  {
    "objectID": "chapter004_ProductionStatistics.html#the-role-of-measurement-accuracy-in-production",
    "href": "chapter004_ProductionStatistics.html#the-role-of-measurement-accuracy-in-production",
    "title": "6  Production Statistics",
    "section": "6.5 The role of measurement accuracy in production",
    "text": "6.5 The role of measurement accuracy in production\n\n6.5.1 Measurement Errors\n\n\n\n\n\n\n\n\nFigure 6.18: Measurement Errors arise during every measurement.\n\n\n\n\n\nIn scientific experiments and real-world measurements, there are often inherent sources of random error (Nuzzo 2014). These errors can introduce variability into measurements, and the accumulation of these errors often conforms to a normal distribution. For instance, when measuring the diameter of an object with a caliper, small measurement errors can cause the observed values to follow a normal distribution. Even during such a simple measurement some random errors may include:\n\nParallax Error: Parallax can introduce random errors if the observer’s eye is not consistently aligned with the scale or graduations during measurements.\nDirt or Debris: Foreign particles or debris on the measuring surfaces can lead to random measurement errors by causing slight variations in the contact points between the caliper and the object.\nJaw Alignment: Small variations in the alignment of the caliper jaws from one measurement to another can introduce random errors in measurements.\nMaterial Deformation: When measuring soft or deformable materials, random errors can occur due to variations in the material’s response to pressure during different measurements.\nHuman Error: Random errors can arise from misreading the scale or not positioning the caliper precisely on the object, especially if different operators are involved.\nSlop or Play in the Jaws: Variability in the amount of play or slop in the caliper’s jaws from one measurement to another can lead to random errors in measurements.\n\n\n\n6.5.2 Significant Digits in Production\n\n\n\n\n\n\n\n\nFigure 6.19: Drawings and specifications are just an approximation of reality.\n\n\n\n\n\nSignificant digits, or significant figures, are vital for precision and quality in production. They ensure precision, quality, and consistency in production, leading to better efficiency and customer satisfaction. Significant digits indicate the precision of measurements, ensuring products meet quality standards and specifications.\nApplications:\n\nQuality Control: Accurate measurements ensure consistent product quality.\nTolerances: Precise tolerances (e.g., \\(\\pm0.05 mm\\)) must be adhered to.\nFit and Interchangeability: Parts must fit together correctly, requiring precise measurements.\nCalibration: Instruments must match the required significant digits for accuracy.\nDocumentation: Accurate recording of measurements is essential for quality reports and compliance.\nTraining: Employees must understand and apply significant digits to maintain standards.\n\nBest Practices:\n\nReduce Human Error: Training and audits are essential.\nUse Proper Instruments: Ensure tools can measure accurately.\nControl Environment: Manage factors like temperature and humidity.\nFollow Rounding Rules: Apply proper rounding to maintain precision.\n\n\n6.5.2.1 General Rule of Thumb\nTo maintain accuracy and avoid overestimating the precision of results, it’s advisable not to report more significant digits than justified by the precision of the input measurements.\n\n\n6.5.2.2 Rule of Ten\nIn practical terms, for a number to be considered significant, it should be at least ten times greater than the smallest unit of measure (i.e., the least significant digit). This helps in avoiding overestimating the precision and ensures that the reported figures are meaningful.\n\n\n6.5.2.3 Addition and Subtraction\nWhen performing addition or subtraction, the result should be reported with the same number of decimal places as the measurement with the fewest decimal places. For instance, if you add \\(12.11\\) (two decimal places) to \\(0.4\\) (one decimal place), the result should be reported with one decimal place, as \\(12.5\\).\n\n\n6.5.2.4 Multiplication and Division\nWhen performing multiplication or division, the result should be reported with the same number of significant digits as the measurement with the fewest significant digits. For example, if you multiply \\(2.34\\) (three significant digits) by $0.0$5 (one significant digit), the result should be reported with one significant digit, as \\(0.1\\).\n\n\n6.5.2.5 edge cases\n\n\n\n\n\n\n\n\nFigure 6.20: Edge cases during measuring a simple part.\n\n\n\n\n\nSignificant digits can help with edge cases that naturally occur during measurement processes. As depicted in Figure 6.20, the first two measurements are well within specification. The third measurement can actually not be interpreted, as the measurement instrument seems not to be fit for purpose. The fourth measurement shows, that the product is within the specification, it always holds the number with the smallest number of digits. The measurement of the fifth product is just within specification, the gage that shoed the last reading is not accurate enough.\nThere are many rules involved in these kind of edge cases including the rounding of number. It is referred to (Standards, (U.S.), and SEMATECH. 2002) or the national standards for more elaborate discussions about this manner.\n\n\n\n6.5.3 Measurement System Analysis Type I\nIn conducting a Measurement System Analysis Type I (MSA1), the initial step involves focusing on gage as the sole source of variation. To achieve this, 50 measurements are performed, each repeated on a reference part. This process allows for the isolation and assessment of the gage’s impact on the overall measurement system, ensuring that any observed variability is attributed solely to the gage. The process of doing a MSA1 is fairly standardized.\n\n6.5.3.1 Potential Capability index \\(C_g\\)\nFrom a MSA1 the potential Measurement System Capability Index \\(C_g\\) can be computed via \\(\\eqref{Cg}\\).\n\\[\\begin{align}\nC_g = \\frac{K/100*Tol}{L*\\sigma} \\label{Cg}\n\\end{align}\\]\n\n\\(Tol\\)\n\nTolerance\n\n\\(C_g\\)\n\nCapability Gage\n\nK\n\npercentage of the tolerance (\\(20\\%\\))\n\n\\(\\sigma\\)\n\nstandard deviations of the tolerance\n\nL\n\nnumber of standard deviations that represent the process (\\(6\\times\\))\n\n\n\n6.5.3.1.1 Capability index with systematic error \\(C_{gk}\\)\nVery similar to the process capability, a \\(C_g\\) gives only the potential capability as it does not include if the measures are centered around a mean. This is overcome by computing the Measurement Capability Index with systematic error \\(C_{gk}\\), which incorporates the mean via \\(\\eqref{Cgk}\\).\n\\[\\begin{align}\nC_{gk} = \\frac{(0.5*K/100)*Tol - |\\bar{x}-x_{true}|}{3*\\sigma} \\label{Cgk}\n\\end{align}\\]\n\n\\(Tol\\)\n\nTolerance\n\n\\(\\bar{x}\\)\n\nmean of the measurements\n\nK\n\npercentage of the tolerance (\\(20\\%\\))\n\n\\(x_{true}\\)\n\nthe “true” value of the reference (calibration)\n\n\\(\\sigma\\)\n\nstandard deviation of the measurements\n\n\n\n\n\n6.5.3.2 MSA1 example\n\n\n\n\nTable 6.1: The summary of the raw data for the MSA1.\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 501\n\n\n\n\nmeasured_data\n20.303 (0.005)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\nIn Table 6.1 the raw data that was collected during the experiments is depicted, whereas in Figure 6.21 the same data is shown in graphical format.\n\n\n\n\n\n\n\n\nFigure 6.21: The data as measured during the MSA1 with all measures included.\n\n\n\n\n\nOn the x-axis the measurement index is shown, the y-axis shows the measurment value. One of the main advantages of a MSA1 is, that a reference value is known, because the values are taken agains a standard reference normal. This true value (x_true in Figure 6.21, dashed black line) allows the estimation of a systematic error. The \\(20\\%\\) tolerance \\(\\eqref{Cgk}\\) is shown as dashed green line. This is the reduced tolerance in which the gage shall be capable to produce good measurement values.\n\n6.5.3.2.1 Data Distribution\n\n\n\n\n\n\n\n\nFigure 6.22: By definition, measurement errors shoul be normally disitrbuted.\n\n\n\n\n\nMeasurement errors are often assumed to be normally distributed due to the CLT and the nature of random processes involved. The CLT states that the sum of many independent, random variables tends to follow a normal distribution, even if the original variables are not normally distributed. Measurement errors typically result from the combination of numerous small, independent errors, such as instrument precision, environmental factors, and human mistakes. This aggregation leads to a normal distribution of the overall errors.\nAdditionally, many error sources are random and independent, further supporting the normal distribution assumption. The normal distribution is mathematically convenient, being fully described by its mean and variance, which simplifies statistical analysis and hypothesis testing. Empirical evidence across various fields also shows that measurement errors often approximate a normal distribution.\nWhile the normal distribution is a useful assumption, it may not always be valid. In cases with asymmetric errors, heavy tails, or significant outliers, other distributions may be more appropriate. Nonetheless, for many practical purposes, assuming a normal distribution for measurement errors is reasonable and effective.\n\n\n6.5.3.2.2 computed values\n\n\n\n\nTable 6.2: \\(C_g, C_{gk}\\) for the measured values\n\n\n\n\n\n\n\n\n\nCg\nCgk\n\n\n\n\n2.13\n2.02\n\n\n\n\n\n\n\n\n\n\nIn Table 6.2 the numeric values for \\(C_g\\) and \\(C_{gk}\\) are shown. Both values are well above \\(1.33\\) which indicates that the gage is fit for the measurement purpose at hand (defined by the tolerance). The potential gage capability (\\(C_g\\)) is greater than the actual gage capability \\(C_{gk}\\) which implies a systematic error, but the numeric values being \\(&gt;2\\) there seems not to be any reason to take serious action. If the systematic error is significant could be tested using the t-test for one variable.\n\n\n\n\n\n6.5.4 Measurement System Analysis Type II (Gage R&R)\n\n\n\n\n\n\n\n\n\n\nFigure 6.23: The general principle of a gage R & R\n\n\n\n\n\n\nA Gage R&R study assesses the variation in measurements from a specific process by measuring the same parts multiple times with the same instrument by different operators. It helps determine the reliability of the measurement system and identifies areas for improvement.\n\n\n6.5.4.1 Definitions\n\n\nAccuracy\n\nThe closeness of agreement between a test result and the accepted reference value(Cano, Moguerza, and Redchuk 2012).\n\n\n\n\n\nTrueness\n\nThe closeness of agreement between the average value obtained from a large series of test results and an accepted reference value(Cano, Moguerza, and Redchuk 2012).\n\n\n\n\n\nPrecision\n\nThe closeness of agreement between independent test results obtained under stipulated conditions(Cano, Moguerza, and Redchuk 2012).\n\n\n\n\n\nRepeatability\n\nPrecision under repeatability conditions (where independent test results are obtained using the same method on identical test items in the same laboratory by the same operator using the same equipment within short intervals of time)(Cano, Moguerza, and Redchuk 2012).\n\n\n\n\n\nReproducibility\n\nPrecision under reproducibility conditions (where test results are obtained using the same method on identical test items in different laboratories with different operators using different equipment)(Cano, Moguerza, and Redchuk 2012).\n\n\n\n\n\n6.5.4.2 Introductory example\n\nA battery manufacturer makes several types of batteries for domestic use.\nVoltage is Critical To Quality (CTQ)\n\n\n\nthe parts are the batteries \\(a = 3\\)\nthe appraisers are the voltmeters \\(b = 2\\)\nmeasurement is taken three times \\(n = 3\\)\n\\(a \\times b \\times n = 3 \\times 2 \\times 3 = 18\\) measurements\n\n\n\n\n6.5.4.3 The data\n\n\n\n\n\n\n\n\nFigure 6.24: The data from the 18 experiments for the GageR&R\n\n\n\n\n\n\n\n6.5.4.4 The analysis\n\nanova(lm(voltage ~ battery + voltmeter + battery * voltmeter, \n         data = ss.data.batteries))\n\nAnalysis of Variance Table\n\nResponse: voltage\n                  Df   Sum Sq  Mean Sq F value Pr(&gt;F)\nbattery            2 0.063082 0.031541  1.9939 0.1788\nvoltmeter          1 0.044442 0.044442  2.8095 0.1195\nbattery:voltmeter  2 0.018472 0.009236  0.5839 0.5728\nResiduals         12 0.189821 0.015818               \n\n\n\n\nWOW!\n\n\n\n6.5.4.5 Variance decomposition - the theory\n\n\n6.5.4.5.1 Repeatability\n\\[\\begin{align}\n\\sigma^2_{Repeatability} = MSE\n\\end{align}\\]\n\ndirectly obtainable in ANOVA table\n\n\n\n6.5.4.5.2 Reproducibility\n\n\n\\[\\begin{align}\n\\sigma^2_{Reproducibilty} = \\sigma^2_{Appraiser} + \\sigma^2_{Interaction}\n\\end{align}\\]\n\n\n\\[\\begin{align}\n\\sigma^2_{Appraiser} = \\frac{MSB-MSAB}{a \\times n}\n\\end{align}\\]\n\n\\(\\sigma^2_{Appraiser}\\)\n\nVariance introduced by appraisers\n\n\\(MSB\\)\n\nMean of squares - B\n\n\\(MSAB\\)\n\nMean squares of interaction - AB\n\n\\(a\\)\n\nnumber of levels for factor - number of batteries: 3\n\n\\(n\\)\n\nnumber of replicated measures: 3\n\n\n\n\n\\[\\begin{align}\n\\sigma^2_{Interaction} = \\frac{MSBA-MSE}{n}\n\\end{align}\\]\n\n\\(\\sigma^2_{Interaction}\\)\n\nVariance introduced by interaction\n\n\\(MSAB\\)\n\nMean squares of interaction - AB\n\n\\(MSE\\)\n\nMean squares of error\n\n\\(n\\)\n\nnumber of replicated measures: 3\n\n\n\n\n\n\n6.5.4.5.3 Gage R&R\n\\[\\begin{align}\n\\sigma^2_{Gage\\;R\\&R} = \\sigma^2_{Repeatability} + \\sigma^2_{Reproducibility}\n\\end{align}\\]\n\nAll variance is calculated that comes from the Gage!\n\n\nAre we finished?\n\n\nWe measure something, so what about the part?\n\n\n\n6.5.4.5.4 Part to Part\n\\[\\begin{align}\n\\sigma^2_{Part\\; to \\; Part} = \\frac{MSA-MSAB}{b \\times n}\n\\end{align}\\]\n\n\\(\\sigma^2_{Part\\; to \\; Part}\\)\n\nVariance introduced by the parts\n\n\\(MSA\\)\n\nMean of squares - A\n\n\\(MSAB\\)\n\nMean squares of interaction - AB\n\n\\(b\\)\n\nnumber of appraisers - number of voltmeters: 2\n\n\\(n\\)\n\nnumber of replicated measures: 3\n\n\n\n\n6.5.4.5.5 Total Variability\n\n\n\n\n6.5.4.6 Variance decomposition - the values\n\\[\\begin{align}\n\\sigma^2_{Repeatability} &= 0.0158 \\nonumber \\\\\n\\sigma^2_{Appraiser} &= 0.0039 \\nonumber \\\\\n\\sigma^2_{Interaction} &= 0 &lt;0 \\rightarrow 0 \\nonumber \\\\\n\\sigma^2_{Reproducibility} &= 0.0039 \\nonumber \\\\\n\\sigma^2_{Gage\\;R\\&R} &= 0.0197 \\nonumber \\\\\n\\sigma^2_{Part\\; to \\; Part} &= 0.0037 \\nonumber \\\\\n\\sigma^2_{Total} &= 0.0234 \\nonumber\n\\end{align}\\]\n\n\n6.5.4.7 Gage R&R “standardized output”\n\n6.5.4.7.1 AVNOVA table\n\n\n                  Df  Sum Sq Mean Sq F value Pr(&gt;F)\nbattery            2 0.06308 0.03154   3.415  0.227\nvoltmeter          1 0.04444 0.04444   4.812  0.160\nbattery:voltmeter  2 0.01847 0.00924   0.584  0.573\nRepeatability     12 0.18982 0.01582               \nTotal             17 0.31582                       \n\n\n\n\n6.5.4.7.2 ANOVA reduced model\n\n\n              Df  Sum Sq Mean Sq F value Pr(&gt;F)\nbattery        2 0.06308 0.03154   2.120  0.157\nvoltmeter      1 0.04444 0.04444   2.987  0.106\nRepeatability 14 0.20829 0.01488               \nTotal         17 0.31582                       \n\n\n\n\n6.5.4.7.3 Variance decomposition\n\n\n                      VarComp %Contrib\nTotal Gage R&R    0.018162959    86.74\n  Repeatability   0.014878111    71.05\n  Reproducibility 0.003284848    15.69\n    voltmeter     0.003284848    15.69\nPart-To-Part      0.002777127    13.26\nTotal Variation   0.020940086   100.00\n\n\n\n\n6.5.4.7.4 Study Variance\n\n\n                      StdDev  StudyVar %StudyVar %Tolerance\nTotal Gage R&R    0.13477002 0.8086201     93.13      80.86\n  Repeatability   0.12197586 0.7318552     84.29      73.19\n  Reproducibility 0.05731359 0.3438816     39.61      34.39\n    voltmeter     0.05731359 0.3438816     39.61      34.39\nPart-To-Part      0.05269846 0.3161907     36.42      31.62\nTotal Variation   0.14470690 0.8682414    100.00      86.82\n\n\n\n\n6.5.4.7.5 ndc - number of distinct categories\n\n\n[1] 1\n\n\n\n\n6.5.4.7.6 standardized graphical output\n\n\n\n\n\n\n\n\nFigure 6.25: A standardized graphical output after a complete GageR&R\n\n\n\n\n\n\n\n\n6.5.4.8 Gage R&R in the classroom\n\n3 parts\n3 volunteers\n1 recorder\n1 gage\n10 experiments\n3 repetitions\nrandomize the trials\nnow do it\n\n\n\n6.5.4.9 Attribute Agreement Analysis\nAttribute Agreement Analysis (AAA) is a statistical method used to evaluate the agreement among multiple observers when assigning categorical ratings to items. It involves defining attributes, selecting observers, collecting ratings, and analyzing the data to determine the level of agreement. This helps ensure the reliability of assessments and informs decision-making processes.\n\n6.5.4.9.1 Setup\n\n\n\n6.5.4.9.2 Results\n\n\n\n\nTable 6.3\n\n\n\n\n\n\n\n\n\nappraiser\nruns\nunits\nreference\nresults\n\n\n\n\n1\n1\n3\nbad\nbad\n\n\n1\n1\n1\ngood\ngood\n\n\n1\n1\n2\nbad\ngood\n\n\n2\n1\n3\nbad\ngood\n\n\n2\n1\n1\ngood\ngood\n\n\n2\n1\n2\nbad\ngood\n\n\n1\n2\n3\ngood\ngood\n\n\n1\n2\n1\nbad\nbad\n\n\n1\n2\n2\nbad\nbad\n\n\n2\n2\n3\ngood\nbad\n\n\n2\n2\n1\nbad\nbad\n\n\n2\n2\n2\nbad\ngood\n\n\n\n\n\n\n\n\n\n\n\n\n6.5.4.9.3 Overall agreement\n\\[\\begin{align}\nAgreement_{overall} = 100 \\times \\frac{X}{N}\n\\end{align}\\]\n\n\\(X\\)\n\nnumber of times appraisers agree with reference\n\n\\(N\\)\n\nnumber of rows with valid data\n\n\n\n\\[\\begin{align}\nAgreement_{overall} = 58.3\\% \\nonumber\n\\end{align}\\]\n\n\n\n6.5.4.9.4 Appraiser Agreement\n\\[\\begin{align}\nAgreement_{appraiser} = 100 \\times \\frac{X}{N}\n\\end{align}\\]\n\n\\(X\\)\n\nnumber of times the single appraisers agrees with reference\n\n\\(N_i\\)\n\nnumber of runs for the \\(i\\)-th appraiser\n\n\n\n\\[\\begin{align}\nAppraiser_{1} &= 83.3\\% \\nonumber \\\\\nAppraiser_{2} &= 33.3\\% \\nonumber\n\\end{align}\\]\n\n\n\n6.5.4.9.5 Reference Agreement\n\\[\\begin{align}\nAgreement_{reference} = 100 \\times \\frac{X}{N}\n\\end{align}\\]\n\n\\(X\\)\n\nnumber of times result agrees with the reference\n\n\\(N_i\\)\n\nnumber of runs for the \\(i\\)-th result\n\n\n\n\\[\\begin{align}\nReference_{bad} &= 50\\% \\nonumber \\\\\nReference_{good} &= 75\\% \\nonumber\n\\end{align}\\]\n\n\n\n6.5.4.9.6 Run agreement\n\\[\\begin{align}\nAgreement_{run} = 100 \\times \\frac{X}{N}\n\\end{align}\\]\n\n\\(X\\)\n\nnumber of reference agreement in runs\n\n\\(N_i\\)\n\nnumber of runs for the \\(i\\)-th run\n\n\n\n\\[\\begin{align}\nReference_{1} &= 50\\% \\nonumber \\\\\nReference_{2} &= 66.7\\% \\nonumber\n\\end{align}\\]\n\n\n\n6.5.4.9.7 Appraiser and reference agreement\n\\[\\begin{align}\nAgreement_{appraiser \\; ref} = 100 \\times \\frac{X}{N}\n\\end{align}\\]\n\n\\(X\\)\n\nnumber of reference agreement in for appraisers in reference class\n\n\\(N_i\\)\n\nnumber of agreements for the \\(i\\)-th appraiser and the \\(i\\)-th standard\n\n\n\n\n\n\nTable 6.4\n\n\n\n\n\n\n\n\n\nappraiser\nreference\noverall_agreement\n\n\n\n\n1\nbad\n75.00%\n\n\n1\ngood\n100.00%\n\n\n2\nbad\n25.00%\n\n\n2\ngood\n50.00%\n\n\n\n\n\n\n\n\n\n\n\n\n6.5.4.9.8 graphical representation\n\n\n\n\n\n\n\n\n\n\nFigure 6.26: Single appraiser agreement to reference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.27: How good is the agreement in the reference?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.28: Single run agreement to reference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.29: Appraiser ref agreement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCano, Emilio L., Javier M. Moguerza, and Andrés Redchuk. 2012. “Six Sigma with r” Not available: Not available. https://doi.org/10.1007/978-1-4614-3652-2.\n\n\nNuzzo, Regina. 2014. “Scientific Method: Statistical Errors.” Nature 506 (7487): 150–52. https://doi.org/10.1038/506150a.\n\n\nShewhart, Walter Andrew, and William Edwards Deming. 1986. Statistical Method from the Viewpoint of Quality Control. Courier Corporation.\n\n\nStandards, National Institute of, Technology (U.S.), and International SEMATECH. 2002. “NIST/SEMATECH Engineering Statistics Handbook,” January. https://doi.org/10.18434/M32189.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Production Statistics</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html",
    "href": "chapter005_DoE.html",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "",
    "text": "7.1 (O)ne (F)actor (A)t a (T)ime\nFigure 7.1: OFAT quickly becomes cumbersome",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#curse-of-dimensionality",
    "href": "chapter005_DoE.html#curse-of-dimensionality",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.2 curse of dimensionality",
    "text": "7.2 curse of dimensionality\n\\[\\begin{align}\nn_{experiments} = n_{levels}^{n_{factors}}\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#concept-of-anova",
    "href": "chapter005_DoE.html#concept-of-anova",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.3 Concept of ANOVA",
    "text": "7.3 Concept of ANOVA\n\n\n\n\n\n\n\n\nFigure 7.2: classical ANOVA concept",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#basics-of-experimental-design",
    "href": "chapter005_DoE.html#basics-of-experimental-design",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.4 Basics of Experimental Design",
    "text": "7.4 Basics of Experimental Design\n\n\n\n\n\n\n\n\nFigure 7.3: The connection between ANOVA and DoE.\n\n\n\n\n\nDesign of Experiments",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#experimental-planning-strategies",
    "href": "chapter005_DoE.html#experimental-planning-strategies",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.5 Experimental planning strategies",
    "text": "7.5 Experimental planning strategies\n\n\n1.No planning\n\nbad way of conducting an experiment\nhappens often enough (trial-and-error approach)\n\n2.Plan everything at the beginning\n\nafter definition the entire budget is allocated to perform all possible experiments\ndoes not take into account intermediate results\nspend money on experiments that contributed nothing to our knowledge of the process\n\n3.Sequential planning\n\nfirst stage, a reduced number of trials will be conducted to make decisions about the next stage\nfirst stage should consume between \\(25\\%\\) and \\(40\\%\\) of the budget\nmost of the budget should be spent in subsequent stages, taking into account previous results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#pizza-dough-example",
    "href": "chapter005_DoE.html#pizza-dough-example",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.6 pizza dough example",
    "text": "7.6 pizza dough example\n\nrepresentation of factors and levels for a designed experiment\nexample: pizza dough\n\nfood manufacturer is looking for the best recipe for its main product: pizza dough sold in retailers\nthree factors shall be determined: flour, salt, baking powder: bakPow\nresponse will be determined by experts as score\nfactors are to be set low (\\(-\\)) and high (\\(+\\))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#design-matrix",
    "href": "chapter005_DoE.html#design-matrix",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.7 design matrix",
    "text": "7.7 design matrix\n\n\n\n\nTable 7.1: The design matrix for the pizza dough experimentation\n\n\n\n\n\n\n\n\n\nflour\nsalt\nbakPow\nscore\n\n\n\n\n-\n-\n-\nNA\n\n\n+\n-\n-\nNA\n\n\n-\n+\n-\nNA\n\n\n+\n+\n-\nNA\n\n\n-\n-\n+\nNA\n\n\n+\n-\n+\nNA\n\n\n-\n+\n+\nNA\n\n\n+\n+\n+\nNA\n\n\n\n\n\n\n\n\n\n\n\n\nBe bold, but not stupid!\n\n\n7.7.1 progressive experimentation\n\n\nOFAT\n\nwill leave out interactions of variables\n\n\\(2^k\\): two-level factor experimentation\nincluding replications\n\n\nScreening experiments: to select the most important factors\nCharacterizing experiments: to study the model (residuals) of \\(Y = f(X)\\)\nOptimization experiments: operational minimum value for the process",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#model-assumptions",
    "href": "chapter005_DoE.html#model-assumptions",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.8 Model assumptions",
    "text": "7.8 Model assumptions\n\nrandomization!\n\n\n\n\n\nTable 7.2: The randomized design matrix for experimental runs\n\n\n\n\n\n\n\n\n\nflour\nsalt\nbakPow\nscore\nord\n\n\n\n\n+\n+\n-\nNA\n1\n\n\n-\n+\n-\nNA\n2\n\n\n-\n-\n+\nNA\n3\n\n\n+\n-\n-\nNA\n4\n\n\n-\n+\n+\nNA\n5\n\n\n+\n+\n+\nNA\n6\n\n\n-\n-\n-\nNA\n7\n\n\n+\n-\n+\nNA\n8",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#experimental-model",
    "href": "chapter005_DoE.html#experimental-model",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.9 experimental model",
    "text": "7.9 experimental model\n\n\n\n\n\n\n\n\nFigure 7.4: The experimental model for a DoE",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#analytical-model",
    "href": "chapter005_DoE.html#analytical-model",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.10 analytical model",
    "text": "7.10 analytical model\n\n\n\n\n\n\n\n\nFigure 7.5: The experimental model with the fitted linear model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#k-factorial-designs",
    "href": "chapter005_DoE.html#k-factorial-designs",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.11 \\(2^k\\) factorial Designs",
    "text": "7.11 \\(2^k\\) factorial Designs\n\n\\(k\\)\n\nnumber of factors to be studied, all with \\(2\\) levels\n\n\\(n\\)\n\nnumber of replications \\(\\rightarrow \\text{total number of experiments} = n \\times 2^k\\)\n\n\\(A,B, \\ldots\\)\n\nfactors (uppercase latin letters)\n\n\\(\\alpha, \\beta, \\ldots\\)\n\nmain effects",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#complete-analytical-model",
    "href": "chapter005_DoE.html#complete-analytical-model",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.12 complete analytical model",
    "text": "7.12 complete analytical model\n\n\n\nthree factors, \\(n\\) replications\n\n\\[\\begin{align}\n\\begin{split}\ny_{ijkl} = \\mu &+ \\alpha_i + \\beta_j + \\gamma_k\\\\\n&+ (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{kl}\\\\\n&+(\\alpha\\beta\\gamma)_{ijk}+\\epsilon_{ijkl} \\\\\n\\end{split}\\\\\n\\begin{split}\ni = 1,2 \\phantom{=} j = 1,2 \\phantom{=} k = 1,2 \\phantom{=} l = 1 \\ldots n \\\\\n\\epsilon_{ijkl} \\sim N(0,\\sigma)\n\\end{split} \\nonumber \\\\\n\\end{align}\\]\n\n\n\n\n\n\n\\(\\mu\\)\n\nglobal mean of the response\n\n\\(\\alpha_i\\)\n\neffect of factor \\(A\\) at level \\(i\\)\n\n\\(\\beta_j\\)\n\neffect of factor \\(B\\) at level \\(j\\)\n\n\\(\\gamma_k\\)\n\neffect of factor \\(C\\) at level \\(k\\)\n\n\\((\\alpha\\beta)_{ij}\\)\n\neffect of the interaction of factors \\(A\\) and \\(B\\) at levels \\(i\\) and \\(j\\)\n\n\n\n\n\\((\\alpha\\gamma)_{ik}\\)\n\neffect of the interaction of factors \\(A\\) and \\(C\\) at levels \\(i\\) and \\(k\\)\n\n\\((\\beta\\gamma)_{jk}\\)\n\neffect of the interaction of factors \\(B\\) and \\(C\\) at levels \\(j\\) and \\(k\\)\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\)\n\neffect of the interaction of factors \\(A,B\\) and \\(C\\) at levels \\(i,j\\) and \\(k\\)\n\n\\(\\epsilon_{ijkl}\\)\n\nrandom error component of the model\n\n\n\n\n\n\n\n\n7.12.1 pizza dough example raw data\n“… bake the pizza for 9min at 180°C …”\n\n\n\n\n\n\n\n\nrepl\nflour\nsalt\nbakPow\nscore\nord\n\n\n\n\n1\n-\n-\n-\n5.33\n2\n\n\n1\n+\n-\n-\n6.99\n4\n\n\n1\n-\n+\n-\n4.23\n8\n\n\n1\n+\n+\n-\n6.61\n5\n\n\n1\n-\n-\n+\n2.26\n1\n\n\n1\n+\n-\n+\n5.75\n6\n\n\n1\n-\n+\n+\n3.26\n3\n\n\n1\n+\n+\n+\n6.24\n7\n\n\n2\n-\n-\n-\n5.70\n2\n\n\n2\n+\n-\n-\n7.71\n4\n\n\n2\n-\n+\n-\n5.13\n8\n\n\n2\n+\n+\n-\n6.76\n5\n\n\n2\n-\n-\n+\n2.79\n1\n\n\n2\n+\n-\n+\n4.57\n6\n\n\n2\n-\n+\n+\n2.48\n3\n\n\n2\n+\n+\n+\n6.18\n7\n\n\n\n\n\n\n\n(Cano, Moguerza, and Redchuk 2012)\n\n\n7.12.2 pizza dough example summarised data\n\n\n\n\n\n\n\n\nflour\nsalt\nbakPow\nmean_score\n\n\n\n\n-\n-\n-\n5.515\n\n\n-\n-\n+\n2.525\n\n\n-\n+\n-\n4.680\n\n\n-\n+\n+\n2.870\n\n\n+\n-\n-\n7.350\n\n\n+\n-\n+\n5.160\n\n\n+\n+\n-\n6.685\n\n\n+\n+\n+\n6.210\n\n\n\n\n\n\n\n\n\n7.12.3 pizza dough recipe full model\n\ndoe.model1 &lt;- lm(score ~ flour + salt + bakPow +\nflour * salt + flour * bakPow +\nsalt * bakPow + flour * salt * bakPow,\ndata = ss.data.doe1)\n\nsummary(doe.model1)\n\n\nCall:\nlm(formula = score ~ flour + salt + bakPow + flour * salt + flour * \n    bakPow + salt * bakPow + flour * salt * bakPow, data = ss.data.doe1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5900 -0.2888  0.0000  0.2888  0.5900 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            5.5150     0.3434  16.061 2.27e-07 ***\nflour+                 1.8350     0.4856   3.779 0.005398 ** \nsalt+                 -0.8350     0.4856  -1.719 0.123843    \nbakPow+               -2.9900     0.4856  -6.157 0.000272 ***\nflour+:salt+           0.1700     0.6868   0.248 0.810725    \nflour+:bakPow+         0.8000     0.6868   1.165 0.277620    \nsalt+:bakPow+          1.1800     0.6868   1.718 0.124081    \nflour+:salt+:bakPow+   0.5350     0.9712   0.551 0.596779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4856 on 8 degrees of freedom\nMultiple R-squared:  0.9565,    Adjusted R-squared:  0.9185 \nF-statistic: 25.15 on 7 and 8 DF,  p-value: 7.666e-05\n\n\n\n\n7.12.4 pizza dough recipe elimination model\n\ndoe.model2 &lt;- lm(score ~ flour + bakPow,data = ss.data.doe1)\n\nsummary(doe.model2)\n\n\nCall:\nlm(formula = score ~ flour + bakPow, data = ss.data.doe1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.84812 -0.54344  0.06063  0.44406  0.86938 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.8306     0.2787  17.330 2.30e-10 ***\nflour+        2.4538     0.3219   7.624 3.78e-06 ***\nbakPow+      -1.8662     0.3219  -5.798 6.19e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6437 on 13 degrees of freedom\nMultiple R-squared:  0.8759,    Adjusted R-squared:  0.8568 \nF-statistic: 45.87 on 2 and 13 DF,  p-value: 1.288e-06\n\n\n\n\n7.12.5 pizza dough statistical model\n\\[\\begin{align}\n\\widehat{score} = 4.83 + 2.45\\times flour  -1.87 \\times bakPow \\\\\n\\widehat{score} = 5.12 + 1.23\\times flour  -0.93 \\times bakPow\n\\end{align}\\]\n\n\n7.12.6 main effect plot\n\n\n\n\n\n\n\n\nFigure 7.6: The main effect plot for the pizza dough model\n\n\n\n\n\n\n\n7.12.7 interaction plot\n\n\n\n\n\n\n\n\nFigure 7.7: The interaction plot for the pizza dough model\n\n\n\n\n\n\n\n7.12.8 model validity\n\n7.12.8.1 residual patterns\n\n\n\n\n\n\n\n\nFigure 7.8: Check for any pattern in the model residuals\n\n\n\n\n\n\n\n7.12.8.2 residual distribution\n\n\n\n\n\n\n\n\nFigure 7.9: Check for the residuals normality (QQ plot)\n\n\n\n\n\n\nshapiro.test(doe.model2_aug$.resid)\n\n\n    Shapiro-Wilk normality test\n\ndata:  doe.model2_aug$.resid\nW = 0.90652, p-value = 0.1023",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#design-of-experiments-for-process-improvement",
    "href": "chapter005_DoE.html#design-of-experiments-for-process-improvement",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.13 Design of Experiments for process improvement",
    "text": "7.13 Design of Experiments for process improvement\nWhat if …\n\n… not all influencing factors (\\(X\\)) on the process have been identified?\n\n\n… some \\(X\\) depend on external conditions and are not under control?\n\n\nrobust design\n\n\n… means also including noise factors that are not under our control.\n\n\n7.13.1 pizza dough example\n\npizzas came out pretty bad as reported by the customers\npizza quality heavily relies on baking conditions! (\\(T = 180°C, t = 9min\\))\nalmost nobody followed the recipe\nnoise factors are included with two levels\n\n\\(7min\\) and \\(11min\\) as \\(t+\\) and \\(t-\\)\n\\(160°C\\) and \\(200°C\\) as \\(T+\\) and \\(t-\\)\n\n\\(2^5\\) factorial design with \\(2\\) replications \\(=64\\) experimental runs",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#linear-model---first-run",
    "href": "chapter005_DoE.html#linear-model---first-run",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.14 linear model - first run",
    "text": "7.14 linear model - first run\n\n\n\nCall:\nlm(formula = score ~ (. - repl)^3, data = ss.data.doe2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.20094 -0.32937  0.02625  0.35656  1.07187 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           3.16906    0.42203   7.509 5.09e-09 ***\nflour+                0.07406    0.54902   0.135  0.89340    \nsalt+                -1.47219    0.54902  -2.681  0.01078 *  \nbakPow+              -1.43219    0.54902  -2.609  0.01293 *  \ntemp+                 2.56156    0.54902   4.666 3.75e-05 ***\ntime+                 1.49594    0.54902   2.725  0.00967 ** \nflour+:salt+          1.71000    0.66214   2.583  0.01378 *  \nflour+:bakPow+        2.14000    0.66214   3.232  0.00254 ** \nflour+:temp+         -1.26250    0.66214  -1.907  0.06414 .  \nflour+:time+          0.46375    0.66214   0.700  0.48796    \nsalt+:bakPow+         0.89250    0.66214   1.348  0.18567    \nsalt+:temp+          -0.19500    0.66214  -0.294  0.76998    \nsalt+:time+           1.38625    0.66214   2.094  0.04302 *  \nbakPow+:temp+        -1.17000    0.66214  -1.767  0.08526 .  \nbakPow+:time+        -1.30375    0.66214  -1.969  0.05628 .  \ntemp+:time+          -3.91125    0.66214  -5.907 7.64e-07 ***\nflour+:salt+:bakPow+  0.14875    0.66214   0.225  0.82346    \nflour+:salt+:temp+    1.52375    0.66214   2.301  0.02696 *  \nflour+:salt+:time+   -1.11875    0.66214  -1.690  0.09930 .  \nflour+:bakPow+:temp+  0.22375    0.66214   0.338  0.73728    \nflour+:bakPow+:time+  0.09125    0.66214   0.138  0.89112    \nflour+:temp+:time+    0.30125    0.66214   0.455  0.65172    \nsalt+:bakPow+:temp+  -0.33125    0.66214  -0.500  0.61977    \nsalt+:bakPow+:time+   0.33625    0.66214   0.508  0.61451    \nsalt+:temp+:time+    -1.04375    0.66214  -1.576  0.12324    \nbakPow+:temp+:time+   2.19125    0.66214   3.309  0.00205 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6621 on 38 degrees of freedom\nMultiple R-squared:  0.9037,    Adjusted R-squared:  0.8404 \nF-statistic: 14.27 on 25 and 38 DF,  p-value: 1.428e-12",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "chapter005_DoE.html#linear-model---stepwise-elimination",
    "href": "chapter005_DoE.html#linear-model---stepwise-elimination",
    "title": "7  Introduction to Design of Experiments (DoE)",
    "section": "7.15 linear model - stepwise elimination",
    "text": "7.15 linear model - stepwise elimination\n\n7.15.1 get rid of non-significant\n\nselectionvar &lt;- step(model.prob1, method=\"backwards\")\n\nStart:  AIC=-34.13\nscore ~ ((repl + flour + salt + bakPow + temp + time) - repl)^3\n\n                    Df Sum of Sq    RSS     AIC\n- flour:bakPow:time  1    0.0083 16.669 -36.102\n- flour:salt:bakPow  1    0.0221 16.683 -36.049\n- flour:bakPow:temp  1    0.0501 16.710 -35.942\n- flour:temp:time    1    0.0908 16.751 -35.787\n- salt:bakPow:temp   1    0.1097 16.770 -35.714\n- salt:bakPow:time   1    0.1131 16.773 -35.701\n&lt;none&gt;                           16.660 -34.134\n- salt:temp:time     1    1.0894 17.750 -32.080\n- flour:salt:time    1    1.2516 17.912 -31.498\n- flour:salt:temp    1    2.3218 18.982 -27.784\n- bakPow:temp:time   1    4.8016 21.462 -19.926\n\nStep:  AIC=-36.1\nscore ~ flour + salt + bakPow + temp + time + flour:salt + flour:bakPow + \n    flour:temp + flour:time + salt:bakPow + salt:temp + salt:time + \n    bakPow:temp + bakPow:time + temp:time + flour:salt:bakPow + \n    flour:salt:temp + flour:salt:time + flour:bakPow:temp + flour:temp:time + \n    salt:bakPow:temp + salt:bakPow:time + salt:temp:time + bakPow:temp:time\n\n                    Df Sum of Sq    RSS     AIC\n- flour:salt:bakPow  1    0.0221 16.691 -38.017\n- flour:bakPow:temp  1    0.0501 16.719 -37.910\n- flour:temp:time    1    0.0908 16.759 -37.755\n- salt:bakPow:temp   1    0.1097 16.779 -37.682\n- salt:bakPow:time   1    0.1131 16.782 -37.670\n&lt;none&gt;                           16.669 -36.102\n- salt:temp:time     1    1.0894 17.758 -34.050\n- flour:salt:time    1    1.2516 17.920 -33.469\n- flour:salt:temp    1    2.3218 18.991 -29.756\n- bakPow:temp:time   1    4.8016 21.470 -21.902\n\nStep:  AIC=-38.02\nscore ~ flour + salt + bakPow + temp + time + flour:salt + flour:bakPow + \n    flour:temp + flour:time + salt:bakPow + salt:temp + salt:time + \n    bakPow:temp + bakPow:time + temp:time + flour:salt:temp + \n    flour:salt:time + flour:bakPow:temp + flour:temp:time + salt:bakPow:temp + \n    salt:bakPow:time + salt:temp:time + bakPow:temp:time\n\n                    Df Sum of Sq    RSS     AIC\n- flour:bakPow:temp  1    0.0501 16.741 -39.826\n- flour:temp:time    1    0.0908 16.782 -39.670\n- salt:bakPow:temp   1    0.1097 16.801 -39.598\n- salt:bakPow:time   1    0.1131 16.804 -39.585\n&lt;none&gt;                           16.691 -38.017\n- salt:temp:time     1    1.0894 17.780 -35.971\n- flour:salt:time    1    1.2516 17.942 -35.390\n- flour:salt:temp    1    2.3218 19.013 -31.682\n- bakPow:temp:time   1    4.8016 21.492 -23.836\n\nStep:  AIC=-39.83\nscore ~ flour + salt + bakPow + temp + time + flour:salt + flour:bakPow + \n    flour:temp + flour:time + salt:bakPow + salt:temp + salt:time + \n    bakPow:temp + bakPow:time + temp:time + flour:salt:temp + \n    flour:salt:time + flour:temp:time + salt:bakPow:temp + salt:bakPow:time + \n    salt:temp:time + bakPow:temp:time\n\n                   Df Sum of Sq    RSS     AIC\n- flour:temp:time   1    0.0908 16.832 -41.480\n- salt:bakPow:temp  1    0.1097 16.851 -41.408\n- salt:bakPow:time  1    0.1131 16.854 -41.395\n&lt;none&gt;                          16.741 -39.826\n- salt:temp:time    1    1.0894 17.830 -37.791\n- flour:salt:time   1    1.2516 17.993 -37.211\n- flour:salt:temp   1    2.3218 19.063 -33.513\n- bakPow:temp:time  1    4.8016 21.543 -25.687\n- flour:bakPow      1   22.5032 39.244  12.699\n\nStep:  AIC=-41.48\nscore ~ flour + salt + bakPow + temp + time + flour:salt + flour:bakPow + \n    flour:temp + flour:time + salt:bakPow + salt:temp + salt:time + \n    bakPow:temp + bakPow:time + temp:time + flour:salt:temp + \n    flour:salt:time + salt:bakPow:temp + salt:bakPow:time + salt:temp:time + \n    bakPow:temp:time\n\n                   Df Sum of Sq    RSS     AIC\n- salt:bakPow:temp  1    0.1097 16.941 -43.064\n- salt:bakPow:time  1    0.1131 16.945 -43.051\n&lt;none&gt;                          16.832 -41.480\n- salt:temp:time    1    1.0894 17.921 -39.466\n- flour:salt:time   1    1.2516 18.083 -38.889\n- flour:salt:temp   1    2.3218 19.154 -35.209\n- bakPow:temp:time  1    4.8016 21.633 -27.418\n- flour:bakPow      1   22.5032 39.335  10.847\n\nStep:  AIC=-43.06\nscore ~ flour + salt + bakPow + temp + time + flour:salt + flour:bakPow + \n    flour:temp + flour:time + salt:bakPow + salt:temp + salt:time + \n    bakPow:temp + bakPow:time + temp:time + flour:salt:temp + \n    flour:salt:time + salt:bakPow:time + salt:temp:time + bakPow:temp:time\n\n                   Df Sum of Sq    RSS     AIC\n- salt:bakPow:time  1    0.1131 17.054 -44.638\n&lt;none&gt;                          16.941 -43.064\n- salt:temp:time    1    1.0894 18.031 -41.075\n- flour:salt:time   1    1.2516 18.193 -40.502\n- flour:salt:temp   1    2.3218 19.263 -36.844\n- bakPow:temp:time  1    4.8016 21.743 -29.094\n- flour:bakPow      1   22.5032 39.445   9.025\n\nStep:  AIC=-44.64\nscore ~ flour + salt + bakPow + temp + time + flour:salt + flour:bakPow + \n    flour:temp + flour:time + salt:bakPow + salt:temp + salt:time + \n    bakPow:temp + bakPow:time + temp:time + flour:salt:temp + \n    flour:salt:time + salt:temp:time + bakPow:temp:time\n\n                   Df Sum of Sq    RSS     AIC\n&lt;none&gt;                          17.054 -44.638\n- salt:temp:time    1    1.0894 18.144 -42.675\n- flour:salt:time   1    1.2516 18.306 -42.106\n- flour:salt:temp   1    2.3218 19.376 -38.469\n- salt:bakPow       1    3.7588 20.813 -33.891\n- bakPow:temp:time  1    4.8016 21.856 -30.762\n- flour:bakPow      1   22.5032 39.558   7.208\n\n\n\n\n7.15.2 main effect and interaction\n\n\n\n\n\n\n\n\n\n\n\n7.15.3 check residuals\n\n\n\n\n\n\n\n\n\n\n\n7.15.4 pragmatic result\n\n\n\n\nTable 7.3: The pragmatic results for the DoE\n\n\n\n\n\n\n\n\n\nflour\nsalt\nbakPow\nscore\nT1t1\nT2t1\nT1t2\nT2t2\nMean\nSD\n\n\n\n\n-\n-\n-\n5.515\n3.675\n5.120\n4.185\n3.900\n4.479\n0.6352559\n\n\n+\n-\n-\n7.350\n3.370\n4.520\n5.050\n2.940\n4.646\n0.9814615\n\n\n-\n+\n-\n4.680\n0.955\n4.910\n5.295\n1.170\n3.402\n2.3394319\n\n\n+\n+\n-\n6.685\n3.590\n5.895\n5.625\n3.870\n5.133\n1.1827299\n\n\n-\n-\n+\n2.525\n1.915\n3.055\n1.725\n1.700\n2.184\n0.6446882\n\n\n+\n-\n+\n5.160\n3.140\n5.010\n5.535\n2.900\n4.349\n1.3216617\n\n\n-\n+\n+\n2.870\n1.215\n1.860\n3.040\n1.310\n2.059\n0.8388223\n\n\n+\n+\n+\n6.210\n5.805\n6.110\n5.980\n5.965\n6.014\n0.1249667\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCano, Emilio L., Javier M. Moguerza, and Andrés Redchuk. 2012. “Six Sigma with r” Not available: Not available. https://doi.org/10.1007/978-1-4614-3652-2.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Design of Experiments (DoE)</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bartlett, Maurice Stevenson. 1937. “Properties of Sufficiency and\nStatistical Tests.” Proceedings of the Royal Society of\nLondon. Series A - Mathematical and Physical Sciences 160 (901):\n268–82. https://doi.org/10.1098/rspa.1937.0109.\n\n\nBonferroni, C. E. 1936. Teoria Statistica Delle Classi e Calcolo\nDelle Probabilità. Pubblicazioni Del r. Istituto\nSuperiore Di Scienze Economiche e Commerciali Di Firenze. Seeber. https://books.google.de/books?id=3CY-HQAACAAJ.\n\n\nCano, Emilio L., Javier M. Moguerza, and Andrés Redchuk. 2012.\n“Six Sigma with r” Not available: Not available. https://doi.org/10.1007/978-1-4614-3652-2.\n\n\nChampely, Stephane. 2020. Pwr: Basic Functions for Power\nAnalysis. https://CRAN.R-project.org/package=pwr.\n\n\nCohen, Jacob. 2013. Statistical Power Analysis for the Behavioral\nSciences. Routledge. https://doi.org/10.4324/9780203771587.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022.\ndatasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDelignette-Muller, Marie Laure, and Christophe Dutang. 2015.\n“fitdistrplus: An R\nPackage for Fitting Distributions.” Journal of Statistical\nSoftware 64 (4): 1–34. https://doi.org/10.18637/jss.v064.i04.\n\n\nFriedman, Milton. 1937. “The Use of Ranks to Avoid the Assumption\nof Normality Implicit in the Analysis of Variance.” Journal\nof the American Statistical Association 32 (December): 675–701. https://doi.org/10.1080/01621459.1937.10503522.\n\n\nGreenhouse, Samuel W., and Seymour Geisser. 1959. “On Methods in\nthe Analysis of Profile Data.” Psychometrika 24 (June):\n95–112. https://doi.org/10.1007/bf02289823.\n\n\nHahs-Vaughn, Debbie L., and Richard G. Lomax. 2013. An Introduction\nto Statistical Concepts. Routledge. https://doi.org/10.4324/9780203137819.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings\nAre False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nJ. Bibby, E. J. G. Pitman. 1980. “Some Basic Theory for\nStatistical Inference.” The Mathematical Gazette 64\n(428): 138–38. https://doi.org/10.2307/3615104.\n\n\nJohnson, Norman Lloyd. 1994. Continuous Univariate\nDistributions. Wiley.\n\n\nMann, H. B., and D. R. Whitney. 1947. “On a Test of Whether One of\nTwo Random Variables Is Stochastically Larger Than the Other.”\nThe Annals of Mathematical Statistics. https://doi.org/10.1214/aoms/1177730491.\n\n\nMauchly, John W. 1940. “Significance Test for Sphericity of a\nNormal n-Variate Distribution.” The Annals of Mathematical\nStatistics 11 (2): 204–9. http://www.jstor.org/stable/2235878.\n\n\nMeyna, Arno. 2023. Sicherheit Und Zuverlässigkeit Technischer\nSysteme. Carl Hanser Verlag GmbH &\nCo. KG. https://doi.org/10.3139/9783446468085.fm.\n\n\nNuzzo, Regina. 2014. “Scientific Method: Statistical\nErrors.” Nature 506 (7487): 150–52. https://doi.org/10.1038/506150a.\n\n\nOlkin, Ingram. June. Contributions to Probability and\nStatistics. Stanford Univ Pr.\n\n\nPearson, Karl. 1895. “Note on Regression and Inheritance in the\nCase of Two Parents.” Proceedings of the Royal Society of\nLondon Series I 58: 240–42.\n\n\nRamalho, Joao. 2021. industRial: Data, Functions and Support\nMaterials from the Book \"industRial Data Science\". https://CRAN.R-project.org/package=industRial.\n\n\nRuder, Sebastian. 2016. “An Overview of Gradient Descent\nOptimization Algorithms.” http://arxiv.org/pdf/1609.04747.pdf.\n\n\nShapiro, S. S., and M. B. Wilk. 1965. “An Analysis of Variance\nTest for Normality (Complete Samples).” Biometrika 52\n(December): 591–611. https://doi.org/10.1093/biomet/52.3-4.591.\n\n\nShewhart, Walter Andrew, and William Edwards Deming. 1986.\nStatistical Method from the Viewpoint of Quality Control.\nCourier Corporation.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association\nBetween Two Things.” The American Journal of Psychology.\nhttps://doi.org/10.2307/1412159.\n\n\nStandards, National Institute of, Technology (U.S.), and International\nSEMATECH. 2002. “NIST/SEMATECH Engineering Statistics\nHandbook,” January. https://doi.org/10.18434/M32189.\n\n\nStarmer, J. 2022. The StatQuest Illustrated Guide to Machine\nLearning!!!: Master the Concepts, One Full-Color Picture at a Time, from\nthe Basics All the Way to Neural Networks. BAM! Packt Publishing,\nLimited. https://books.google.de/books?id=gWRGzwEACAAJ.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika 6 (1): 1. https://doi.org/10.2307/2331554.\n\n\nTaboga, Marco. 2017. Lectures on Probability Theory and Mathematical\nStatistics - 3rd Edition. Createspace Independent Publishing\nPlatform.\n\n\nTamhane, Ajit C. 1977. “Multiple Comparisons in Model i One-Way\nAnova with Unequal Variances.” Communications in Statistics -\nTheory and Methods 6 (January): 15–32. https://doi.org/10.1080/03610927708827466.\n\n\n“The R Graph Gallery – Help and Inspiration for r\nCharts.” 2022. https://r-graph-gallery.com/.\n\n\nTiedemann, Frederik. 2022. Gghalves: Compose Half-Half Plots Using\nYour Favourite Geoms. https://CRAN.R-project.org/package=gghalves.\n\n\nWeibull, Waloddi. 1951. “A Statistical Distribution Function of\nWide Applicability.” Journal of Applied Mechanics 18\n(3): 293–97. https://doi.org/10.1115/1.4010337.\n\n\nWELCH, B. L. 1947. “The Generalization of \"STUDENT’s\" Problem When\nSeveral Different Population Variances Are Involved.”\nBiometrika. https://doi.org/10.1093/biomet/34.1-2.28.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data\nScience. \"O’Reilly Media, Inc.\".\n\n\nWilke, Claus O. 2022. Ggridges: Ridgeline Plots in ’Ggplot2’.\nhttps://CRAN.R-project.org/package=ggridges.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>References</span>"
    ]
  }
]